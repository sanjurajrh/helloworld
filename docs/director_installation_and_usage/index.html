<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video#" >

<head profile="http://www.w3.org/1999/xhtml/vocab">
	  <!--[if IE]><![endif]-->
<meta charset="utf-8" />
<meta name="revision" title="e9d0e4c8-0013-4d12-8e17-c2955dc22645" product="7eff92b4-effc-4326-8604-6f852e3e9747" revision="c2973328ba7082bc1ee91cdd08745139619ae7de:en-us" page="a906e2af-dac2-4385-b754-99e929d94ba8" />
<meta name="generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/index" />
<meta property="og:title" content="Director Installation and Usage Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta property="og:description" content="Install Red Hat OpenStack Platform 16 in an enterprise environment using the Red Hat OpenStack Platform director. This includes installing the director, planning your environment, and creating an OpenStack environment with the director." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/index" />
<meta name="twitter:title" content="Director Installation and Usage Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta name="twitter:description" content="Install Red Hat OpenStack Platform 16 in an enterprise environment using the Red Hat OpenStack Platform director. This includes installing the director, planning your environment, and creating an OpenStack environment with the director." />
<meta name="twitter:image:src" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
  <title>Director Installation and Usage Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal</title>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="kcs06.web.prod.ext.phx2.redhat.com" />
<meta name="avalon-version" content="fc526cb5" />
<meta name="cp-chrome-build-date" content="2020-10-15T21:45:53.836Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />

 

<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en",  
        version   : "fc526cb5",
        builddate : "2020-10-15T21:45:53.836Z",
        fetchdate : "2020-10-19T10:37:35-0400",
        nrid      : "14615289",
        nrlk      : "2a497fa56f"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="favicon.ico" />

<link media="all" rel="stylesheet" type="text/css" href="bootstrap.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="bootstrap-grid.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="main.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="components.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="pages.css%3Fv=fc526cb5.css" />

<link href="chosen.css%3Fv=fc526cb5.css" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]-->

<noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>

<!-- /cssInclude -->
<script type="text/javascript" src="require.js%3Fv=fc526cb5" data-main="/webassets/avalon/j/"></script>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->

  <!-- TrustArc -->
  <script src="https://static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>

  <link type="text/css" rel="stylesheet" href="css__c2Nkkx_5vYh8rvZbfBAGB4EMzMtH5ouFJDBlDsnhSR8__-ax0_8bam15ZDJT9j7LilCfJrDyEhGGAgc0KC8HYvJg__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__Jy3BSr8TrxptaufAzQDT1skBUlX2CnL_wm6BizzYuGw__-YZvqB8yuA4kK_iKklbk5HdZCoG2qAgz1l-8Qi2NFH4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="messages.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__m76iIFREtc70Nmw5xe1ZwHbNBlwOP2Zjc3DcacNWnFQ__STEh8aY3w8E_bKzhB4Xke2WOQ9XMDQquHIP5B8SeDIY__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__4sM4s6XOQ2Nm0pdkrWRLvrgtwpTmHAFzR_LcdesKYj8__vJ0jjVEhoPh3KBeLZfkqg51T3AFxhQCuXg1reipa__k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="print" />
  <link rel="stylesheet" type="text/css" media="all" href="list.css" />
  <script src="js__ZyeOaiFuDejQQbhUV7yg7atYZnj4WLfH77o0scv4068__MZdWWgUEYpsEWLcU0RqkaXMsEyksbpCgnf4XwXRkqz0__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__5ysXPc5KIyMmizxqRY68ILfrEGrj0P29WBIifnPTJvQ__Cap0DACEVMsefumg1gS1APLLd8stDkdGfp6c1uswMo4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>chrometwo_require(["analytics/attributes"], function(attributes) {
attributes.set('ResourceID',       'e9d0e4c8-0013-4d12-8e17-c2955dc22645');
attributes.set('ResourceTitle',    'Director Installation and Usage');
attributes.set('Language',         'en-us');
attributes.set('RevisionId',       'c2973328ba7082bc1ee91cdd08745139619ae7de:en-us');
attributes.set('PublicationState', ['active','published']);
attributes.set('Product',          'Red Hat OpenStack Platform');
attributes.set('ProductVersion',   '16.1');
attributes.set('ProductId',        'Red Hat OpenStack Platform 16.1');
});</script>
<script>breadcrumbs = [ ["Products &amp; Services", "/products/"], ["Product Documentation", "/documentation/"], ["Red Hat OpenStack Platform", "/documentation/en-us/red_hat_openstack_platform/"], ["16.1", "/documentation/en-us/red_hat_openstack_platform/16.1/"], ["Director Installation and Usage", "/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/"] ];</script>
<script>window.siteMapState = "products";</script>
<script src="js__i6ieGBO-OPmIKm_f0srsb6gM7QELIWrBpKh6ub_yj8A__wjRg_duSK4rEZCRV2uwrCIPMl80Z_LJ9ew61H5hE-ZI__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__vGeEtorfXGI3aPAp71YA74N5p8wfpYVhnoIqwaJrOiQ__8abcou7ybLtGOLy-V9E-NQlDejOMZmdStjlYfyX9W-k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"kcs","theme_token":"zUjd9q8Ggh9YTmWL1eZbmV_hdxLadZNtuP2XhSBekyo","js":{"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.jquery.min.js":1,"modules\/chosen\/chosen.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/prism.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/underscore.js":1,"sites\/all\/themes\/kcs\/js\/kcs_base.js":1,"sites\/all\/themes\/kcs\/js\/showdown.js":1,"sites\/all\/themes\/kcs\/js\/case_links.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/book\/book.css":1,"modules\/comment\/comment.css":1,"modules\/date\/date_api\/date.css":1,"modules\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/user_prune\/css\/user_prune.css":1,"modules\/views\/css\/views.css":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.css":1,"modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/panels\/css\/panels.css":1,"sites\/all\/modules\/custom\/rate\/rate.css":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/plugins\/layouts\/docs_page\/docs_page.css":1,"https:\/\/access.redhat.com\/webassets\/avalon\/s\/messages.css":1,"sites\/all\/themes\/zen\/system.base.css":1,"sites\/all\/themes\/zen\/system.menus.css":1,"sites\/all\/themes\/zen\/system.messages.css":1,"sites\/all\/themes\/zen\/system.theme.css":1,"sites\/all\/themes\/zen\/comment.css":1,"sites\/all\/themes\/zen\/node.css":1,"sites\/all\/themes\/kcs\/css\/html-reset.css":1,"sites\/all\/themes\/kcs\/css\/wireframes.css":1,"sites\/all\/themes\/kcs\/css\/layout-fixed.css":1,"sites\/all\/themes\/kcs\/css\/page-backgrounds.css":1,"sites\/all\/themes\/kcs\/css\/tabs.css":1,"sites\/all\/themes\/kcs\/css\/pages.css":1,"sites\/all\/themes\/kcs\/css\/blocks.css":1,"sites\/all\/themes\/kcs\/css\/navigation.css":1,"sites\/all\/themes\/kcs\/css\/views-styles.css":1,"sites\/all\/themes\/kcs\/css\/nodes.css":1,"sites\/all\/themes\/kcs\/css\/comments.css":1,"sites\/all\/themes\/kcs\/css\/forms.css":1,"sites\/all\/themes\/kcs\/css\/fields.css":1,"sites\/all\/themes\/kcs\/css\/kcs.css":1,"sites\/all\/themes\/kcs\/css\/print.css":1}},"chosen":{"selector":"#edit-field-kcs-component-select-und, #edit-field-kcs-sbr-select-und, #edit-field-kcs-product-select-und, #edit-field-kcs-type-select-und, #edit-language, #edit-field-kcs-a-category-select-und, #edit-field-kcs-state-select-und, #edit-field-kcs-tags-select-und, #edit-field-kcs-state-select-value, #edit-field-vid-reference-product-und, #edit-state, #edit-product, #edit-category, #edit-field-kcs-product-select-tid, #edit-field-kcs-type-select-tid, #edit-field-kcs-a-category-select-tid, #edit-sort-bef-combine, #edit-kcs-state, #edit-field-category-tid, #edit-field-product-tid, #edit-field-tags-tid, #edit-field-category-und, #edit-field-product-und, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-category, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-product, #views-exposed-form-questions-list-questions-filter-block #edit-field-tags, #edit-field-mega-menu-tab-und, #edit-field-internal-tags-tid, #edit-tags, #edit-field-kcs-tags-select-tid, #edit-subscriptions-and-choose-field-select, #edit-subscriptions-and-type, #edit-vid-13, #edit-vid-4, #edit-vid-5, #edit-vid-53, #edit-vid-1, #edit-vid-3, #edit-group-blog, #edit-subscriptions-and-choose-field-select-two, #edit-subscriptions-and-type-two, #edit-vid-13-two, #edit-vid-4-two, #edit-vid-5-two, #edit-vid-53-two, #edit-vid-1-two, #edit-vid-3-two, #edit-group-blog-two, #edit-subscriptions-and-edit-choose-field-select, #edit-subscriptions-and-type-edit, #edit-subscriptions-and-edit-type-two, #edit-vid-13-edit-two, #edit-vid-4-edit-two, #edit-vid-5-edit-two, #edit-vid-53-edit-two, #edit-vid-1-edit-two, #edit-vid-3-edit-two, #edit-subscriptions-and-edit-choose-field-select-two, #edit-vid-13-edit, #edit-vid-4-edit, #edit-vid-5-edit, #edit-vid-53-edit, #edit-vid-1-edit, #edit-vid-3-edit, #edit-group-blog-edit, #edit-field-supported-languages-tid, #edit-field-geography-und, #edit-field-supported-products-und, #edit-field-supported-languages-und, #edit-field-vendor-und, #edit-field-errata-type-text-und, #edit-field-errata-severity-text-und, #edit-field-software-partner-level-und, #edit-field-scert-product-category-und, #edit-field-certifications-und-0-field-product-und, #edit-kcs-article-type, #edit-field-eco-industry-tag-select-tid, #edit-field-eco-software-catego-select-tid, #edit-field-ecosystem-tag-select-und, #edit-field-eco-industry-tag-select-und, #edit-field-eco-software-catego-select-und, #edit-field-vendor-tsanet-member-ref-und, #edit-field-og-vendor-ref-und-0-default, #edit-field-eco-products-enabled-col-und-0-field-eco-subscription-model-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-support-level-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-archite-select-und, #edit-field-eco-certifications-select-und-0-field-eco-certificati-lvl-select-und, #edit-field-eco-certifications-select-und-0-field-eco-hypervisor-str-und, #edit-field-eco-supported-language-ref-und, #edit-field-eco-region-ref-und, #edit-field-cs-product-category-str-und, #edit-field-eco-certifications-select-und-0-field-eco-format-ref-und, #edit-field-eco-group-access-ref-und, #edit-field-hw-category-tag-ref-und, #edit-field-profile-industry-und, #edit-field-profile-tech-interests-und, #edit-field-product-page-features-ref-und, #edit-field-eco-product-select-und, #edit-field-base-product-ref-und, #edit-field-eco-product-archite-select-und, #edit-field-eco-format-ref-und, #edit-field-certification-status-ref-und, #edit-field-certification-result-ref-und, #edit-field-og-certified-product-ref-und-0-default, #edit-field-eco-subscription-model-ref-und, #edit-field-eco-support-level-ref-und, #edit-field-eco-certificati-lvl-select-und, #edit-field-eco-hypervisor-str-und, #edit-field-ccp-thirdparty-cert-select-und, .use_, #edit-field-eco-cert-product-tag-und, #edit-field-documentation-location-ref-und, #edit-field-documentation-title-und, #edit-field-accelerator-products-und","minimum":"0"},"rh_doc_fetcher":{"page_type":"single"},"section":"","kcs":{"nodeType":null,"nodeId":null}});</script>
    <!--[if lt IE 9]>
  <script src="https://access.redhat.com/sites/all/themes/kcs/js/html5shiv.js"></script>
  <![endif]-->
  
      


    <!--kcs06-->
<script type="text/javascript">
  Drupal.portal = {"version":{"redhat_portal":"package drupal7-redhat_portal is not installed"}};
  Drupal.portal.currentUser = {};
  </script>

</head>

<body class="portal-page  kcs_external" >
  
  <div id="page-wrap" class="page-wrap">
    <div class="top-page-wrap">

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <script>
    chrometwo_require(["wc"], function(wc){    
        wc.include("@cpelements/cp-search-autocomplete/dist/cp-search-autocomplete.umd");
    }); 
</script>

<!-- Accessibility Nav & Header -->
<div class="accessibility-nav sr-only">
    <a href="https://access.redhat.com/">
        <h1>Red Hat <span>Customer </span><span>Portal</span></h1>
    </a>
    <p><a href="index.html#cp-main">Skip to main content</a></p>
    <nav aria-labelledby="accessibility-nav-heading">
        <h2 id="accessibility-nav-heading">Main Navigation</h2>
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="accessibility-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="accessibility-accountUser">
                    <div class="account-name"><strong id="accessibility-userFullName"></strong></div>
                    <div class="account-org"><span id="accessibility-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="accessibility-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="accessibility-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="accessibility-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="accessibility-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="accessibility-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="accessibility-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- Mobile Header -->
<nav class="mobile-nav-bar hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <button id="menu-btn" class="menu menu-white" type="button" aria-label="Toggle Navigation">
        <span class="lines"></span>
    </button>
    <a class="logo" href="https://access.redhat.com/">
        <span class="logo-crop">
          <!-- logo -->
          <span class="sr-only">Red Hat Customer Portal</span>
          <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
            <defs>
            <style>
              .rh-logo-type {
                fill:#fff;
              }
              .rh-logo-hat {
                fill:#e00;
              }
            </style>
            </defs>
            <g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
          </svg>
        </span>
            </a>
    <button class="btn btn-search btn-utility" data-target="#site-search">
        <span class="web-icon-search"></span>
        <span class="link-text">Search</span>
    </button>
</nav>

<!-- Mobile Menu Drawer -->
<div class="nav-drawer mobile-nav-drawer hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <nav class="nav-container">
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="mobile-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="mobile-accountUser">
                    <div class="account-name"><strong id="mobile-userFullName"></strong></div>
                    <div class="account-org"><span id="mobile-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="mobile-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="mobile-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="mobile-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="mobile-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="mobile-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="mobile-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- White Utility Menu for large screens -->
<div class="utility-wrap" aria-hidden="true">
    <div class="utility-container">
        <div class="utility-bar hidden-xs">
            <div role="navigation" class="top-nav">
                <ul>
                    <li id="nav-subscription" data-portal-tour-1="1"><a class="top-nav-subscriptions" href="https://access.redhat.com/management/">Subscriptions</a></li>
                    <li id="nav-downloads" data-portal-tour-1="2"><a class="top-nav-downloads" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li id="nav-containers"><a class="top-nav-containers" href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
                    <li id="nav-support" data-portal-tour-1="3"><a class="top-nav-support-cases" href="https://access.redhat.com/support/cases/">Support Cases</a></li>
                </ul>
            </div>

            <div role="navigation" class="utility-nav">
                <ul>
                    <li id="searchABTestHide">
                        <a class="btn-search" data-target="#site-search" title="Search" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-search" aria-label="search"></span>
                            <span class="link-text">Search</span>
                        </a>
                    </li>
                    <!-- AB Test -->
                    <li id="searchABTestShow">
                        <form id="topSearchFormABTest" name="topSearchFormABTest">
                            <label for="topSearchInputABTest" class="sr-only">Search</label>
                            <input id="topSearchInputABTest" name="keyword" placeholder="Search" value="" type="text" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" class="form-control">
                            <button type="submit" class="btn btn-app btn-sm btn-link"><span class="web-icon-search" aria-label="search"></span></button>
                        </form>
                    </li>
                    <!-- End of AB Test -->
                    <li>
                        <a class="btn-profile" data-target="#account-info" data-portal-tour-1="4a" title="Account" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-user" aria-label="log in"></span>
                            <span class="link-text">Log In</span>
                            <span class="account-user" id="accountUserName"></span>
                        </a>
                    </li>
                    <li>
                        <a class="btn-language" data-target="#language" data-portal-tour-1="6" title="Language" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-globe" aria-label="language"></span>
                            <span class="link-text">Language</span>
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- Utility Tray -->
        <div class="utility-tray-container">
            <div class="utility-tray">
                <div id="site-search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="https://access.redhat.com/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>
                            <div class="input-group push-bottom">
                                <input class="form-control searchField" id="topSearchInput" name="keyword" value="" placeholder="Enter your search term" type="text">
                                <span class="input-group-btn">
                                    <button type="submit" class="btn btn-primary">Search</button>
                                </span>
                            </div>
                            <div>Or <a href="https://access.redhat.com/support/cases/#/troubleshoot">troubleshoot an issue</a>.</div>
                        </form>
                    </div>
                </div>
                <div id="account-info" class="utility-link account-info">
                    <div class="content">

                        <!-- Account Unauthenticated -->
                        <div id="accountLinksLoggedOut" class="unauthenticated">
                            <h2 class="utility-header">Log in to Your Red Hat Account</h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border">
                                    <p><a href="https://access.redhat.com/login" id="accountLogin" class="btn btn-primary">Log In</a></p>
                                    <p>Your Red Hat account gives you access to your profile, preferences, and services, depending on your status.</p>
                                </div>
                                <div class="col-sm-6 col-border col-border-left">
                                    <p><a href="https://www.redhat.com/wapps/ugc/register.html" class="btn btn-primary">Register</a></p>
                                    <p>If you are a new customer, register now for access to product evaluations and purchasing capabilities. </p>

                                    <strong>Need access to an account?</strong><p>If your company has an existing Red Hat account, your organization administrator can grant you access.</p>

                                    <p><a href="https://access.redhat.com/support/contact/customerService/">If you have any questions, please contact customer service.</a></p>
                                </div>
                            </div>
                        </div>

                        <!-- Account Authenticated -->
                        <div id="accountLinksLoggedIn" class="authenticated">
                            <h2 class="utility-header"><span id="userFirstName"></span></h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border col-border-right">
                                    <div class="account-info" id="accountUser">
                                        <div class="avatar"><!-- placeholder--></div>
                                        <div class="account-name"><strong id="userFullName"></strong></div>
                                        <div class="account-org"><span id="userOrg"></span></div>
                                        <div class="account-number accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                                    </div>

                                    <div class="row account-settings">
                                        <div class="col-md-6" data-portal-tour-1="4">
                                            <h3>Red Hat Account</h3>
                                            <ul class="reset">
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
                                                <!-- <li><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
                                                <li><a href="https://access.redhat.com/account-team">Account Team</a></li>
                                            </ul>
                                        </div>
                                        <div class="col-md-6" data-portal-tour-1="5">
                                            <h3>Customer Portal</h3>
                                            <ul class="reset">
                                                <li><a href="https://access.redhat.com/user">My Profile</a></li>
                                                <li><a href="https://access.redhat.com/user" id="userNotificationsLink">Notifications</a></li>
                                                <li><a href="https://access.redhat.com/help/">Help</a></li>
                                            </ul>
                                        </div>
                                    </div>

                                </div>
                                <div class="col-sm-6 col-border">
                                    <p>For your security, if you’re on a public computer and have finished using your Red Hat services, please be sure to log out.</p>
                                    <p><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout" id="accountLogout" class="btn btn-primary">Log Out</a></p>
                                </div>
                            </div>
                        </div>

                    </div>
                </div>
                <div id="language" class="utility-link language">
                    <div class="content">
                        <h2 class="utility-header">Select Your Language</h2>
                        <div class="row" id="localesMenu">
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en">English</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko">한국어</a></li>
                                </ul>
                            </div>
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja">日本語</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN">中文 (中国)</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div id="scroll-anchor"></div>

<!-- Main Menu for large screens -->
<div class="header-nav visible-sm visible-md visible-lg" aria-hidden="true">
    <div id="header-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">

                    <a href="https://access.redhat.com/" class="logo">
                      <span class="sr-only">Red Hat Customer Portal</span>
                      <span class="logo-crop">
                        <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
                          <defs>
                          <style>
                            .rh-logo-type {
                              fill:#fff;
                            }
                            .rh-logo-hat {
                              fill:#e00;
                            }
                          </style>
                          </defs>
                          <title>Red Hat Customer Portal</title><g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
                        </svg>
                      </span>
                                            </a>

                    <nav class="primary-nav hidden-sm">
                        <ul>
                            <li id="nav-products"><a class="products" data-link="mega" data-target="products-menu" href="https://access.redhat.com/products/" id="products-menu">Products &amp; Services</a></li>
                            <li id="nav-tools"><a class="tools" data-link="mega" data-target="tools-menu" href="https://access.redhat.com/labs/" id="tools-menu">Tools</a></li>
                            <li id="nav-security"><a class="security" data-link="mega" data-target="security-menu" href="https://access.redhat.com/security/" id="security-menu">Security</a></li>
                            <li id="nav-community"><a class="community" data-link="mega" data-target="community-menu" href="https://access.redhat.com/community/" id="community-menu">Community</a></li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Main Mega Menu for large screens -->
<div class="mega-wrap visible-sm visible-md visible-lg" aria-hidden="true">
    <nav class="mega">
        <div class="container">
            <div class="mega-menu-wrap">

                <!-- Products Menu -->
                <div aria-labelledby="products-menu" class="products-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-md-6 col-sm-8">
                            <div class="root clearfix" data-portal-tour-1="7">
                                <ul class="subnav subnav-root">
                                    <li data-target="infrastructure-menu" class="active">
                                        <h3>Infrastructure and Management</h3>
                                    </li>
                                    <li data-target="cloud-menu">
                                        <h3>Cloud Computing</h3>
                                    </li>
                                    <li data-target="storage-menu">
                                        <h3>Storage</h3>
                                    </li>
                                    <li data-target="jboss-dev-menu">
                                        <h3>Runtimes</h3>
                                    </li>
                                    <li data-target="jboss-int-menu">
                                        <h3>Integration and Automation</h3>
                                    </li>
                                </ul>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="infrastructure-menu" style="display: block;">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="cloud-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="storage-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a>
                                        </li>
                                        <li>
                                          <a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-dev-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                                        </li>
                                    </ul>
                                </div>
                                <!-- <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                        </li>
                                    </ul>
                                </div> -->
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul class="border-bottom" id="portal-menu-border-bottom">
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                      </li>
                                    </ul>
                                    <ul>
                                      <li>
                                        <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                      </li>
                                    </ul>
                                </div>
                            </div>
                            <a href="https://access.redhat.com/products" class="btn btn-primary">View All Products</a>
                        </div>

                        <div class="col-md-6 col-sm-4 pull-right" data-portal-tour-1="8">
                            <div class="row">
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/support" class="cta-link cta-link-darkbg">Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
                                        <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>
                                    </ul>
                                    <h4 class="nav-title">Services</h4>
                                    <ul>
                                        <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
                                        <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>
                                    </ul>
                                </div>
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/documentation" class="cta-link cta-link-darkbg">Documentation</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>
                                    </ul>
                                    <ul>
                                        
                                        <li><a href="https://catalog.redhat.com/" class="cta-link cta-link-darkbg">Ecosystem Catalog</a></li>
                                        <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
                                        <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Tools Menu -->
                <div aria-labelledby="tools-menu" class="tools-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row" data-portal-tour-1="9">
                        <div class="col-sm-12">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h3 class="nav-title">Tools</h3>
                                    <ul>
                                        <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
                                        <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
                                        <li><a href="https://access.redhat.com/errata/">Errata</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <ul class="list-flat">
                                        <li><a href="https://access.redhat.com/labs/" class="cta-link cta-link-darkbg">Customer Portal Labs</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <div class="card card-dark-grey">
                                        <h4 class="card-heading">Red Hat Insights</h4>
                                        <p class="text-white">Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                        <ul class="list-flat rh-l-grid rh-m-gutters rh-m-all-6-col-on-md">
                                          <li><a href="https://www.redhat.com/en/technologies/management/insights" class="cta-link cta-link-md cta-link-darkbg">Learn more</a></li>
                                          <li><a href="https://cloud.redhat.com/insights" class="cta-link cta-link-md cta-link-darkbg">Go to Insights</a></li>
                                        </ul>
                                    </div>
                                </div>

                            </div>
                        </div>

                    </div>
                </div>

                <!-- Security Menu -->
                <div aria-labelledby="security-menu" class="security-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="10">

                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Red Hat Product Security Center</h2>
                                    <p class="text-white">Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.</p>
                                    <p><a href="https://access.redhat.com/security/" class="btn btn-primary">Product Security Center</a></p>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Security Updates</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
                                    </ul>
                                    <p class="text-white">Keep your systems secure with Red Hat&#039;s specialized responses for high-priority security vulnerabilities.</p>
                                    <ul>
                                        <li class="more-link"><a href="https://access.redhat.com/security/vulnerability">View Responses</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Resources</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
                                        <li><a href="https://www.redhat.com/en/blog/channel/security">Security Blog</a></li>
                                        <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
                                        <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Community Menu -->
                <div aria-labelledby="community-menu" class="community-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="11">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Portal Community</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
                                        <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
                                        <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
                                        <p class="push-top"><a href="https://access.redhat.com/community/" class="btn btn-primary">Community Activity</a></p>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Events</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
                                        <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Stories</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
                                        <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
                                        <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <!--<a href="https://access.redhat.com/community/" class="btn btn-primary">Explore Community</a>-->
                    </div>
                </div>
            </div>
        </div>
    </nav>
</div>

            <!--[if IE 8]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
            </div>
            <![endif]-->
            <!--[if IE 9]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
            </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">
  <script type="text/javascript">
    chrometwo_require(['jquery-ui']);
  </script>


      <article class="rh_docs">
  <div class="container">
        <!-- Display: Book Page Content -->
    
  

<div class="row">
  <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
  <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
  


<a class="toc-toggle toc-show" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
  <span class="sr-only">Show Table of Contents</span>
  <span class="web-icon-mobile-menu" aria-hidden="true"></span>
</a>
<nav id="toc-main" class="toc-main collapse in">
  <div class="toc-menu affix-top">
    <a class="toc-toggle toc-hide" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
      <span class="sr-only">Hide Table of Contents</span>
      <span class="icon-remove" aria-hidden="true"></span>
    </a>
    <div class="doc-options">
      <div class="doc-language btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          English <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="index.html">English</a></li>
                      <li><a href="https://access.redhat.com/documentation/ja-jp/red_hat_openstack_platform/16.1/html/director_installation_and_usage/">日本語</a></li>
                      <li><a href="https://access.redhat.com/documentation/ko-kr/red_hat_openstack_platform/16.1/html/director_installation_and_usage/">한국어</a></li>
                      <li><a href="https://access.redhat.com/documentation/zh-cn/red_hat_openstack_platform/16.1/html/director_installation_and_usage/">简体中文</a></li>
                  </ul>
      </div>
      <div class="doc-format btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          Single-page HTML <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/">Multi-page HTML</a></li>
                                <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/pdf/director_installation_and_usage/Red_Hat_OpenStack_Platform-16.1-Director_Installation_and_Usage-en-US.pdf">PDF</a></li>
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/epub/director_installation_and_usage/Red_Hat_OpenStack_Platform-16.1-Director_Installation_and_Usage-en-US.epub">ePub</a></li>
                  </ul>
      </div>
    </div>
    <ol class="menu"><li class=" leaf"><a href="index.html">Director Installation and Usage</a></li><li class=" leaf"><a href="index.html#chap-Introduction">1. Introduction to director</a><ol class="menu"><li class=" leaf"><a href="index.html#sect-Undercloud">1.1. Undercloud</a></li><li class=" leaf"><a href="index.html#sect-Overcloud">1.2. Understanding the overcloud</a></li><li class=" leaf"><a href="index.html#sect-High_Availability">1.3. Understanding high availability in Red Hat OpenStack Platform</a></li><li class=" leaf"><a href="index.html#sect-Containerization">1.4. Understanding containerization in Red Hat OpenStack Platform</a></li><li class=" leaf"><a href="index.html#sect-Ceph_Storage">1.5. Working with Ceph Storage in Red Hat OpenStack Platform</a></li></ol></li><li class=" leaf"><a href="index.html#director_installation_and_configuration">I. Director installation and configuration</a><ol class="menu"><li class=" leaf"><a href="index.html#planning-your-undercloud">2. Planning your undercloud</a><ol class="menu"><li class=" leaf"><a href="index.html#containerised-undercloud">2.1. Containerized undercloud</a></li><li class=" leaf"><a href="index.html#preparing-your-network">2.2. Preparing your undercloud networking</a></li><li class=" leaf"><a href="index.html#determining-environment-scale">2.3. Determining environment scale</a></li><li class=" leaf"><a href="index.html#undercloud-disk-sizing">2.4. Undercloud disk sizing</a></li><li class=" leaf"><a href="index.html#virtualization-support">2.5. Virtualization support</a></li><li class=" leaf"><a href="index.html#character-encoding-configuration-osp">2.6. Character encoding configuration</a></li><li class=" leaf"><a href="index.html#undercloud-repositories">2.7. Undercloud repositories</a></li></ol></li><li class=" leaf"><a href="index.html#preparing-for-director-installation">3. Preparing for director installation</a><ol class="menu"><li class=" leaf"><a href="index.html#preparing-the-undercloud">3.1. Preparing the undercloud</a></li><li class=" leaf"><a href="index.html#registering-the-undercloud-and-attaching-subscriptions">3.2. Registering the undercloud and attaching subscriptions</a></li><li class=" leaf"><a href="index.html#enabling-repositories-for-the-undercloud">3.3. Enabling repositories for the undercloud</a></li><li class=" leaf"><a href="index.html#configuring-an-undercloud-proxy">3.4. Configuring an undercloud proxy</a></li><li class=" leaf"><a href="index.html#installing_director_packages">3.5. Installing director packages</a></li><li class=" leaf"><a href="index.html#installing-ceph-ansible">3.6. Installing ceph-ansible</a></li><li class=" leaf"><a href="index.html#preparing-container-images">3.7. Preparing container images</a></li><li class=" leaf"><a href="index.html#container-image-preparation-parameters">3.8. Container image preparation parameters</a></li><li class=" leaf"><a href="index.html#layering-image-preparation-entries">3.9. Layering image preparation entries</a></li><li class=" leaf"><a href="index.html#obtaining-container-images-from-private-registries">3.10. Obtaining container images from private registries</a></li><li class=" leaf"><a href="index.html#modifying-images-during-preparation">3.11. Modifying images during preparation</a></li><li class=" leaf"><a href="index.html#updating-existing-packages-on-container-images">3.12. Updating existing packages on container images</a></li><li class=" leaf"><a href="index.html#installing-additional-rpm-files-to-container-images">3.13. Installing additional RPM files to container images</a></li><li class=" leaf"><a href="index.html#modifying-container-images-with-a-custom-dockerfile">3.14. Modifying container images with a custom Dockerfile</a></li><li class=" leaf"><a href="index.html#preparing-a-satellite-server-for-container-images">3.15. Preparing a Satellite server for container images</a></li></ol></li><li class=" leaf"><a href="index.html#installing-the-undercloud">4. Installing director</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring-director">4.1. Configuring director</a></li><li class=" leaf"><a href="index.html#director-configuration-parameters">4.2. Director configuration parameters</a></li><li class=" leaf"><a href="index.html#configuring-the-undercloud-with-environment-files">4.3. Configuring the undercloud with environment files</a></li><li class=" leaf"><a href="index.html#common-heat-parameters-for-undercloud-configuration">4.4. Common heat parameters for undercloud configuration</a></li><li class=" leaf"><a href="index.html#configuring-hieradata-on-the-undercloud">4.5. Configuring hieradata on the undercloud</a></li><li class=" leaf"><a href="index.html#configuring-the-undercloud-for-bare-metal-provisioning-over-ipv6">4.6. Configuring the undercloud for bare metal provisioning over IPv6</a></li><li class=" leaf"><a href="index.html#installing-director">4.7. Installing director</a></li><li class=" leaf"><a href="index.html#sect-Obtaining_Images_for_Overcloud_Nodes">4.8. Obtaining images for overcloud nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#single_cpu_architecture_overclouds">4.8.1. Single CPU architecture overclouds</a></li><li class=" leaf"><a href="index.html#multiple_cpu_architecture_overclouds">4.8.2. Multiple CPU architecture overclouds</a></li><li class=" leaf"><a href="index.html#sect-obtaining-overcloud-minimal-image">4.8.3. Minimal overcloud image</a></li></ol></li><li class=" leaf"><a href="index.html#sect-Setting_a_Nameserver_on_the_Underclouds_Neutron_Subnet">4.9. Setting a nameserver for the control plane</a></li><li class=" leaf"><a href="index.html#installing-the-undercloud-configuration">4.10. Updating the undercloud configuration</a></li><li class=" leaf"><a href="index.html#undercloud-container-registry">4.11. Undercloud container registry</a></li><li class=" leaf"><a href="index.html#next_steps">4.12. Next steps</a></li></ol></li><li class=" leaf"><a href="index.html#installing-undercloud-minions">5. Installing undercloud minions</a><ol class="menu"><li class=" leaf"><a href="index.html#undercloud-minion">5.1. Undercloud minion</a></li><li class=" leaf"><a href="index.html#undercloud-minion-requirements">5.2. Undercloud minion requirements</a></li><li class=" leaf"><a href="index.html#preparing-the-minion">5.3. Preparing a minion</a></li><li class=" leaf"><a href="index.html#copying-the-undercloud-configuration-files-to-the-minion">5.4. Copying the undercloud configuration files to the minion</a></li><li class=" leaf"><a href="index.html#copying-the-undercloud-certificate-authority">5.5. Copying the undercloud certificate authority</a></li><li class=" leaf"><a href="index.html#configuring-the-minion">5.6. Configuring the minion</a></li><li class=" leaf"><a href="index.html#minion-configuration-parameters">5.7. Minion configuration parameters</a></li><li class=" leaf"><a href="index.html#installing-the-minion">5.8. Installing the minion</a></li><li class=" leaf"><a href="index.html#verifying-the-minion-installation">5.9. Verifying the minion installation</a></li><li class=" leaf"><a href="index.html#next_steps_2">5.10. Next Steps</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#basic_overcloud_deployment">II. Basic overcloud deployment</a><ol class="menu"><li class=" leaf"><a href="index.html#planning-your-Overcloud">6. Planning your overcloud</a><ol class="menu"><li class=" leaf"><a href="index.html#node-roles">6.1. Node roles</a></li><li class=" leaf"><a href="index.html#overcloud-networks">6.2. Overcloud networks</a></li><li class=" leaf"><a href="index.html#overcloud-storage">6.3. Overcloud storage</a></li><li class=" leaf"><a href="index.html#overcloud-security">6.4. Overcloud security</a></li><li class=" leaf"><a href="index.html#sect-planning-HA">6.5. Overcloud high availability</a></li><li class=" leaf"><a href="index.html#controller-node-requirements">6.6. Controller node requirements</a></li><li class=" leaf"><a href="index.html#compute-node-requirements">6.7. Compute node requirements</a></li><li class=" leaf"><a href="index.html#ceph-storage-node-requirements">6.8. Ceph Storage node requirements</a></li><li class=" leaf"><a href="index.html#object-storage-node-requirements">6.9. Object Storage node requirements</a></li><li class=" leaf"><a href="index.html#overcloud-repositories">6.10. Overcloud repositories</a></li><li class=" leaf"><a href="index.html#Provisioning-methods">6.11. Provisioning methods</a></li></ol></li><li class=" leaf"><a href="index.html#creating-a-basic-overcloud-with-cli-tools">7. Configuring a basic overcloud with CLI tools</a><ol class="menu"><li class=" leaf"><a href="index.html#sect-Registering_Nodes_for_the_Overcloud-basic">7.1. Registering nodes for the overcloud</a></li><li class=" leaf"><a href="index.html#validating-the-introspection-requirements">7.2. Validating the introspection requirements</a></li><li class=" leaf"><a href="index.html#inspecting-the-hardware-of-nodes-basic">7.3. Inspecting the hardware of nodes</a></li><li class=" leaf"><a href="index.html#tagging-nodes-into-profiles">7.4. Tagging nodes into profiles</a></li><li class=" leaf"><a href="index.html#setting-uefi-boot-mode">7.5. Setting UEFI boot mode</a></li><li class=" leaf"><a href="index.html#enabling-virtual-media-boot_basic">7.6. Enabling virtual media boot</a></li><li class=" leaf"><a href="index.html#defining-the-root-disk">7.7. Defining the root disk for multi-disk clusters</a></li><li class=" leaf"><a href="index.html#using-the-overcloud-minimal-image-to-avoid-using-a-Red-Hat-subscription-entitlement">7.8. Using the overcloud-minimal image to avoid using a Red Hat subscription entitlement</a></li><li class=" leaf"><a href="index.html#creating-architecture-specific-roles">7.9. Creating architecture specific roles</a></li><li class=" leaf"><a href="index.html#environment-files">7.10. Environment files</a></li><li class=" leaf"><a href="index.html#creating-an-environment-file-that-defines-node-counts-and-flavors">7.11. Creating an environment file that defines node counts and flavors</a></li><li class=" leaf"><a href="index.html#creating-an-environment-file-for-undercloud-ca-trust">7.12. Creating an environment file for undercloud CA trust</a></li><li class=" leaf"><a href="index.html#deployment-command">7.13. Deployment command</a></li><li class=" leaf"><a href="index.html#deployment-command-options">7.14. Deployment command options</a></li><li class=" leaf"><a href="index.html#sect-Including_Environment_Files_in_Overcloud_Creation">7.15. Including environment files in an overcloud deployment</a></li><li class=" leaf"><a href="index.html#validating-the-deployment-requirements">7.16. Validating the deployment requirements</a></li><li class=" leaf"><a href="index.html#overcloud-deployment-output-basic">7.17. Overcloud deployment output</a></li><li class=" leaf"><a href="index.html#accessing-the-overcloud-basic">7.18. Accessing the overcloud</a></li><li class=" leaf"><a href="index.html#validating-the-post-deployment-state">7.19. Validating the post-deployment state</a></li><li class=" leaf"><a href="index.html#next_steps_3">7.20. Next steps</a></li></ol></li><li class=" leaf"><a href="index.html#provisioning-bare-metal-nodes-before-deploying-the-overcloud">8. Provisioning bare metal nodes before deploying the overcloud</a><ol class="menu"><li class=" leaf"><a href="index.html#sect-Registering_Nodes_for_the_Overcloud-novaless-provisioning">8.1. Registering nodes for the overcloud</a></li><li class=" leaf"><a href="index.html#inspecting-the-hardware-of-nodes-novaless-provisioning">8.2. Inspecting the hardware of nodes</a></li><li class=" leaf"><a href="index.html#provisioning-bare-metal-nodes">8.3. Provisioning bare metal nodes</a></li><li class=" leaf"><a href="index.html#scaling-up-bare-metal-nodes">8.4. Scaling up bare metal nodes</a></li><li class=" leaf"><a href="index.html#scaling-down-bare-metal-nodes">8.5. Scaling down bare metal nodes</a></li><li class=" leaf"><a href="index.html#bare-metal-node-provisioning-attributes">8.6. Bare metal node provisioning attributes</a></li></ol></li><li class=" leaf"><a href="index.html#configuring-a-basic-overcloud-with-pre-provisioned-nodes">9. Configuring a basic overcloud with pre-provisioned nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#pre-provisioned-node-requirements">9.1. Pre-provisioned node requirements</a></li><li class=" leaf"><a href="index.html#creating-a-user-on-pre-provisioned-nodes">9.2. Creating a user on pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#registering-the-operating-system-for-pre-provisioned-nodes">9.3. Registering the operating system for pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#sect-Configuring_SSLTLS_on_Nodes">9.4. Configuring SSL/TLS access to director</a></li><li class=" leaf"><a href="index.html#configuring-networking-for-the-control-plane">9.5. Configuring networking for the control plane</a></li><li class=" leaf"><a href="index.html#using-a-separate-network-for-pre-provisioned-nodes">9.6. Using a separate network for pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#mapping-pre-provisioned-node-hostnames">9.7. Mapping pre-provisioned node hostnames</a></li><li class=" leaf"><a href="index.html#configuring_ceph_storage_for_pre_provisioned_nodes">9.8. Configuring Ceph Storage for pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#creating-the-overcloud-with-pre-provisioned-nodes">9.9. Creating the overcloud with pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#overcloud-deployment-output-preprovisioned">9.10. Overcloud deployment output</a></li><li class=" leaf"><a href="index.html#accessing-the-overcloud-preprovisioned">9.11. Accessing the overcloud</a></li><li class=" leaf"><a href="index.html#scaling-pre-provisioned-nodes">9.12. Scaling pre-provisioned nodes</a></li><li class=" leaf"><a href="index.html#sect-Deleting_a_Pre-Provisioned_Overcloud">9.13. Removing a pre-provisioned overcloud</a></li><li class=" leaf"><a href="index.html#sect-Completing_the_Overcloud_Creation_Pre_Provisioned">9.14. Next steps</a></li></ol></li><li class=" leaf"><a href="index.html#deploying-multiple-overclouds">10. Deploying multiple overclouds</a><ol class="menu"><li class=" leaf"><a href="index.html#deploying-additional-overclouds">10.1. Deploying additional overclouds</a></li><li class=" leaf"><a href="index.html#managing-multiple-overclouds">10.2. Managing multiple overclouds</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#post_deployment_operations">III. Post deployment operations</a><ol class="menu"><li class=" leaf"><a href="index.html#performing-overcloud-post-installation-tasks">11. Performing overcloud post-installation tasks</a><ol class="menu"><li class=" leaf"><a href="index.html#checking-overcloud-deployment-status">11.1. Checking overcloud deployment status</a></li><li class=" leaf"><a href="index.html#sect-Creating-basic-overcloud-flavors">11.2. Creating basic overcloud flavors</a></li><li class=" leaf"><a href="index.html#creating-a-default-tenant-network">11.3. Creating a default tenant network</a></li><li class=" leaf"><a href="index.html#creating-a-default-floating-ip-network">11.4. Creating a default floating IP network</a></li><li class=" leaf"><a href="index.html#creating-a-default-provider-network">11.5. Creating a default provider network</a></li><li class=" leaf"><a href="index.html#creating-additional-bridge-mappings">11.6. Creating additional bridge mappings</a></li><li class=" leaf"><a href="index.html#validating-the-overcloud">11.7. Validating the overcloud</a></li><li class=" leaf"><a href="index.html#protecting-the-overcloud-from-removal">11.8. Protecting the overcloud from removal</a></li></ol></li><li class=" leaf"><a href="index.html#performing-basic-overcloud-administration-tasks">12. Performing basic overcloud administration tasks</a><ol class="menu"><li class=" leaf"><a href="index.html#managing-containerized-services-temp">12.1. Managing containerized services</a></li><li class=" leaf"><a href="index.html#modifying-the-overcloud-environment">12.2. Modifying the overcloud environment</a></li><li class=" leaf"><a href="index.html#importing-virtual-machines-into-the-overcloud">12.3. Importing virtual machines into the overcloud</a></li><li class=" leaf"><a href="index.html#running-the-dynamic-inventory-script">12.4. Running the dynamic inventory script</a></li><li class=" leaf"><a href="index.html#removing-the-overcloud">12.5. Removing the overcloud</a></li></ol></li><li class=" leaf"><a href="index.html#configuring-the-overcloud-with-ansible">13. Configuring the overcloud with Ansible</a><ol class="menu"><li class=" leaf"><a href="index.html#ansible-based-overcloud-configuration-config-download">13.1. Ansible-based overcloud configuration (config-download)</a></li><li class=" leaf"><a href="index.html#config-download-working-directory">13.2. config-download working directory</a></li><li class=" leaf"><a href="index.html#enabling-access-to-config-download-working-directories">13.3. Enabling access to config-download working directories</a></li><li class=" leaf"><a href="index.html#checking-config-download-logs">13.4. Checking config-download log</a></li><li class=" leaf"><a href="index.html#separating-the-provisioning-and-configuration-processes">13.5. Separating the provisioning and configuration processes</a></li><li class=" leaf"><a href="index.html#running-config-download-manually">13.6. Running config-download manually</a></li><li class=" leaf"><a href="index.html#performing-git-operations-on-the-working-directory">13.7. Performing Git operations on the working directory</a></li><li class=" leaf"><a href="index.html#creating-config-download-files-manually">13.8. Creating config-download files manually</a></li><li class=" leaf"><a href="index.html#config-download-top-level-files">13.9. config-download top level files</a></li><li class=" leaf"><a href="index.html#config-download-tags">13.10. config-download tags</a></li><li class=" leaf"><a href="index.html#config-download-deployment-steps">13.11. config-download deployment steps</a></li><li class=" leaf"><a href="index.html#next_steps_4">13.12. Next Steps</a></li></ol></li><li class=" leaf"><a href="index.html#managing-containers-with-ansible_preprovisioned">14. Managing containers with Ansible</a><ol class="menu"><li class=" leaf"><a href="index.html#enabling-the-tripleo-container-manage-ansible-role-on-the-undercloud_managing-containers-with-ansible">14.1. Enabling the tripleo-container-manage Ansible role on the undercloud</a></li><li class=" leaf"><a href="index.html#enabling-the-tripleo-container-manage-ansible-role-on-the-overcloud_managing-containers-with-ansible">14.2. Enabling the tripleo-container-manage Ansible role on the overcloud</a></li><li class=" leaf"><a href="index.html#performing-operations-on-a-single-container_managing-containers-with-ansible">14.3. Performing operations on a single container</a></li><li class=" leaf"><a href="index.html#tripleo-container-manage-role-variables_managing-containers-with-ansible">14.4. tripleo-container-manage role variables</a></li></ol></li><li class=" leaf"><a href="index.html#using-the-validation-framework">15. Using the validation framework</a><ol class="menu"><li class=" leaf"><a href="index.html#ansible-based-validations">15.1. Ansible-based validations</a></li><li class=" leaf"><a href="index.html#listing-validations">15.2. Listing validations</a></li><li class=" leaf"><a href="index.html#running-validations">15.3. Running validations</a></li><li class=" leaf"><a href="index.html#in-flight-validations">15.4. In-flight validations</a></li></ol></li><li class=" leaf"><a href="index.html#scaling-overcloud-nodes">16. Scaling overcloud nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#adding-nodes-to-the-overcloud">16.1. Adding nodes to the overcloud</a></li><li class=" leaf"><a href="index.html#increasing-node-counts-for-roles">16.2. Increasing node counts for roles</a></li><li class=" leaf"><a href="index.html#removing-compute-nodes">16.3. Removing Compute nodes</a></li><li class=" leaf"><a href="index.html#replacing-ceph-storage-nodes">16.4. Replacing Ceph Storage nodes</a></li><li class=" leaf"><a href="index.html#replacing-object-storage-nodes">16.5. Replacing Object Storage nodes</a></li><li class=" leaf"><a href="index.html#blacklisting-nodes">16.6. Blacklisting nodes</a></li></ol></li><li class=" leaf"><a href="index.html#replacing-controller-nodes">17. Replacing Controller nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#preparing-for-controller-replacement">17.1. Preparing for Controller replacement</a></li><li class=" leaf"><a href="index.html#removing-a-ceph-monitor-daemon">17.2. Removing a Ceph Monitor daemon</a></li><li class=" leaf"><a href="index.html#preparing-the-cluster-for-controller-replacement">17.3. Preparing the cluster for Controller node replacement</a></li><li class=" leaf"><a href="index.html#replacing-a-controller-node">17.4. Replacing a Controller node</a></li><li class=" leaf"><a href="index.html#triggering-the-rode-replacement">17.5. Triggering the Controller node replacement</a></li><li class=" leaf"><a href="index.html#cleaning-up-after-the-controller-node-replacement">17.6. Cleaning up after Controller node replacement</a></li></ol></li><li class=" leaf"><a href="index.html#rebooting-nodes">18. Rebooting nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#rebooting_the_undercloud">18.1. Rebooting the undercloud node</a></li><li class=" leaf"><a href="index.html#rebooting_controller_nodes">18.2. Rebooting Controller and composable nodes</a></li><li class=" leaf"><a href="index.html#rebooting_standalone_ceph_mon_nodes">18.3. Rebooting standalone Ceph MON nodes</a></li><li class=" leaf"><a href="index.html#rebooting_a_ceph_storage_cluster">18.4. Rebooting a Ceph Storage (OSD) cluster</a></li><li class=" leaf"><a href="index.html#rebooting_compute_nodes">18.5. Rebooting Compute nodes</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#additional_director_operations_and_configuration">IV. Additional director operations and configuration</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring-custom-ssl-tls-certificates">19. Configuring custom SSL/TLS certificates</a><ol class="menu"><li class=" leaf"><a href="index.html#initializing-the-signing-host">19.1. Initializing the signing host</a></li><li class=" leaf"><a href="index.html#creating-a-certificate-authority">19.2. Creating a certificate authority</a></li><li class=" leaf"><a href="index.html#adding-the-certificate-authority-to-clients">19.3. Adding the certificate authority to clients</a></li><li class=" leaf"><a href="index.html#creating-an-ssl-tls-key">19.4. Creating an SSL/TLS key</a></li><li class=" leaf"><a href="index.html#creating-an-ssl-tls-certificate-signing-request">19.5. Creating an SSL/TLS certificate signing request</a></li><li class=" leaf"><a href="index.html#creating-the-ssl-tls-certificate">19.6. Creating the SSL/TLS certificate</a></li><li class=" leaf"><a href="index.html#adding-the-certificate-to-the-undercloud">19.7. Adding the certificate to the undercloud</a></li></ol></li><li class=" leaf"><a href="index.html#additional-introspection-operations">20. Additional introspection operations</a><ol class="menu"><li class=" leaf"><a href="index.html#performing_individual_node_introspection">20.1. Performing individual node introspection</a></li><li class=" leaf"><a href="index.html#performing_node_introspection_after_initial_introspection">20.2. Performing node introspection after initial introspection</a></li><li class=" leaf"><a href="index.html#performing_network_introspection_for_interface_information">20.3. Performing network introspection for interface information</a></li></ol></li><li class=" leaf"><a href="index.html#automatically-discover-bare-metal-nodes">21. Automatically discovering bare metal nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#prerequisites">21.1. Prerequisites</a></li><li class=" leaf"><a href="index.html#enabling_auto_discovery">21.2. Enabling auto-discovery</a></li><li class=" leaf"><a href="index.html#testing_auto_discovery">21.3. Testing auto-discovery</a></li><li class=" leaf"><a href="index.html#using_rules_to_discover_different_vendor_hardware">21.4. Using rules to discover different vendor hardware</a></li></ol></li><li class=" leaf"><a href="index.html#configuring-automatic-profile-tagging">22. Configuring automatic profile tagging</a><ol class="menu"><li class=" leaf"><a href="index.html#policy-file-syntax">22.1. Policy file syntax</a></li><li class=" leaf"><a href="index.html#policy-file-example">22.2. Policy file example</a></li><li class=" leaf"><a href="index.html#importing-policy-files">22.3. Importing policy files</a></li></ol></li><li class=" leaf"><a href="index.html#creating-whole-disk-images">23. Creating whole disk images</a><ol class="menu"><li class=" leaf"><a href="index.html#security-hardening-measures">23.1. Security hardening measures</a></li><li class=" leaf"><a href="index.html#whole-disk-image-workflow">23.2. Whole disk image workflow</a></li><li class=" leaf"><a href="index.html#downloading-the-base-cloud-image">23.3. Downloading the base cloud image</a></li><li class=" leaf"><a href="index.html#disk-image-environment-variables">23.4. Disk image environment variables</a></li><li class=" leaf"><a href="index.html#customizing-the-disk-layout">23.5. Customizing the disk layout</a></li><li class=" leaf"><a href="index.html#modifying-the-partitioning-schema">23.6. Modifying the partitioning schema</a></li><li class=" leaf"><a href="index.html#modifying-the-image-size">23.7. Modifying the image size</a></li><li class=" leaf"><a href="index.html#building-the-whole-disk-image">23.8. Building the whole disk image</a></li><li class=" leaf"><a href="index.html#uploading-the-whole-disk-image">23.9. Uploading the whole disk image</a></li></ol></li><li class=" leaf"><a href="index.html#configuring_direct_deploy">24. Configuring Direct Deploy</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring-direct-deploy-on-the-undercloud">24.1. Configuring the direct deploy interface on the undercloud</a></li></ol></li><li class=" leaf"><a href="index.html#creating-virtualized-control-planes">25. Creating virtualized control planes</a><ol class="menu"><li class=" leaf"><a href="index.html#virtualized-control-planes">25.1. Virtualized control plane architecture</a></li><li class=" leaf"><a href="index.html#benefits_and_limitations_of_virtualizing_your_rhosp_overcloud_control_plane">25.2. Benefits and limitations of virtualizing your RHOSP overcloud control plane</a></li><li class=" leaf"><a href="index.html#provisioning-virtualized-controllers-using-RHV-driver">25.3. Provisioning virtualized controllers using the Red Hat Virtualization driver</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#troubleshooting_and_tips">V. Troubleshooting and tips</a><ol class="menu"><li class=" leaf"><a href="index.html#troubleshooting-director-errors">26. Troubleshooting director errors</a><ol class="menu"><li class=" leaf"><a href="index.html#troubleshooting-node-registration">26.1. Troubleshooting node registration</a></li><li class=" leaf"><a href="index.html#troubleshooting-hardware-introspection">26.2. Troubleshooting hardware introspection</a></li><li class=" leaf"><a href="index.html#troubleshooting-workflows-and-executions">26.3. Troubleshooting workflows and executions</a></li><li class=" leaf"><a href="index.html#troubleshooting-overcloud-creation-and-deployment">26.4. Troubleshooting overcloud creation and deployment</a></li><li class=" leaf"><a href="index.html#troubleshooting-node-provisioning">26.5. Troubleshooting node provisioning</a></li><li class=" leaf"><a href="index.html#sect-Troubleshooting_IP_Address_Conflicts_on_the_Provisioning_Network">26.6. Troubleshooting IP address conflicts during provisioning</a></li><li class=" leaf"><a href="index.html#sect-Troubleshooting_No_Valid_Host_Found_Errors">26.7. Troubleshooting &quot;No Valid Host Found&quot; errors</a></li><li class=" leaf"><a href="index.html#troubleshooting-overcloud-configuration">26.8. Troubleshooting overcloud configuration</a></li><li class=" leaf"><a href="index.html#troubleshooting-container-configuration">26.9. Troubleshooting container configuration</a></li><li class=" leaf"><a href="index.html#sect-Compute_Service_Failures">26.10. Troubleshooting Compute node failures</a></li><li class=" leaf"><a href="index.html#creating-an-sosreport">26.11. Creating an sosreport</a></li><li class=" leaf"><a href="index.html#log-locations">26.12. Log locations</a></li></ol></li><li class=" leaf"><a href="index.html#tips-for-undercloud-and-overcloud-services">27. Tips for undercloud and overcloud services</a><ol class="menu"><li class=" leaf"><a href="index.html#review-the-database-flush-intervals">27.1. Review the database flush intervals</a></li><li class=" leaf"><a href="index.html#tuning-deployment-performance">27.2. Tuning deployment performance</a></li><li class=" leaf"><a href="index.html#running-swift-ringbuilder-in-a-container">27.3. Running swift-ring-builder in a container</a></li><li class=" leaf"><a href="index.html#changing-the-ssl-tls-cipher-rules-for-haproxy">27.4. Changing the SSL/TLS cipher rules for HAProxy</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#appendices">VI. Appendices</a><ol class="menu"><li class=" leaf"><a href="index.html#appe-Power_Management_Drivers">A. Power management drivers</a><ol class="menu"><li class=" leaf"><a href="index.html#sect-IPMI">A.1. Intelligent Platform Management Interface (IPMI)</a></li><li class=" leaf"><a href="index.html#sect-Redfish">A.2. Redfish</a></li><li class=" leaf"><a href="index.html#sect-Dell_Remote_Access_Controller_DRAC">A.3. Dell Remote Access Controller (DRAC)</a></li><li class=" leaf"><a href="index.html#sect-Integrated_LightsOut_iLO">A.4. Integrated Lights-Out (iLO)</a></li><li class=" leaf"><a href="index.html#sect-Fujitsu_Integrated_Remote_Management_Controller_iRMC">A.5. Fujitsu Integrated Remote Management Controller (iRMC)</a></li><li class=" leaf"><a href="index.html#sect-Red_Hat_Virtualization">A.6. Red Hat Virtualization</a></li><li class=" leaf"><a href="index.html#sect-manual-management-driver">A.7. manual-management Driver</a></li></ol></li><li class=" leaf"><a href="index.html#appe-OSP_on_POWER">B. Red Hat OpenStack Platform for POWER</a><ol class="menu"><li class=" leaf"><a href="index.html#ceph_storage">B.1. Ceph Storage</a></li><li class=" leaf"><a href="index.html#composable_services">B.2. Composable services</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#idm140301166572880">Legal Notice</a></li></ol>  </div>
</nav>


  <div class="doc-wrapper">
    <div class="panel-pane pane-page-title"  >
  
      
  
  <h1 class="title" itemprop="name">Director Installation and Usage</h1>
  
  </div>
<div class="panel-pane pane-page-body"  >
  
      
  
  <div class="body"><div xml:lang="en-US" class="book" id="idm140301182747504"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat OpenStack Platform</span> <span class="productnumber">16.1</span></div><div><h2 class="subtitle">An end-to-end scenario on using Red Hat OpenStack Platform director to create an OpenStack cloud</h2></div><div><div xml:lang="en-US" class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">OpenStack</span> <span class="othername">Documentation</span> <span class="surname">Team</span></h3><code class="email"><a class="email" href="mailto:rhos-docs@redhat.com">rhos-docs@redhat.com</a></code></div></div></div><div><a href="index.html#idm140301166572880">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				Install Red Hat OpenStack Platform 16 in an enterprise environment using the Red Hat OpenStack Platform director. This includes installing the director, planning your environment, and creating an OpenStack environment with the director.
			</div></div></div></div><hr/></div><section class="chapter" id="chap-Introduction"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Introduction to director</h1></div></div></div><p>
			The Red Hat OpenStack Platform (RHOSP) director is a toolset for installing and managing a complete OpenStack environment. Director is based primarily on the OpenStack project TripleO. With director you can install a fully-operational, lean, and robust RHOSP environment that can provision and control bare metal systems to use as OpenStack nodes.
		</p><p>
			Director uses two main concepts: an undercloud and an overcloud. First you install the undercloud, and then use the undercloud as a tool to install and configure the overcloud.
		</p><p>
			<span class="inlinemediaobject"><img src="Diagram-001-Topology.png" alt="Basic Layout of undercloud and overcloud"/></span>

		</p><section class="section" id="sect-Undercloud"><div class="titlepage"><div><div><h2 class="title">1.1. Undercloud</h2></div></div></div><p>
				The undercloud is the main management node that contains the Red Hat OpenStack Platform director toolset. It is a single-system OpenStack installation that includes components for provisioning and managing the OpenStack nodes that form your OpenStack environment (the overcloud). The components that form the undercloud have multiple functions:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Environment planning</span></dt><dd>
							The undercloud includes planning functions that you can use to create and assign certain node roles. The undercloud includes a default set of nodes: Compute, Controller, and various Storage roles. You can also design custom roles. Additionally, you can select which OpenStack Platform services to include on each node role, which provides a method to model new node types or isolate certain components on their own host.
						</dd><dt><span class="term">Bare metal system control</span></dt><dd>
							The undercloud uses the out-of-band management interface, usually Intelligent Platform Management Interface (IPMI), of each node for power management control and a PXE-based service to discover hardware attributes and install OpenStack on each node. You can use this feature to provision bare metal systems as OpenStack nodes. For a full list of power management drivers, see <a class="xref" href="index.html#appe-Power_Management_Drivers" title="Appendix A. Power management drivers">Appendix A, <em>Power management drivers</em></a>.
						</dd><dt><span class="term">Orchestration</span></dt><dd>
							The undercloud contains a set of YAML templates that represent a set of plans for your environment. The undercloud imports these plans and follows their instructions to create the resulting OpenStack environment. The plans also include hooks that you can use to incorporate your own customizations as certain points in the environment creation process.
						</dd><dt><span class="term">Undercloud components</span></dt><dd><p class="simpara">
							The undercloud uses OpenStack components as its base tool set. Each component operates within a separate container on the undercloud:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									OpenStack Identity (keystone) - Provides authentication and authorization for the director components.
								</li><li class="listitem">
									OpenStack Bare Metal (ironic) and OpenStack Compute (nova) - Manages bare metal nodes.
								</li><li class="listitem">
									OpenStack Networking (neutron) and Open vSwitch - Control networking for bare metal nodes.
								</li><li class="listitem">
									OpenStack Image Service (glance) - Stores images that director writes to bare metal machines.
								</li><li class="listitem">
									OpenStack Orchestration (heat) and Puppet - Provides orchestration of nodes and configuration of nodes after director writes the overcloud image to disk.
								</li><li class="listitem"><p class="simpara">
									OpenStack Telemetry (ceilometer) - Performs monitoring and data collection. Telemetry also includes the following components:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											OpenStack Telemetry Metrics (gnocchi) - Provides a time series database for metrics.
										</li><li class="listitem">
											OpenStack Telemetry Alarming (aodh) - Provide an alarming component for monitoring.
										</li><li class="listitem">
											OpenStack Telemetry Event Storage (panko) - Provides event storage for monitoring.
										</li></ul></div></li><li class="listitem">
									OpenStack Workflow Service (mistral) - Provides a set of workflows for certain director-specific actions, such as importing and deploying plans.
								</li><li class="listitem">
									OpenStack Messaging Service (zaqar) - Provides a messaging service for the OpenStack Workflow Service.
								</li><li class="listitem"><p class="simpara">
									OpenStack Object Storage (swift) - Provides object storage for various OpenStack Platform components, including:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Image storage for OpenStack Image Service
										</li><li class="listitem">
											Introspection data for OpenStack Bare Metal
										</li><li class="listitem">
											Deployment plans for OpenStack Workflow Service
										</li></ul></div></li></ul></div></dd></dl></div></section><section class="section" id="sect-Overcloud"><div class="titlepage"><div><div><h2 class="title">1.2. Understanding the overcloud</h2></div></div></div><p>
				The overcloud is the resulting Red Hat OpenStack Platform (RHOSP) environment that the undercloud creates. The overcloud consists of multiple nodes with different roles that you define based on the OpenStack Platform environment that you want to create. The undercloud includes a default set of overcloud node roles:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Controller</span></dt><dd><p class="simpara">
							Controller nodes provide administration, networking, and high availability for the OpenStack environment. A recommended OpenStack environment contains three Controller nodes together in a high availability cluster.
						</p><p class="simpara">
							A default Controller node role supports the following components. Not all of these services are enabled by default. Some of these components require custom or pre-packaged environment files to enable:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									OpenStack Dashboard (horizon)
								</li><li class="listitem">
									OpenStack Identity (keystone)
								</li><li class="listitem">
									OpenStack Compute (nova) API
								</li><li class="listitem">
									OpenStack Networking (neutron)
								</li><li class="listitem">
									OpenStack Image Service (glance)
								</li><li class="listitem">
									OpenStack Block Storage (cinder)
								</li><li class="listitem">
									OpenStack Object Storage (swift)
								</li><li class="listitem">
									OpenStack Orchestration (heat)
								</li><li class="listitem">
									OpenStack Telemetry Metrics (gnocchi)
								</li><li class="listitem">
									OpenStack Telemetry Alarming (aodh)
								</li><li class="listitem">
									OpenStack Telemetry Event Storage (panko)
								</li><li class="listitem">
									OpenStack Shared File Systems (manila)
								</li><li class="listitem">
									OpenStack Bare Metal (ironic)
								</li><li class="listitem">
									MariaDB
								</li><li class="listitem">
									Open vSwitch
								</li><li class="listitem">
									Pacemaker and Galera for high availability services.
								</li></ul></div></dd><dt><span class="term">Compute</span></dt><dd><p class="simpara">
							Compute nodes provide computing resources for the OpenStack environment. You can add more Compute nodes to scale out your environment over time. A default Compute node contains the following components:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									OpenStack Compute (nova)
								</li><li class="listitem">
									KVM/QEMU
								</li><li class="listitem">
									OpenStack Telemetry (ceilometer) agent
								</li><li class="listitem">
									Open vSwitch
								</li></ul></div></dd><dt><span class="term">Storage</span></dt><dd><p class="simpara">
							Storage nodes provide storage for the OpenStack environment. The following list contains information about the various types of Storage node in RHOSP:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Ceph Storage nodes - Used to form storage clusters. Each node contains a Ceph Object Storage Daemon (OSD). Additionally, director installs Ceph Monitor onto the Controller nodes in situations where you deploy Ceph Storage nodes as part of your environment.
								</li><li class="listitem"><p class="simpara">
									Block storage (cinder) - Used as external block storage for highly available Controller nodes. This node contains the following components:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											OpenStack Block Storage (cinder) volume
										</li><li class="listitem">
											OpenStack Telemetry agents
										</li><li class="listitem">
											Open vSwitch.
										</li></ul></div></li><li class="listitem"><p class="simpara">
									Object storage (swift) - These nodes provide an external storage layer for OpenStack Swift. The Controller nodes access object storage nodes through the Swift proxy. Object storage nodes contain the following components:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											OpenStack Object Storage (swift) storage
										</li><li class="listitem">
											OpenStack Telemetry agents
										</li><li class="listitem">
											Open vSwitch.
										</li></ul></div></li></ul></div></dd></dl></div></section><section class="section" id="sect-High_Availability"><div class="titlepage"><div><div><h2 class="title">1.3. Understanding high availability in Red Hat OpenStack Platform</h2></div></div></div><p>
				The Red Hat OpenStack Platform (RHOSP) director uses a Controller node cluster to provide highly available services to your OpenStack Platform environment. For each service, director installs the same components on all Controller nodes and manages the Controller nodes together as a single service. This type of cluster configuration provides a fallback in the event of operational failures on a single Controller node. This provides OpenStack users with a certain degree of continuous operation.
			</p><p>
				The OpenStack Platform director uses some key pieces of software to manage components on the Controller node:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Pacemaker - Pacemaker is a cluster resource manager. Pacemaker manages and monitors the availability of OpenStack components across all nodes in the cluster.
					</li><li class="listitem">
						HAProxy - Provides load balancing and proxy services to the cluster.
					</li><li class="listitem">
						Galera - Replicates the RHOSP database across the cluster.
					</li><li class="listitem">
						Memcached - Provides database caching.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							From version 13 and later, you can use director to deploy High Availability for Compute Instances (Instance HA). With Instance HA you can automate evacuating instances from a Compute node when the Compute node fails.
						</li></ul></div></div></div></section><section class="section" id="sect-Containerization"><div class="titlepage"><div><div><h2 class="title">1.4. Understanding containerization in Red Hat OpenStack Platform</h2></div></div></div><p>
				Each OpenStack Platform service on the undercloud and overcloud runs inside an individual Linux container on their respective node. This containerization provides a method to isolate services, maintain the environment, and upgrade Red Hat OpenStack Platform (RHOSP).
			</p><p>
				Red Hat OpenStack Platform 16.1 supports installation on the Red Hat Enterprise Linux 8.2 operating system. Red Hat Enterprise Linux 8.2 no longer includes Docker and provides a new set of tools to replace the Docker ecosystem. This means OpenStack Platform 16.1 replaces Docker with these new tools for OpenStack Platform deployment and upgrades.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Podman</span></dt><dd><p class="simpara">
							Pod Manager (Podman) is a container management tool. It implements almost all Docker CLI commands, not including commands related to Docker Swarm. Podman manages pods, containers, and container images. One of the major differences between Podman and Docker is that Podman can manage resources without a daemon running in the background.
						</p><p class="simpara">
							For more information about Podman, see the <a class="link" href="https://podman.io/">Podman website</a>.
						</p></dd><dt><span class="term">Buildah</span></dt><dd><p class="simpara">
							Buildah specializes in building Open Containers Initiative (OCI) images, which you use in conjunction with Podman. Buildah commands replicate the contents of a Dockerfile. Buildah also provides a lower-level <code class="literal">coreutils</code> interface to build container images, so that you do not require a Dockerfile to build containers. Buildah also uses other scripting languages to build container images without requiring a daemon.
						</p><p class="simpara">
							For more information about Buildah, see the <a class="link" href="https://buildah.io/">Buildah website</a>.
						</p></dd><dt><span class="term">Skopeo</span></dt><dd>
							Skopeo provides operators with a method to inspect remote container images, which helps director collect data when it pulls images. Additional features include copying container images from one registry to another and deleting images from registries.
						</dd></dl></div><p>
				Red Hat supports the following methods for managing container images for your overcloud:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Pulling container images from the Red Hat Container Catalog to the <code class="literal">image-serve</code> registry on the undercloud and then pulling the images from the <code class="literal">image-serve</code> registry. When you pull images to the undercloud first, you avoid multiple overcloud nodes simultaneously pulling container images over an external connection.
					</li><li class="listitem">
						Pulling container images from your Satellite 6 server. You can pull these images directly from the Satellite because the network traffic is internal.
					</li></ul></div><p>
				This guide contains information about configuring your container image registry details and performing basic container operations.
			</p></section><section class="section" id="sect-Ceph_Storage"><div class="titlepage"><div><div><h2 class="title">1.5. Working with Ceph Storage in Red Hat OpenStack Platform</h2></div></div></div><p>
				It is common for large organizations that use Red Hat OpenStack Platform (RHOSP) to serve thousands of clients or more. Each OpenStack client is likely to have their own unique needs when consuming block storage resources. Deploying glance (images), cinder (volumes), and nova (Compute) on a single node can become impossible to manage in large deployments with thousands of clients. Scaling OpenStack externally resolves this challenge.
			</p><p>
				However, there is also a practical requirement to virtualize the storage layer with a solution like Red Hat Ceph Storage so that you can scale the RHOSP storage layer from tens of terabytes to petabytes, or even exabytes of storage. Red Hat Ceph Storage provides this storage virtualization layer with high availability and high performance while running on commodity hardware. While virtualization might seem like it comes with a performance penalty, Ceph stripes block device images as objects across the cluster, meaning that large Ceph Block Device images have better performance than a standalone disk. Ceph Block devices also support caching, copy-on-write cloning, and copy-on-read cloning for enhanced performance.
			</p><p>
				For more information about Red Hat Ceph Storage, see <a class="link" href="https://access.redhat.com/products/red-hat-ceph-storage">Red Hat Ceph Storage</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					For multi-architecture clouds, Red Hat supports only pre-installed or external Ceph implementation. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/integrating_an_overcloud_with_an_existing_red_hat_ceph_cluster/">Integrating an Overcloud with an Existing Red Hat Ceph Cluster</a> and <a class="xref" href="index.html#appe-OSP_on_POWER" title="Appendix B. Red Hat OpenStack Platform for POWER">Appendix B, <em>Red Hat OpenStack Platform for POWER</em></a>.
				</p></div></div></section></section><div class="part" id="director_installation_and_configuration"><div class="titlepage"><div><div><h1 class="title">Part I. Director installation and configuration</h1></div></div></div><section class="chapter" id="planning-your-undercloud"><div class="titlepage"><div><div><h2 class="title">Chapter 2. Planning your undercloud</h2></div></div></div><section class="section" id="containerised-undercloud"><div class="titlepage"><div><div><h2 class="title">2.1. Containerized undercloud</h2></div></div></div><p>
					The undercloud is the node that controls the configuration, installation, and management of your final Red Hat OpenStack Platform (RHOSP) environment, which is called the overcloud. The undercloud itself uses OpenStack Platform components in the form of containers to create a toolset called director. This means that the undercloud pulls a set of container images from a registry source, generates configuration for the containers, and runs each OpenStack Platform service as a container. As a result, the undercloud provides a containerized set of services that you can use as a toolset to create and manage your overcloud.
				</p><p>
					Since both the undercloud and overcloud use containers, both use the same architecture to pull, configure, and run containers. This architecture is based on the OpenStack Orchestration service (heat) for provisioning nodes and uses Ansible to configure services and containers. It is useful to have some familiarity with heat and Ansible to help you troubleshoot issues that you might encounter.
				</p></section><section class="section" id="preparing-your-network"><div class="titlepage"><div><div><h2 class="title">2.2. Preparing your undercloud networking</h2></div></div></div><p>
					The undercloud requires access to two main networks:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <span class="strong strong"><strong>Provisioning or Control Plane network</strong></span>, which is the network that director uses to provision your nodes and access them over SSH when executing Ansible configuration. This network also enables SSH access from the undercloud to overcloud nodes. The undercloud contains DHCP services for introspection and provisioning other nodes on this network, which means that no other DHCP services should exist on this network. The director configures the interface for this network.
						</li><li class="listitem">
							The <span class="strong strong"><strong>External network</strong></span>, which enables access to OpenStack Platform repositories, container image sources, and other servers such as DNS servers or NTP servers. Use this network for standard access the undercloud from your workstation. You must manually configure an interface on the undercloud to access the external network.
						</li></ul></div><p>
					The undercloud requires a minimum of 2 x 1 Gbps Network Interface Cards: one for the <span class="strong strong"><strong>Provisioning or Control Plane network</strong></span> and one for the <span class="strong strong"><strong>External network</strong></span>. However, it is recommended to use a 10 Gbps interface for Provisioning network traffic, especially if you want to provision a large number of nodes in your overcloud environment.
				</p><p>
					Note:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Do not use the same Provisioning or Control Plane NIC as the one that you use to access the director machine from your workstation. The director installation creates a bridge by using the Provisioning NIC, which drops any remote connections. Use the External NIC for remote connections to the director system.
						</li><li class="listitem"><p class="simpara">
							The Provisioning network requires an IP range that fits your environment size. Use the following guidelines to determine the total number of IP addresses to include in this range:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Include at least one temporary IP address for each node that connects to the Provisioning network during introspection.
								</li><li class="listitem">
									Include at least one permanent IP address for each node that connects to the Provisioning network during deployment.
								</li><li class="listitem">
									Include an extra IP address for the virtual IP of the overcloud high availability cluster on the Provisioning network.
								</li><li class="listitem">
									Include additional IP addresses within this range for scaling the environment.
								</li></ul></div></li></ul></div></section><section class="section" id="determining-environment-scale"><div class="titlepage"><div><div><h2 class="title">2.3. Determining environment scale</h2></div></div></div><p>
					Before you install the undercloud, determine the scale of your environment. Include the following factors when you plan your environment:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">How many nodes do you want to deploy in your overcloud?</span></dt><dd>
								The undercloud manages each node within an overcloud. Provisioning overcloud nodes consumes resources on the undercloud. You must provide your undercloud with enough resources to adequately provision and control all of your overcloud nodes.
							</dd><dt><span class="term">How many simultaneous operations do you want the undercloud to perform?</span></dt><dd>
								Most OpenStack services on the undercloud use a set of workers. Each worker performs an operation specific to that service. Multiple workers provide simultaneous operations. The default number of workers on the undercloud is determined by halving the total CPU thread count on the undercloud. In this instance, thread count refers to the number of CPU cores multiplied by the hyper-threading value. For example, if your undercloud has a CPU with 16 threads, then the director services spawn 8 workers by default. Director also uses a set of minimum and maximum caps by default:
							</dd></dl></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301179839040" scope="col">Service</th><th align="left" valign="top" id="idm140301179837952" scope="col">Minimum</th><th align="left" valign="top" id="idm140301179836864" scope="col">Maximum</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301179839040"> <p>
									OpenStack Orchestration (heat)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179837952"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm140301179836864"> <p>
									24
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179839040"> <p>
									All other service
								</p>
								 </td><td align="left" valign="top" headers="idm140301179837952"> <p>
									2
								</p>
								 </td><td align="left" valign="top" headers="idm140301179836864"> <p>
									12
								</p>
								 </td></tr></tbody></table></div><p>
					The undercloud has the following minimum CPU and memory requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							An 8-thread 64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions. This provides 4 workers for each undercloud service.
						</li><li class="listitem"><p class="simpara">
							A minimum of 24 GB of RAM.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									The <code class="literal">ceph-ansible</code> playbook consumes 1 GB resident set size (RSS) for every 10 hosts that the undercloud deploys. If you want to use a new or existing Ceph cluster in your deployment, you must provision the undercloud RAM accordingly.
								</li></ul></div></li></ul></div><p>
					To use a larger number of workers, increase the vCPUs and memory of your undercloud using the following recommendations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>Minimum:</strong></span> Use 1.5 GB of memory for each thread. For example, a machine with 48 threads requires 72 GB of RAM to provide the minimum coverage for 24 heat workers and 12 workers for other services.
						</li><li class="listitem">
							<span class="strong strong"><strong>Recommended:</strong></span> Use 3 GB of memory for each thread. For example, a machine with 48 threads requires 144 GB of RAM to provide the recommended coverage for 24 heat workers and 12 workers for other services.
						</li></ul></div></section><section class="section" id="undercloud-disk-sizing"><div class="titlepage"><div><div><h2 class="title">2.4. Undercloud disk sizing</h2></div></div></div><p>
					The recommended minimum undercloud disk size is <span class="strong strong"><strong>100 GB</strong></span> of available disk space on the root disk:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							20 GB for container images
						</li><li class="listitem">
							10 GB to accommodate QCOW2 image conversion and caching during the node provisioning process
						</li><li class="listitem">
							70 GB+ for general usage, logging, metrics, and growth
						</li></ul></div></section><section class="section" id="virtualization-support"><div class="titlepage"><div><div><h2 class="title">2.5. Virtualization support</h2></div></div></div><p>
					Red Hat only supports a virtualized undercloud on the following platforms:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301182412848" scope="col">Platform</th><th align="left" valign="top" id="idm140301182411760" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301182412848"> <p>
									Kernel-based Virtual Machine (KVM)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182411760"> <p>
									Hosted by Red Hat Enterprise Linux 8, as listed on certified hypervisors.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182412848"> <p>
									Red Hat Virtualization
								</p>
								 </td><td align="left" valign="top" headers="idm140301182411760"> <p>
									Hosted by Red Hat Virtualization 4.x, as listed on certified hypervisors.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182412848"> <p>
									Microsoft Hyper-V
								</p>
								 </td><td align="left" valign="top" headers="idm140301182411760"> <p>
									Hosted by versions of Hyper-V as listed on the <a class="link" href="https://access.redhat.com/ecosystem/search/#/ecosystem/Red%20Hat%20OpenStack%20Platform">Red Hat Customer Portal Certification Catalogue</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182412848"> <p>
									VMware ESX and ESXi
								</p>
								 </td><td align="left" valign="top" headers="idm140301182411760"> <p>
									Hosted by versions of ESX and ESXi as listed on the <a class="link" href="https://access.redhat.com/ecosystem/search/#/ecosystem/Red%20Hat%20OpenStack%20Platform">Red Hat Customer Portal Certification Catalogue</a>.
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Red Hat OpenStack Platform director requires that the latest version of Red Hat Enterprise Linux 8 is installed as the host operating system. This means your virtualization platform must also support the underlying Red Hat Enterprise Linux version.
					</p></div></div><div class="formalpara"><p class="title"><strong>Virtual Machine Requirements</strong></p><p>
						Resource requirements for a virtual undercloud are similar to those of a bare metal undercloud. You should consider the various tuning options when provisioning such as network model, guest CPU capabilities, storage backend, storage format, and caching mode.
					</p></div><div class="formalpara"><p class="title"><strong>Network Considerations</strong></p><p>
						Note the following network considerations for your virtualized undercloud:
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Power Management</span></dt><dd>
								The undercloud VM requires access to the overcloud nodes' power management devices. This is the IP address set for the <code class="literal">pm_addr</code> parameter when registering nodes.
							</dd><dt><span class="term">Provisioning network</span></dt><dd>
								The NIC used for the provisioning (<code class="literal">ctlplane</code>) network requires the ability to broadcast and serve DHCP requests to the NICs of the overcloud’s bare metal nodes. As a recommendation, create a bridge that connects the VM’s NIC to the same network as the bare metal NICs.
							</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						A common problem occurs when the hypervisor technology blocks the undercloud from transmitting traffic from an unknown address. - If using Red Hat Enterprise Virtualization, disable <code class="literal">anti-mac-spoofing</code> to prevent this. - If using VMware ESX or ESXi, allow forged transmits to prevent this. You must power off and on the director VM after you apply these settings. Rebooting the VM is not sufficient.
					</p></div></div></section><section class="section" id="character-encoding-configuration-osp"><div class="titlepage"><div><div><h2 class="title">2.6. Character encoding configuration</h2></div></div></div><p>
					Red Hat OpenStack Platform has special character encoding requirements as part of the locale settings:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use UTF-8 encoding on all nodes. Ensure the <code class="literal">LANG</code> environment variable is set to <code class="literal">en_US.UTF-8</code> on all nodes.
						</li><li class="listitem">
							Avoid using non-ASCII characters if you use Red Hat Ansible Tower to automate the creation of Red Hat OpenStack Platform resources.
						</li></ul></div></section><section class="section" id="undercloud-repositories"><div class="titlepage"><div><div><h2 class="title">2.7. Undercloud repositories</h2></div></div></div><p>
					Red Hat OpenStack Platform 16.1 runs on Red Hat Enterprise Linux 8.2. As a result, you must lock the content from these repositories to the respective Red Hat Enterprise Linux version.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you synchronize repositories with Red Hat Satellite, you can enable specific versions of the Red Hat Enterprise Linux repositories. However, the repository remains the same despite the version you choose. For example, you can enable the 8.2 version of the BaseOS repository, but the repository name is still <code class="literal">rhel-8-for-x86_64-baseos-eus-rpms</code> despite the specific version you choose.
					</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Any repositories outside the ones specified here are not supported. Unless recommended, do not enable any other products or repositories outside the ones listed in the following tables or else you might encounter package dependency issues. Do not enable Extra Packages for Enterprise Linux (EPEL).
					</p></div></div><div class="formalpara"><p class="title"><strong>Core repositories</strong></p><p>
						The following table lists core repositories for installing the undercloud.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301177309216" scope="col">Name</th><th align="left" valign="top" id="idm140301177308128" scope="col">Repository</th><th align="left" valign="top" id="idm140301177307040" scope="col">Description of requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs) Extended Update Support (EUS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">rhel-8-for-x86_64-baseos-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Base operating system repository for x86_64 systems.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">rhel-8-for-x86_64-appstream-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Contains Red Hat OpenStack Platform dependencies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - High Availability (RPMs) Extended Update Support (EUS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">rhel-8-for-x86_64-highavailability-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									High availability tools for Red Hat Enterprise Linux. Used for Controller node high availability.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Ansible Engine 2.9 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">ansible-2.9-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Ansible Engine for Red Hat Enterprise Linux. Used to provide the latest version of Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Advanced Virtualization for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">advanced-virt-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Provides virtualization packages for OpenStack Platform.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Satellite Tools for RHEL 8 Server RPMs x86_64
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">satellite-tools-6.5-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Tools for managing hosts with Red Hat Satellite 6.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat OpenStack Platform 16.1 for RHEL 8 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">openstack-16.1-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Core Red Hat OpenStack Platform repository, which contains packages for Red Hat OpenStack Platform director.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177309216"> <p>
									Red Hat Fast Datapath for RHEL 8 (RPMS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301177308128"> <p>
									<code class="literal">fast-datapath-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301177307040"> <p>
									Provides Open vSwitch (OVS) packages for OpenStack Platform.
								</p>
								 </td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Ceph repositories</strong></p><p>
						The following table lists Ceph Storage related repositories for the undercloud.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301178072128" scope="col">Name</th><th align="left" valign="top" id="idm140301178071040" scope="col">Repository</th><th align="left" valign="top" id="idm140301178069952" scope="col">Description of Requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301178072128"> <p>
									Red Hat Ceph Storage Tools 4 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178071040"> <p>
									<code class="literal">rhceph-4-tools-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178069952"> <p>
									Provides tools for nodes to communicate with the Ceph Storage cluster. The undercloud requires the <code class="literal">ceph-ansible</code> package from this repository if you plan to use Ceph Storage in your overcloud.
								</p>
								 </td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>IBM POWER repositories</strong></p><p>
						The following table contains a list of repositories for Red Hat Openstack Platform on POWER PC architecture. Use these repositories in place of equivalents in the Core repositories.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301179659264" scope="col">Name</th><th align="left" valign="top" id="idm140301179658176" scope="col">Repository</th><th align="left" valign="top" id="idm140301179657088" scope="col">Description of requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301179659264"> <p>
									Red Hat Enterprise Linux for IBM Power, little endian - BaseOS (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179658176"> <p>
									<code class="literal">rhel-8-for-ppc64le-baseos-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179657088"> <p>
									Base operating system repository for ppc64le systems.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179659264"> <p>
									Red Hat Enterprise Linux 8 for IBM Power, little endian - AppStream (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179658176"> <p>
									<code class="literal">rhel-8-for-ppc64le-appstream-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179657088"> <p>
									Contains Red Hat OpenStack Platform dependencies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179659264"> <p>
									Red Hat Enterprise Linux 8 for IBM Power, little endian - High Availability (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179658176"> <p>
									<code class="literal">rhel-8-for-ppc64le-highavailability-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179657088"> <p>
									High availability tools for Red Hat Enterprise Linux. Used for Controller node high availability.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179659264"> <p>
									Red Hat Ansible Engine 2.8 for RHEL 8 IBM Power, little endian (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179658176"> <p>
									<code class="literal">ansible-2.8-for-rhel-8-ppc64le-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179657088"> <p>
									Ansible Engine for Red Hat Enterprise Linux. Provides the latest version of Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179659264"> <p>
									Red Hat OpenStack Platform 16.1 for RHEL 8 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301179658176"> <p>
									<code class="literal">openstack-16.1-for-rhel-8-ppc64le-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179657088"> <p>
									Core Red Hat OpenStack Platform repository for ppc64le systems.
								</p>
								 </td></tr></tbody></table></div></section></section><section class="chapter" id="preparing-for-director-installation"><div class="titlepage"><div><div><h2 class="title">Chapter 3. Preparing for director installation</h2></div></div></div><section class="section" id="preparing-the-undercloud"><div class="titlepage"><div><div><h2 class="title">3.1. Preparing the undercloud</h2></div></div></div><p>
					Before you can install director, you must complete some basic configuration on the host machine.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud as the <code class="literal">root</code> user.
						</li><li class="listitem"><p class="simpara">
							Create the <code class="literal">stack</code> user:
						</p><pre class="screen">[root@director ~]# useradd stack</pre></li><li class="listitem"><p class="simpara">
							Set a password for the user:
						</p><pre class="screen">[root@director ~]# passwd stack</pre></li><li class="listitem"><p class="simpara">
							Disable password requirements when using <code class="literal">sudo</code>:
						</p><pre class="screen">[root@director ~]# echo "stack ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/stack
[root@director ~]# chmod 0440 /etc/sudoers.d/stack</pre></li><li class="listitem"><p class="simpara">
							Switch to the new <code class="literal">stack</code> user:
						</p><pre class="screen">[root@director ~]# su - stack
[stack@director ~]$</pre></li><li class="listitem"><p class="simpara">
							Create directories for system images and heat templates:
						</p><pre class="screen">[stack@director ~]$ mkdir ~/images
[stack@director ~]$ mkdir ~/templates</pre><p class="simpara">
							Director uses system images and heat templates to create the overcloud environment. Red Hat recommends creating these directories to help you organize your local file system.
						</p></li><li class="listitem"><p class="simpara">
							Check the base and full hostname of the undercloud:
						</p><pre class="screen">[stack@director ~]$ hostname
[stack@director ~]$ hostname -f</pre><p class="simpara">
							If either of the previous commands do not report the correct fully-qualified hostname or report an error, use <code class="literal">hostnamectl</code> to set a hostname:
						</p><pre class="screen">[stack@director ~]$ sudo hostnamectl set-hostname manager.example.com
[stack@director ~]$ sudo hostnamectl set-hostname --transient manager.example.com</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">/etc/hosts</code> and include an entry for the system hostname. The IP address in <code class="literal">/etc/hosts</code> must match the address that you plan to use for your undercloud public API. For example, if the system is named <code class="literal">manager.example.com</code> and uses <code class="literal">10.0.0.1</code> for its IP address, add the following line to the <code class="literal">/etc/hosts</code> file:
						</p><pre class="screen">10.0.0.1  manager.example.com manager</pre></li></ol></div></section><section class="section" id="registering-the-undercloud-and-attaching-subscriptions"><div class="titlepage"><div><div><h2 class="title">3.2. Registering the undercloud and attaching subscriptions</h2></div></div></div><p>
					Before you can install director, you must run <code class="literal">subscription-manager</code> to register the undercloud and attach a valid Red Hat OpenStack Platform subscription.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Register your system either with the Red Hat Content Delivery Network or with a Red Hat Satellite. For example, run the following command to register the system to the Content Delivery Network. Enter your Customer Portal user name and password when prompted:
						</p><pre class="screen">[stack@director ~]$ sudo subscription-manager register</pre></li><li class="listitem"><p class="simpara">
							Find the entitlement pool ID for Red Hat OpenStack Platform (RHOSP) director:
						</p><pre class="screen">[stack@director ~]$ sudo subscription-manager list --available --all --matches="Red Hat OpenStack"
Subscription Name:   Name of SKU
Provides:            Red Hat Single Sign-On
                     Red Hat Enterprise Linux Workstation
                     Red Hat CloudForms
                     Red Hat OpenStack
                     Red Hat Software Collections (for RHEL Workstation)
                     Red Hat Virtualization
SKU:                 SKU-Number
Contract:            Contract-Number
Pool ID:             Valid-Pool-Number-123456
Provides Management: Yes
Available:           1
Suggested:           1
Service Level:       Support-level
Service Type:        Service-Type
Subscription Type:   Sub-type
Ends:                End-date
System Type:         Physical</pre></li><li class="listitem"><p class="simpara">
							Locate the <code class="literal">Pool ID</code> value and attach the Red Hat OpenStack Platform 16.1 entitlement:
						</p><pre class="screen">[stack@director ~]$ sudo subscription-manager attach --pool=Valid-Pool-Number-123456</pre></li><li class="listitem"><p class="simpara">
							Lock the undercloud to Red Hat Enterprise Linux 8.2:
						</p><pre class="screen">$ sudo subscription-manager release --set=8.2</pre></li></ol></div></section><section class="section" id="enabling-repositories-for-the-undercloud"><div class="titlepage"><div><div><h2 class="title">3.3. Enabling repositories for the undercloud</h2></div></div></div><p>
					Enable the repositories that are required for the undercloud, and update the system packages to the latest versions.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Disable all default repositories, and enable the required Red Hat Enterprise Linux repositories:
						</p><pre class="screen">[stack@director ~]$ sudo subscription-manager repos --disable=*
[stack@director ~]$ sudo subscription-manager repos --enable=rhel-8-for-x86_64-baseos-eus-rpms --enable=rhel-8-for-x86_64-appstream-eus-rpms --enable=rhel-8-for-x86_64-highavailability-eus-rpms --enable=ansible-2.9-for-rhel-8-x86_64-rpms --enable=openstack-16.1-for-rhel-8-x86_64-rpms --enable=fast-datapath-for-rhel-8-x86_64-rpms --enable=advanced-virt-for-rhel-8-x86_64-rpms</pre><p class="simpara">
							These repositories contain packages that the director installation requires.
						</p></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">container-tools</code> repository module to version <code class="literal">2.0</code>:
						</p><pre class="screen">[stack@director ~]$ sudo dnf module disable -y container-tools:rhel8
[stack@director ~]$ sudo dnf module enable -y container-tools:2.0</pre></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">virt</code> repository module to version <code class="literal">8.2</code>:
						</p><pre class="screen">[stack@director ~]$ sudo dnf module disable -y virt:rhel
[stack@director ~]$ sudo dnf module enable -y virt:8.2</pre></li><li class="listitem"><p class="simpara">
							Perform an update on your system to ensure that you have the latest base system packages:
						</p><pre class="screen">[stack@director ~]$ sudo dnf update -y
[stack@director ~]$ sudo reboot</pre></li></ol></div></section><section class="section" id="configuring-an-undercloud-proxy"><div class="titlepage"><div><div><h2 class="title">3.4. Configuring an undercloud proxy</h2></div></div></div><p>
					If your environment uses a proxy, you can pre-configure the undercloud to use the proxy details. This procedure is optional and only applies to users requiring proxy configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud host as the <code class="literal">root</code> user.
						</li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">/etc/environment</code> file:
						</p><pre class="screen"># vi /etc/environment</pre></li><li class="listitem"><p class="simpara">
							Add the following parameters to the <code class="literal">/etc/environment</code> file:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">http_proxy</span></dt><dd>
										The proxy to use for standard HTTP requests.
									</dd><dt><span class="term">https_proxy</span></dt><dd>
										The proxy to use for HTTPs requests.
									</dd><dt><span class="term">no_proxy</span></dt><dd><p class="simpara">
										A comma-separated list of IP addresses and domains excluded from proxy communications. Include all IP addresses and domains relevant to the undercloud.
									</p><p class="simpara">
										For example, use the following syntax to define the <code class="literal">http_proxy</code>, <code class="literal">https_proxy</code>, and <code class="literal">no_proxy</code> parameters:
									</p></dd></dl></div><pre class="screen">http_proxy=https://10.0.0.1:8080/
https_proxy=https://10.0.0.1:8080/
no_proxy=127.0.0.1,172.16.0.0/16,172.17.0.0/16,172.18.0.0/16,192.168.0.0/16,<span class="emphasis"><em>HOSTNAME</em></span>.ctlplane.localdomain</pre></li><li class="listitem">
							Restart your shell session. For example, logout and re-login to the undercloud.
						</li></ol></div></section><section class="section" id="installing_director_packages"><div class="titlepage"><div><div><h2 class="title">3.5. Installing director packages</h2></div></div></div><p>
					Install packages relevant to Red Hat OpenStack Platform director.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Install the command line tools for director installation and configuration:
						</p><pre class="screen">[stack@director ~]$ sudo dnf install -y python3-tripleoclient</pre></li></ol></div></section><section class="section" id="installing-ceph-ansible"><div class="titlepage"><div><div><h2 class="title">3.6. Installing ceph-ansible</h2></div></div></div><p>
					The <code class="literal">ceph-ansible</code> package is required when you use Ceph Storage with Red Hat OpenStack Platform.
				</p><p>
					If you use Red Hat Ceph Storage, or if your deployment uses an external Ceph Storage cluster, install the <code class="literal">ceph-ansible</code> package. For more information about integrating with an existing Ceph Storage cluster, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/integrating_an_overcloud_with_an_existing_red_hat_ceph_cluster/index">Integrating an Overcloud with an Existing Red Hat Ceph Cluster</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enable the Ceph Tools repository:
						</p><pre class="screen">[stack@director ~]$ sudo subscription-manager repos --enable=rhceph-4-tools-for-rhel-8-x86_64-rpms</pre></li><li class="listitem"><p class="simpara">
							Install the <code class="literal">ceph-ansible</code> package:
						</p><pre class="screen">[stack@director ~]$ sudo dnf install -y ceph-ansible</pre></li></ol></div></section><section class="section" id="preparing-container-images"><div class="titlepage"><div><div><h2 class="title">3.7. Preparing container images</h2></div></div></div><p>
					The undercloud installation requires an environment file to determine where to obtain container images and how to store them. Generate and customize this environment file that you can use to prepare your container images.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud host as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Generate the default container image preparation file:
						</p><pre class="screen">$ openstack tripleo container image prepare default \
  --local-push-destination \
  --output-env-file containers-prepare-parameter.yaml</pre><p class="simpara">
							This command includes the following additional options:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">--local-push-destination</code> sets the registry on the undercloud as the location for container images. With this option, director pulls the necessary images from the Red Hat Container Catalog and pushes the images to the registry on the undercloud. Director uses the undercloud registry as the container image source. To pull container images directly from the Red Hat Container Catalog, omit this option.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">--output-env-file</code> specifies an environment file that includes include the parameters for preparing your container images. In this example, the name of the file is <code class="literal">containers-prepare-parameter.yaml</code>.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You can use the same <code class="literal">containers-prepare-parameter.yaml</code> file to define a container image source for both the undercloud and the overcloud.
									</p></div></div></li></ul></div></li><li class="listitem">
							Modify the <code class="literal">containers-prepare-parameter.yaml</code> to suit your requirements.
						</li></ol></div></section><section class="section" id="container-image-preparation-parameters"><div class="titlepage"><div><div><h2 class="title">3.8. Container image preparation parameters</h2></div></div></div><p>
					The default file for preparing your containers (<code class="literal">containers-prepare-parameter.yaml</code>) contains the <code class="literal">ContainerImagePrepare</code> heat parameter. This parameter defines a list of strategies for preparing a set of images:
				</p><pre class="screen">parameter_defaults:
  ContainerImagePrepare:
  - (strategy one)
  - (strategy two)
  - (strategy three)
  ...</pre><p>
					Each strategy accepts a set of sub-parameters that defines which images to use and what to do with the images. The following table contains information about the sub-parameters you can use with each <code class="literal">ContainerImagePrepare</code> strategy:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301166132192" scope="col">Parameter</th><th align="left" valign="top" id="idm140301166131104" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">excludes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									List of image name substrings to exclude from a strategy.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">includes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									List of image name substrings to include in a strategy. At least one image name must match an existing image. All <code class="literal">excludes</code> are ignored if <code class="literal">includes</code> is specified.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">modify_append_tag</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									String to append to the tag for the destination image. For example, if you pull an image with the tag <code class="literal">14.0-89</code> and set the <code class="literal">modify_append_tag</code> to <code class="literal">-hotfix</code>, the director tags the final image as <code class="literal">14.0-89-hotfix</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">modify_only_with_labels</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									A dictionary of image labels that filter the images that you want to modify. If an image matches the labels defined, the director includes the image in the modification process.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">modify_role</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									String of ansible role names to run during upload but before pushing the image to the destination registry.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">modify_vars</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									Dictionary of variables to pass to <code class="literal">modify_role</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">push_destination</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									Defines the namespace of the registry that you want to push images to during the upload process.
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											If set to <code class="literal">true</code>, the <code class="literal">push_destination</code> is set to the undercloud registry namespace using the hostname, which is the recommended method.
										</li><li class="listitem">
											If set to <code class="literal">false</code>, the push to a local registry does not occur and nodes pull images directly from the source.
										</li><li class="listitem">
											If set to a custom value, director pushes images to an external local registry.
										</li></ul></div>
								 <p>
									If you choose to pull container images directly from the Red Hat Container Catalog, do not set this parameter to <code class="literal">false</code> in production environments or else all overcloud nodes will simultaneously pull the images from the Red Hat Container Catalog over your external connection, which can cause bandwidth issues. If the <code class="literal">push_destination</code> parameter is set to <code class="literal">false</code> or is not defined and the remote registry requires authentication, set the <code class="literal">ContainerImageRegistryLogin</code> parameter to <code class="literal">true</code> and include the credentials with the <code class="literal">ContainerImageRegistryCredentials</code> parameter.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">pull_source</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									The source registry from where to pull the original container images.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">set</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									A dictionary of <code class="literal">key: value</code> definitions that define where to obtain the initial images.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166132192"> <p>
									<code class="literal">tag_from_label</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166131104"> <p>
									Defines the label pattern to tag the resulting images. Usually sets to <code class="literal">{version}-{release}</code>.
								</p>
								 </td></tr></tbody></table></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When you push images to the undercloud, use <code class="literal">push_destination: true</code> instead of <code class="literal">push_destination: UNDERCLOUD_IP:PORT</code>. The <code class="literal">push_destination: true</code> method provides a level of consistency across both IPv4 and IPv6 addresses.
					</p></div></div><p>
					The <code class="literal">set</code> parameter accepts a set of <code class="literal">key: value</code> definitions:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301184349248" scope="col">Key</th><th align="left" valign="top" id="idm140301173191280" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">ceph_image</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The name of the Ceph Storage container image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">ceph_namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The namespace of the Ceph Storage container image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">ceph_tag</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The tag of the Ceph Storage container image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">name_prefix</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									A prefix for each OpenStack service image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">name_suffix</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									A suffix for each OpenStack service image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">namespace</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The namespace for each OpenStack service image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">neutron_driver</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The driver to use to determine which OpenStack Networking (neutron) container to use. Use a null value to set to the standard <code class="literal">neutron-server</code> container. Set to <code class="literal">ovn</code> to use OVN-based containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301184349248"> <p>
									<code class="literal">tag</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173191280"> <p>
									The tag that the director uses to identify the images to pull from the source registry. You usually keep this key set to the default value, which is the Red Hat OpenStack Platform version number.
								</p>
								 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The container images use multi-stream tags based on Red Hat OpenStack Platform version. This means there is no longer a <code class="literal">latest</code> tag.
					</p></div></div><p>
					The <code class="literal">ContainerImageRegistryCredentials</code> parameter maps a container registry to a username and password to authenticate to that registry.
				</p><p>
					If a container registry requires a username and password, you can use <code class="literal">ContainerImageRegistryCredentials</code> to include credentials with the following syntax:
				</p><pre class="screen">  ContainerImagePrepare:
  - push_destination: true
    set:
      namespace: registry.redhat.io/...
      ...
  ContainerImageRegistryCredentials:
    registry.redhat.io:
      my_username: my_password</pre><p>
					In the example, replace <code class="literal">my_username</code> and <code class="literal">my_password</code> with your authentication credentials. Instead of using your individual user credentials, Red Hat recommends creating a registry service account and using those credentials to access <code class="literal">registry.redhat.io</code> content. For more information, see <a class="link" href="https://access.redhat.com/RegistryAuthentication">"Red Hat Container Registry Authentication"</a>.
				</p><p>
					The <code class="literal">ContainerImageRegistryLogin</code> parameter is used to control the registry login on the systems being deployed. This must be set to <code class="literal">true</code> if <code class="literal">push_destination</code> is set to false or not used.
				</p><pre class="screen">  ContainerImagePrepare:
  - set:
      namespace: registry.redhat.io/...
      ...
  ContainerImageRegistryCredentials:
    registry.redhat.io:
      my_username: my_password
  ContainerImageRegistryLogin: true</pre><p>
					If you have configured <code class="literal">push_destination</code>, do not set <code class="literal">ContainerImageRegistryLogin</code> to <code class="literal">true</code>. If you set this option to <code class="literal">true</code> and the overcloud nodes do not have network connectivity to the registry hosts defined in <code class="literal">ContainerImageRegistryCredentials</code>, the deployment might fail when trying to perform a login.
				</p></section><section class="section" id="layering-image-preparation-entries"><div class="titlepage"><div><div><h2 class="title">3.9. Layering image preparation entries</h2></div></div></div><p>
					The value of the <code class="literal">ContainerImagePrepare</code> parameter is a YAML list. This means that you can specify multiple entries. The following example demonstrates two entries where director uses the latest version of all images except for the <code class="literal">nova-api</code> image, which uses the version tagged with <code class="literal">16.0-44</code>:
				</p><pre class="screen">ContainerImagePrepare:
- tag_from_label: "{version}-{release}"
  push_destination: true
  excludes:
  - nova-api
  set:
    namespace: registry.redhat.io/rhosp-rhel8
    name_prefix: openstack-
    name_suffix: ''
    tag: 16.1
- push_destination: true
  includes:
  - nova-api
  set:
    namespace: registry.redhat.io/rhosp-rhel8
    tag: 16.1-44</pre><p>
					The <code class="literal">includes</code> and <code class="literal">excludes</code> entries control image filtering for each entry. The images that match the <code class="literal">includes</code> strategy take precedence over <code class="literal">excludes</code> matches. The image name must contain the <code class="literal">includes</code> or <code class="literal">excludes</code> value to be considered a match.
				</p></section><section class="section" id="obtaining-container-images-from-private-registries"><div class="titlepage"><div><div><h2 class="title">3.10. Obtaining container images from private registries</h2></div></div></div><p>
					Some container image registries require authentication to access images. In this situation, use the <code class="literal">ContainerImageRegistryCredentials</code> parameter in your <code class="literal">containers-prepare-parameter.yaml</code> environment file.
				</p><pre class="screen">parameter_defaults:
  ContainerImagePrepare:
  - (strategy one)
  - (strategy two)
  - (strategy three)
  ContainerImageRegistryCredentials:
    registry.example.com:
      username: "p@55w0rd!"</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Private registries require <code class="literal">push_destination</code> set to <code class="literal">true</code> for their respective strategy in the <code class="literal">ContainerImagePrepare</code>.
					</p></div></div><p>
					The <code class="literal">ContainerImageRegistryCredentials</code> parameter uses a set of keys based on the private registry URL. Each private registry URL uses its own key and value pair to define the username (key) and password (value). This provides a method to specify credentials for multiple private registries.
				</p><pre class="screen">parameter_defaults:
  ...
  ContainerImageRegistryCredentials:
    registry.redhat.io:
      myuser: 'p@55w0rd!'
    registry.internalsite.com:
      myuser2: '0th3rp@55w0rd!'
    '192.0.2.1:8787':
      myuser3: '@n0th3rp@55w0rd!'</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						The default <code class="literal">ContainerImagePrepare</code> parameter pulls container images from <code class="literal">registry.redhat.io</code>, which requires authentication.
					</p></div></div><p>
					The <code class="literal">ContainerImageRegistryLogin</code> parameter is used to control whether the system needs to log in to the remote registry to fetch the containers.
				</p><pre class="screen">parameter_defaults:
  ...
  ContainerImageRegistryLogin: true</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You must set <code class="literal">ContainerImageRegistryLogin</code> to <code class="literal">true</code> if <code class="literal">push_destination</code> is not configured for a given strategy. If <code class="literal">push_destination</code> is configured in a <code class="literal">ContainerImagePrepare</code> strategy and the <code class="literal">ContainerImageRegistryCredentials</code> parameter is configured, the system logs in to fetch the containers and pushes them to the remote system. If the overcloud nodes do not have network connectivity to the registry hosts defined in the <code class="literal">ContainerImageRegistryCredentials</code>, set <code class="literal">push_destination</code> to <code class="literal">true</code> and <code class="literal">ContainerImageRegistryLogin</code> to <code class="literal">false</code> .
					</p></div></div></section><section class="section" id="modifying-images-during-preparation"><div class="titlepage"><div><div><h2 class="title">3.11. Modifying images during preparation</h2></div></div></div><p>
					It is possible to modify images during image preparation, and then immediately deploy with modified images. Scenarios for modifying images include:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							As part of a continuous integration pipeline where images are modified with the changes being tested before deployment.
						</li><li class="listitem">
							As part of a development workflow where local changes must be deployed for testing and development.
						</li><li class="listitem">
							When changes must be deployed but are not available through an image build pipeline. For example, adding proprietary add-ons or emergency fixes.
						</li></ul></div><p>
					To modify an image during preparation, invoke an Ansible role on each image that you want to modify. The role takes a source image, makes the requested changes, and tags the result. The prepare command can push the image to the destination registry and set the heat parameters to refer to the modified image.
				</p><p>
					The Ansible role <code class="literal">tripleo-modify-image</code> conforms with the required role interface and provides the behaviour necessary for the modify use cases. Control the modification with the modify-specific keys in the <code class="literal">ContainerImagePrepare</code> parameter:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">modify_role</code> specifies the Ansible role to invoke for each image to modify.
						</li><li class="listitem">
							<code class="literal">modify_append_tag</code> appends a string to the end of the source image tag. This makes it obvious that the resulting image has been modified. Use this parameter to skip modification if the <code class="literal">push_destination</code> registry already contains the modified image. Change <code class="literal">modify_append_tag</code> whenever you modify the image.
						</li><li class="listitem">
							<code class="literal">modify_vars</code> is a dictionary of Ansible variables to pass to the role.
						</li></ul></div><p>
					To select a use case that the <code class="literal">tripleo-modify-image</code> role handles, set the <code class="literal">tasks_from</code> variable to the required file in that role.
				</p><p>
					While developing and testing the <code class="literal">ContainerImagePrepare</code> entries that modify images, run the image prepare command without any additional options to confirm that the image is modified as you expect:
				</p><pre class="screen">sudo openstack tripleo container image prepare \
  -e ~/containers-prepare-parameter.yaml</pre></section><section class="section" id="updating-existing-packages-on-container-images"><div class="titlepage"><div><div><h2 class="title">3.12. Updating existing packages on container images</h2></div></div></div><p>
					The following example <code class="literal">ContainerImagePrepare</code> entry updates in all packages on the container images using the dnf repository configuration of the undercloud host:
				</p><pre class="screen">ContainerImagePrepare:
- push_destination: true
  ...
  modify_role: tripleo-modify-image
  modify_append_tag: "-updated"
  modify_vars:
    tasks_from: yum_update.yml
    compare_host_packages: true
    yum_repos_dir_path: /etc/yum.repos.d
  ...</pre></section><section class="section" id="installing-additional-rpm-files-to-container-images"><div class="titlepage"><div><div><h2 class="title">3.13. Installing additional RPM files to container images</h2></div></div></div><p>
					You can install a directory of RPM files in your container images. This is useful for installing hotfixes, local package builds, or any package that is not available through a package repository. For example, the following <code class="literal">ContainerImagePrepare</code> entry installs some hotfix packages only on the <code class="literal">nova-compute</code> image:
				</p><pre class="screen">ContainerImagePrepare:
- push_destination: true
  ...
  includes:
  - nova-compute
  modify_role: tripleo-modify-image
  modify_append_tag: "-hotfix"
  modify_vars:
    tasks_from: rpm_install.yml
    rpms_path: /home/stack/nova-hotfix-pkgs
  ...</pre></section><section class="section" id="modifying-container-images-with-a-custom-dockerfile"><div class="titlepage"><div><div><h2 class="title">3.14. Modifying container images with a custom Dockerfile</h2></div></div></div><p>
					For maximum flexibility, you can specify a directory containing a Dockerfile to make the required changes. When you invoke the <code class="literal">tripleo-modify-image</code> role, the role generates a <code class="literal">Dockerfile.modified</code> file that changes the <code class="literal">FROM</code> directive and adds extra <code class="literal">LABEL</code> directives. The following example runs the custom Dockerfile on the <code class="literal">nova-compute</code> image:
				</p><pre class="screen">ContainerImagePrepare:
- push_destination: true
  ...
  includes:
  - nova-compute
  modify_role: tripleo-modify-image
  modify_append_tag: "-hotfix"
  modify_vars:
    tasks_from: modify_image.yml
    modify_dir_path: /home/stack/nova-custom
  ...</pre><p>
					The following example shows the <code class="literal">/home/stack/nova-custom/Dockerfile</code> file. After you run any <code class="literal">USER</code> root directives, you must switch back to the original image default user:
				</p><pre class="screen">FROM registry.redhat.io/rhosp-rhel8/openstack-nova-compute:latest

USER "root"

COPY customize.sh /tmp/
RUN /tmp/customize.sh

USER "nova"</pre></section><section class="section" id="preparing-a-satellite-server-for-container-images"><div class="titlepage"><div><div><h2 class="title">3.15. Preparing a Satellite server for container images</h2></div></div></div><p>
					Red Hat Satellite 6 offers registry synchronization capabilities. This provides a method to pull multiple images into a Satellite server and manage them as part of an application life cycle. The Satellite also acts as a registry for other container-enabled systems to use. For more information about managing container images, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_satellite/6.6/html/content_management_guide/managing_container_images">Managing Container Images</a> in the <span class="emphasis"><em>Red Hat Satellite 6 Content Management Guide</em></span>.
				</p><p>
					The examples in this procedure use the <code class="literal">hammer</code> command line tool for Red Hat Satellite 6 and an example organization called <code class="literal">ACME</code>. Substitute this organization for your own Satellite 6 organization.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure requires authentication credentials to access container images from <code class="literal">registry.redhat.io</code>. Instead of using your individual user credentials, Red Hat recommends creating a registry service account and using those credentials to access <code class="literal">registry.redhat.io</code> content. For more information, see <a class="link" href="https://access.redhat.com/RegistryAuthentication">"Red Hat Container Registry Authentication"</a>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a list of all container images:
						</p><pre class="screen">$ sudo podman search --limit 1000 "registry.redhat.io/rhosp" | grep rhosp-rhel8 | awk '{ print $2 }' | grep -v beta | sed "s/registry.redhat.io\///g" | tail -n+2 &gt; satellite_images</pre></li><li class="listitem">
							Copy the <code class="literal">satellite_images</code> file to a system that contains the Satellite 6 <code class="literal">hammer</code> tool. Alternatively, use the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_satellite/6.6/html-single/hammer_cli_guide/index"><span class="emphasis"><em>Hammer CLI Guide</em></span></a> to install the <code class="literal">hammer</code> tool to the undercloud.
						</li><li class="listitem"><p class="simpara">
							Run the following <code class="literal">hammer</code> command to create a new product (<code class="literal">OSP16.1 Containers</code>) in your Satellite organization:
						</p><pre class="screen">$ hammer product create \
  --organization "ACME" \
  --name "OSP16.1 Containers"</pre><p class="simpara">
							This custom product will contain your images.
						</p></li><li class="listitem"><p class="simpara">
							Add the base container image to the product:
						</p><pre class="screen">$ hammer repository create \
  --organization "ACME" \
  --product "OSP16.1 Containers" \
  --content-type docker \
  --url https://registry.redhat.io \
  --docker-upstream-name rhosp-rhel8/openstack-base \
  --upstream-username USERNAME \
  --upstream-password PASSWORD \
  --name base</pre></li><li class="listitem"><p class="simpara">
							Add the overcloud container images from the <code class="literal">satellite_images</code> file:
						</p><pre class="screen">$ while read IMAGE; do \
  IMAGENAME=$(echo $IMAGE | cut -d"/" -f2 | sed "s/openstack-//g" | sed "s/:.*//g") ; \
  hammer repository create \
  --organization "ACME" \
  --product "OSP16.1 Containers" \
  --content-type docker \
  --url https://registry.redhat.io \
  --docker-upstream-name $IMAGE \
  --upstream-username USERNAME \
  --upstream-password PASSWORD \
  --name $IMAGENAME ; done &lt; satellite_images</pre></li><li class="listitem"><p class="simpara">
							Add the Ceph Storage 4 container image:
						</p><pre class="screen">$ hammer repository create \
  --organization "ACME" \
  --product "OSP16.1 Containers" \
  --content-type docker \
  --url https://registry.redhat.io \
  --docker-upstream-name rhceph/rhceph-4-rhel8 \
  --upstream-username USERNAME \
  --upstream-password PASSWORD \
  --name rhceph-4-rhel8</pre></li><li class="listitem"><p class="simpara">
							Synchronize the container images:
						</p><pre class="screen">$ hammer product synchronize \
  --organization "ACME" \
  --name "OSP16.1 Containers"</pre><p class="simpara">
							Wait for the Satellite server to complete synchronization.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Depending on your configuration, <code class="literal">hammer</code> might ask for your Satellite server username and password. You can configure <code class="literal">hammer</code> to automatically login using a configuration file. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_satellite/6.6/html-single/hammer_cli_guide/index#sect-CLI_Guide-Authentication">Authentication</a> section in the <span class="emphasis"><em>Hammer CLI Guide</em></span>.
							</p></div></div></li><li class="listitem">
							If your Satellite 6 server uses content views, create a new content view version to incorporate the images and promote it along environments in your application life cycle. This largely depends on how you structure your application lifecycle. For example, if you have an environment called <code class="literal">production</code> in your lifecycle and you want the container images to be available in that environment, create a content view that includes the container images and promote that content view to the <code class="literal">production</code> environment. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_satellite/6.6/html-single/content_management_guide/index#Managing_Content_Views">Managing Content Views</a>.
						</li><li class="listitem"><p class="simpara">
							Check the available tags for the <code class="literal">base</code> image:
						</p><pre class="screen">$ hammer docker tag list --repository "base" \
  --organization "ACME" \
  --environment "production" \
  --content-view "myosp16_1" \
  --product "OSP16.1 Containers"</pre><p class="simpara">
							This command displays tags for the OpenStack Platform container images within a content view for a particular environment.
						</p></li><li class="listitem"><p class="simpara">
							Return to the undercloud and generate a default environment file that prepares images using your Satellite server as a source. Run the following example command to generate the environment file:
						</p><pre class="screen">(undercloud) $ openstack tripleo container image prepare default \
  --output-env-file containers-prepare-parameter.yaml</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">--output-env-file</code> is an environment file name. The contents of this file include the parameters for preparing your container images for the undercloud. In this case, the name of the file is <code class="literal">containers-prepare-parameter.yaml</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">containers-prepare-parameter.yaml</code> file and modify the following parameters:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">push_destination</code> - Set this to <code class="literal">true</code> or <code class="literal">false</code> depending on your chosen container image management strategy. If you set this parameter to <code class="literal">false</code>, the overcloud nodes pull images directly from the Satellite. If you set this parameter to <code class="literal">true</code>, the director pulls the images from the Satellite to the undercloud registry and the overcloud pulls the images from the undercloud registry.
								</li><li class="listitem">
									<code class="literal">namespace</code> - The URL and port of the registry on the Satellite server. The default registry port on Red Hat Satellite is 5000.
								</li><li class="listitem"><p class="simpara">
									<code class="literal">name_prefix</code> - The prefix is based on a Satellite 6 convention. This differs depending on whether you use content views:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											If you use content views, the structure is <code class="literal">[org]-[environment]-[content view]-[product]-</code>. For example: <code class="literal">acme-production-myosp16-osp16_containers-</code>.
										</li><li class="listitem">
											If you do not use content views, the structure is <code class="literal">[org]-[product]-</code>. For example: <code class="literal">acme-osp16_1_containers-</code>.
										</li></ul></div></li><li class="listitem">
									<code class="literal">ceph_namespace</code>, <code class="literal">ceph_image</code>, <code class="literal">ceph_tag</code> - If you use Ceph Storage, include these additional parameters to define the Ceph Storage container image location. Note that <code class="literal">ceph_image</code> now includes a Satellite-specific prefix. This prefix is the same value as the <code class="literal">name_prefix</code> option.
								</li></ul></div></li></ol></div><p>
					The following example environment file contains Satellite-specific parameters:
				</p><pre class="screen">parameter_defaults:
  ContainerImagePrepare:
  - push_destination: false
    set:
      ceph_image: acme-production-myosp16_1-osp16_1_containers-rhceph-4
      ceph_namespace: satellite.example.com:5000
      ceph_tag: latest
      name_prefix: acme-production-myosp16_1-osp16_1_containers-
      name_suffix: ''
      namespace: satellite.example.com:5000
      neutron_driver: null
      tag: 16.1
      ...
    tag_from_label: '{version}-{release}'</pre><p>
					Use this environment file when you create both your undercloud and overcloud.
				</p></section></section><section class="chapter" id="installing-the-undercloud"><div class="titlepage"><div><div><h2 class="title">Chapter 4. Installing director</h2></div></div></div><section class="section" id="configuring-director"><div class="titlepage"><div><div><h2 class="title">4.1. Configuring director</h2></div></div></div><p>
					The director installation process requires certain settings in the <code class="literal">undercloud.conf</code> configuration file, which director reads from the home directory of the <code class="literal">stack</code> user. Complete the following steps to copy default template as a foundation for your configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the default template to the home directory of the <code class="literal">stack</code> user’s:
						</p><pre class="screen">[stack@director ~]$ cp \
  /usr/share/python-tripleoclient/undercloud.conf.sample \
  ~/undercloud.conf</pre></li><li class="listitem">
							Edit the <code class="literal">undercloud.conf</code> file. This file contains settings to configure your undercloud. If you omit or comment out a parameter, the undercloud installation uses the default value.
						</li></ol></div></section><section class="section" id="director-configuration-parameters"><div class="titlepage"><div><div><h2 class="title">4.2. Director configuration parameters</h2></div></div></div><p>
					The following list contains information about parameters for configuring the <code class="literal">undercloud.conf</code> file. Keep all parameters within their relevant sections to avoid errors.
				</p><div class="formalpara"><p class="title"><strong>Defaults</strong></p><p>
						The following parameters are defined in the <code class="literal">[DEFAULT]</code> section of the <code class="literal">undercloud.conf</code> file:
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">additional_architectures</span></dt><dd><p class="simpara">
								A list of additional (kernel) architectures that an overcloud supports. Currently the overcloud supports <code class="literal">ppc64le</code> architecture.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									When you enable support for ppc64le, you must also set <code class="literal">ipxe_enabled</code> to <code class="literal">False</code>
								</p></div></div></dd><dt><span class="term">certificate_generation_ca</span></dt><dd>
								The <code class="literal">certmonger</code> nickname of the CA that signs the requested certificate. Use this option only if you have set the <code class="literal">generate_service_certificate</code> parameter. If you select the <code class="literal">local</code> CA, certmonger extracts the local CA certificate to <code class="literal">/etc/pki/ca-trust/source/anchors/cm-local-ca.pem</code> and adds the certificate to the trust chain.
							</dd><dt><span class="term">clean_nodes</span></dt><dd>
								Defines whether to wipe the hard drive between deployments and after introspection.
							</dd><dt><span class="term">cleanup</span></dt><dd>
								Cleanup temporary files. Set this to <code class="literal">False</code> to leave the temporary files used during deployment in place after you run the deployment command. This is useful for debugging the generated files or if errors occur.
							</dd><dt><span class="term">container_cli</span></dt><dd>
								The CLI tool for container management. Leave this parameter set to <code class="literal">podman</code>. Red Hat Enterprise Linux 8.2 only supports <code class="literal">podman</code>.
							</dd><dt><span class="term">container_healthcheck_disabled</span></dt><dd>
								Disables containerized service health checks. Red Hat recommends that you enable health checks and leave this option set to <code class="literal">false</code>.
							</dd><dt><span class="term">container_images_file</span></dt><dd><p class="simpara">
								Heat environment file with container image information. This file can contain the following entries:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Parameters for all required container images
									</li><li class="listitem">
										The <code class="literal">ContainerImagePrepare</code> parameter to drive the required image preparation. Usually the file that contains this parameter is named <code class="literal">containers-prepare-parameter.yaml</code>.
									</li></ul></div></dd><dt><span class="term">container_insecure_registries</span></dt><dd>
								A list of insecure registries for <code class="literal">podman</code> to use. Use this parameter if you want to pull images from another source, such as a private container registry. In most cases, <code class="literal">podman</code> has the certificates to pull container images from either the Red Hat Container Catalog or from your Satellite server if the undercloud is registered to Satellite.
							</dd><dt><span class="term">container_registry_mirror</span></dt><dd>
								An optional <code class="literal">registry-mirror</code> configured that <code class="literal">podman</code> uses.
							</dd><dt><span class="term">custom_env_files</span></dt><dd>
								Additional environment files that you want to add to the undercloud installation.
							</dd><dt><span class="term">deployment_user</span></dt><dd>
								The user who installs the undercloud. Leave this parameter unset to use the current default user <code class="literal">stack</code>.
							</dd><dt><span class="term">discovery_default_driver</span></dt><dd>
								Sets the default driver for automatically enrolled nodes. Requires the <code class="literal">enable_node_discovery</code> parameter to be enabled and you must include the driver in the <code class="literal">enabled_hardware_types</code> list.
							</dd><dt><span class="term">enable_ironic; enable_ironic_inspector; enable_mistral; enable_nova; enable_tempest; enable_validations; enable_zaqar</span></dt><dd>
								Defines the core services that you want to enable for director. Leave these parameters set to <code class="literal">true</code>.
							</dd><dt><span class="term">enable_node_discovery</span></dt><dd>
								Automatically enroll any unknown node that PXE-boots the introspection ramdisk. New nodes use the <code class="literal">fake_pxe</code> driver as a default but you can set <code class="literal">discovery_default_driver</code> to override. You can also use introspection rules to specify driver information for newly enrolled nodes.
							</dd><dt><span class="term">enable_novajoin</span></dt><dd>
								Defines whether to install the <code class="literal">novajoin</code> metadata service in the undercloud.
							</dd><dt><span class="term">enable_routed_networks</span></dt><dd>
								Defines whether to enable support for routed control plane networks.
							</dd><dt><span class="term">enable_swift_encryption</span></dt><dd>
								Defines whether to enable Swift encryption at-rest.
							</dd><dt><span class="term">enable_telemetry</span></dt><dd>
								Defines whether to install OpenStack Telemetry services (gnocchi, aodh, panko) in the undercloud. Set the <code class="literal">enable_telemetry</code> parameter to <code class="literal">true</code> if you want to install and configure telemetry services automatically. The default value is <code class="literal">false</code>, which disables telemetry on the undercloud. This parameter is required if you use other products that consume metrics data, such as Red Hat CloudForms.
							</dd><dt><span class="term">enabled_hardware_types</span></dt><dd>
								A list of hardware types that you want to enable for the undercloud.
							</dd><dt><span class="term">generate_service_certificate</span></dt><dd>
								Defines whether to generate an SSL/TLS certificate during the undercloud installation, which is used for the <code class="literal">undercloud_service_certificate</code> parameter. The undercloud installation saves the resulting certificate <code class="literal">/etc/pki/tls/certs/undercloud-[undercloud_public_vip].pem</code>. The CA defined in the <code class="literal">certificate_generation_ca</code> parameter signs this certificate.
							</dd><dt><span class="term">heat_container_image</span></dt><dd>
								URL for the heat container image to use. Leave unset.
							</dd><dt><span class="term">heat_native</span></dt><dd>
								Run host-based undercloud configuration using <code class="literal">heat-all</code>. Leave as <code class="literal">true</code>.
							</dd><dt><span class="term">hieradata_override</span></dt><dd>
								Path to <code class="literal">hieradata</code> override file that configures Puppet hieradata on the director, providing custom configuration to services beyond the <code class="literal">undercloud.conf</code> parameters. If set, the undercloud installation copies this file to the <code class="literal">/etc/puppet/hieradata</code> directory and sets it as the first file in the hierarchy. For more information about using this feature, see <a class="link" href="index.html#configuring-hieradata-on-the-undercloud">Configuring hieradata on the undercloud</a>.
							</dd><dt><span class="term">inspection_extras</span></dt><dd>
								Defines whether to enable extra hardware collection during the inspection process. This parameter requires the <code class="literal">python-hardware</code> or <code class="literal">python-hardware-detect</code> packages on the introspection image.
							</dd><dt><span class="term">inspection_interface</span></dt><dd>
								The bridge that director uses for node introspection. This is a custom bridge that the director configuration creates. The <code class="literal">LOCAL_INTERFACE</code> attaches to this bridge. Leave this as the default <code class="literal">br-ctlplane</code>.
							</dd><dt><span class="term">inspection_runbench</span></dt><dd>
								Runs a set of benchmarks during node introspection. Set this parameter to <code class="literal">true</code> to enable the benchmarks. This option is necessary if you intend to perform benchmark analysis when inspecting the hardware of registered nodes.
							</dd><dt><span class="term">ipa_otp</span></dt><dd>
								Defines the one-time password to register the undercloud node to an IPA server. This is required when <code class="literal">enable_novajoin</code> is enabled.
							</dd><dt><span class="term">ipv6_address_mode</span></dt><dd><p class="simpara">
								IPv6 address configuration mode for the undercloud provisioning network. The following list contains the possible values for this parameter:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										dhcpv6-stateless - Address configuration using router advertisement (RA) and optional information using DHCPv6.
									</li><li class="listitem">
										dhcpv6-stateful - Address configuration and optional information using DHCPv6.
									</li></ul></div></dd><dt><span class="term">ipxe_enabled</span></dt><dd>
								Defines whether to use iPXE or standard PXE. The default is <code class="literal">true</code>, which enables iPXE. Set this parameter to <code class="literal">false</code> to use standard PXE.
							</dd><dt><span class="term">local_interface</span></dt><dd><p class="simpara">
								The chosen interface for the director Provisioning NIC. This is also the device that director uses for DHCP and PXE boot services. Change this value to your chosen device. To see which device is connected, use the <code class="literal">ip addr</code> command. For example, this is the result of an <code class="literal">ip addr</code> command:
							</p><pre class="screen">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:75:24:09 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.178/24 brd 192.168.122.255 scope global dynamic eth0
       valid_lft 3462sec preferred_lft 3462sec
    inet6 fe80::5054:ff:fe75:2409/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noop state DOWN
    link/ether 42:0b:c2:a5:c1:26 brd ff:ff:ff:ff:ff:ff</pre><p class="simpara">
								In this example, the External NIC uses <code class="literal">eth0</code> and the Provisioning NIC uses <code class="literal">eth1</code>, which is currently not configured. In this case, set the <code class="literal">local_interface</code> to <code class="literal">eth1</code>. The configuration script attaches this interface to a custom bridge defined with the <code class="literal">inspection_interface</code> parameter.
							</p></dd><dt><span class="term">local_ip</span></dt><dd>
								The IP address defined for the director Provisioning NIC. This is also the IP address that director uses for DHCP and PXE boot services. Leave this value as the default <code class="literal">192.168.24.1/24</code> unless you use a different subnet for the Provisioning network, for example, if this IP address conflicts with an existing IP address or subnet in your environment.
							</dd><dt><span class="term">local_mtu</span></dt><dd>
								The maximum transmission unit (MTU) that you want to use for the <code class="literal">local_interface</code>. Do not exceed 1500 for the undercloud.
							</dd><dt><span class="term">local_subnet</span></dt><dd>
								The local subnet that you want to use for PXE boot and DHCP interfaces. The <code class="literal">local_ip</code> address should reside in this subnet. The default is <code class="literal">ctlplane-subnet</code>.
							</dd><dt><span class="term">net_config_override</span></dt><dd>
								Path to network configuration override template. If you set this parameter, the undercloud uses a JSON format template to configure the networking with <code class="literal">os-net-config</code> and ignores the network parameters set in <code class="literal">undercloud.conf</code>. Use this parameter when you want to configure bonding or add an option to the interface. See <code class="literal">/usr/share/instack-undercloud/templates/net-config.json.template</code> for an example.
							</dd><dt><span class="term">networks_file</span></dt><dd>
								Networks file to override for <code class="literal">heat</code>.
							</dd><dt><span class="term">output_dir</span></dt><dd>
								Directory to output state, processed heat templates, and Ansible deployment files.
							</dd><dt><span class="term">overcloud_domain_name</span></dt><dd><p class="simpara">
								The DNS domain name that you want to use when you deploy the overcloud.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									When you configure the overcloud, you must set the <code class="literal">CloudDomain</code> parameter to a matching value. Set this parameter in an environment file when you configure your overcloud.
								</p></div></div></dd><dt><span class="term">roles_file</span></dt><dd>
								The roles file that you want to use to override the default roles file for undercloud installation. It is highly recommended to leave this parameter unset so that the director installation uses the default roles file.
							</dd><dt><span class="term">scheduler_max_attempts</span></dt><dd>
								The maximum number of times that the scheduler attempts to deploy an instance. This value must be greater or equal to the number of bare metal nodes that you expect to deploy at once to avoid potential race conditions when scheduling.
							</dd><dt><span class="term">service_principal</span></dt><dd>
								The Kerberos principal for the service using the certificate. Use this parameter only if your CA requires a Kerberos principal, such as in FreeIPA.
							</dd><dt><span class="term">subnets</span></dt><dd>
								List of routed network subnets for provisioning and introspection. The default value includes only the <code class="literal">ctlplane-subnet</code> subnet. For more information, see <a class="xref" href="index.html#director-configuration-parameters_subnets" title="Subnets">Subnets</a>.
							</dd><dt><span class="term">templates</span></dt><dd>
								Heat templates file to override.
							</dd><dt><span class="term">undercloud_admin_host</span></dt><dd>
								The IP address or hostname defined for director Admin API endpoints over SSL/TLS. The director configuration attaches the IP address to the director software bridge as a routed IP address, which uses the <code class="literal">/32</code> netmask.
							</dd><dt><span class="term">undercloud_debug</span></dt><dd>
								Sets the log level of undercloud services to <code class="literal">DEBUG</code>. Set this value to <code class="literal">true</code> to enable <code class="literal">DEBUG</code> log level.
							</dd><dt><span class="term">undercloud_enable_selinux</span></dt><dd>
								Enable or disable SELinux during the deployment. It is highly recommended to leave this value set to <code class="literal">true</code> unless you are debugging an issue.
							</dd><dt><span class="term">undercloud_hostname</span></dt><dd>
								Defines the fully qualified host name for the undercloud. If set, the undercloud installation configures all system host name settings. If left unset, the undercloud uses the current host name, but you must configure all system host name settings appropriately.
							</dd><dt><span class="term">undercloud_log_file</span></dt><dd>
								The path to a log file to store the undercloud install and upgrade logs. By default, the log file is <code class="literal">install-undercloud.log</code> in the home directory. For example, <code class="literal">/home/stack/install-undercloud.log</code>.
							</dd><dt><span class="term">undercloud_nameservers</span></dt><dd>
								A list of DNS nameservers to use for the undercloud hostname resolution.
							</dd><dt><span class="term">undercloud_ntp_servers</span></dt><dd>
								A list of network time protocol servers to help synchronize the undercloud date and time.
							</dd><dt><span class="term">undercloud_public_host</span></dt><dd>
								The IP address or hostname defined for director Public API endpoints over SSL/TLS. The director configuration attaches the IP address to the director software bridge as a routed IP address, which uses the <code class="literal">/32</code> netmask.
							</dd><dt><span class="term">undercloud_service_certificate</span></dt><dd>
								The location and filename of the certificate for OpenStack SSL/TLS communication. Ideally, you obtain this certificate from a trusted certificate authority. Otherwise, generate your own self-signed certificate.
							</dd><dt><span class="term">undercloud_timezone</span></dt><dd>
								Host timezone for the undercloud. If you do not specify a timezone, director uses the existing timezone configuration.
							</dd><dt><span class="term">undercloud_update_packages</span></dt><dd>
								Defines whether to update packages during the undercloud installation.
							</dd></dl></div><div id="director-configuration-parameters_subnets" class="formalpara"><p class="title"><strong>Subnets</strong></p><p>
						Each provisioning subnet is a named section in the <code class="literal">undercloud.conf</code> file. For example, to create a subnet called <code class="literal">ctlplane-subnet</code>, use the following sample in your <code class="literal">undercloud.conf</code> file:
					</p></div><pre class="screen">[ctlplane-subnet]
cidr = 192.168.24.0/24
dhcp_start = 192.168.24.5
dhcp_end = 192.168.24.24
inspection_iprange = 192.168.24.100,192.168.24.120
gateway = 192.168.24.1
masquerade = true</pre><p>
					You can specify as many provisioning networks as necessary to suit your environment.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">cidr</span></dt><dd>
								The network that director uses to manage overcloud instances. This is the Provisioning network, which the undercloud <code class="literal">neutron</code> service manages. Leave this as the default <code class="literal">192.168.24.0/24</code> unless you use a different subnet for the Provisioning network.
							</dd><dt><span class="term">masquerade</span></dt><dd><p class="simpara">
								Defines whether to masquerade the network defined in the <code class="literal">cidr</code> for external access. This provides the Provisioning network with a degree of network address translation (NAT) so that the Provisioning network has external access through director.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The director configuration also enables IP forwarding automatically using the relevant <code class="literal">sysctl</code> kernel parameter.
								</p></div></div></dd><dt><span class="term">dhcp_start; dhcp_end</span></dt><dd>
								The start and end of the DHCP allocation range for overcloud nodes. Ensure that this range contains enough IP addresses to allocate your nodes.
							</dd><dt><span class="term">dhcp_exclude</span></dt><dd>
								IP addresses to exclude in the DHCP allocation range.
							</dd><dt><span class="term">dns_nameservers</span></dt><dd>
								DNS nameservers specific to the subnet. If no nameservers are defined for the subnet, the subnet uses nameservers defined in the <code class="literal">undercloud_nameservers</code> parameter.
							</dd><dt><span class="term">gateway</span></dt><dd>
								The gateway for the overcloud instances. This is the undercloud host, which forwards traffic to the External network. Leave this as the default <code class="literal">192.168.24.1</code> unless you use a different IP address for director or want to use an external gateway directly.
							</dd><dt><span class="term">host_routes</span></dt><dd>
								Host routes for the Neutron-managed subnet for the overcloud instances on this network. This also configures the host routes for the <code class="literal">local_subnet</code> on the undercloud.
							</dd><dt><span class="term">inspection_iprange</span></dt><dd>
								Temporary IP range for nodes on this network to use during the inspection process. This range must not overlap with the range defined by <code class="literal">dhcp_start</code> and <code class="literal">dhcp_end</code> but must be in the same IP subnet.
							</dd></dl></div><p>
					Modify the values of these parameters to suit your configuration. When complete, save the file.
				</p></section><section class="section" id="configuring-the-undercloud-with-environment-files"><div class="titlepage"><div><div><h2 class="title">4.3. Configuring the undercloud with environment files</h2></div></div></div><p>
					You configure the main parameters for the undercloud through the <code class="literal">undercloud.conf</code> file. You can also perform additional undercloud configuration with an environment file that contains heat parameters.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create an environment file named <code class="literal">/home/stack/templates/custom-undercloud-params.yaml</code>.
						</li><li class="listitem"><p class="simpara">
							Edit this file and include your heat parameters. For example, to enable debugging for certain OpenStack Platform services include the following snippet in the <code class="literal">custom-undercloud-params.yaml</code> file:
						</p><pre class="screen">parameter_defaults:
  Debug: True</pre><p class="simpara">
							Save this file when you have finished.
						</p></li><li class="listitem"><p class="simpara">
							Edit your <code class="literal">undercloud.conf</code> file and scroll to the <code class="literal">custom_env_files</code> parameter. Edit the parameter to point to your <code class="literal">custom-undercloud-params.yaml</code> environment file:
						</p><pre class="screen">custom_env_files = /home/stack/templates/custom-undercloud-params.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You can specify multiple environment files using a comma-separated list.
							</p></div></div></li></ol></div><p>
					The director installation includes this environment file during the next undercloud installation or upgrade operation.
				</p></section><section class="section" id="common-heat-parameters-for-undercloud-configuration"><div class="titlepage"><div><div><h2 class="title">4.4. Common heat parameters for undercloud configuration</h2></div></div></div><p>
					The following table contains some common heat parameters that you might set in a custom environment file for your undercloud.
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301185849312" scope="col">Parameter</th><th align="left" valign="top" id="idm140301185848224" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301185849312"> <p>
									<code class="literal">AdminPassword</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301185848224"> <p>
									Sets the undercloud <code class="literal">admin</code> user password.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301185849312"> <p>
									<code class="literal">AdminEmail</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301185848224"> <p>
									Sets the undercloud <code class="literal">admin</code> user email address.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301185849312"> <p>
									<code class="literal">Debug</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301185848224"> <p>
									Enables debug mode.
								</p>
								 </td></tr></tbody></table></div><p>
					Set these parameters in your custom environment file under the <code class="literal">parameter_defaults</code> section:
				</p><pre class="screen">parameter_defaults:
  Debug: True
  AdminPassword: "myp@ssw0rd!"
  AdminEmail: "admin@example.com"</pre></section><section class="section" id="configuring-hieradata-on-the-undercloud"><div class="titlepage"><div><div><h2 class="title">4.5. Configuring hieradata on the undercloud</h2></div></div></div><p>
					You can provide custom configuration for services beyond the available <code class="literal">undercloud.conf</code> parameters by configuring Puppet hieradata on the director.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Create a hieradata override file, for example, <code class="literal">/home/stack/hieradata.yaml</code>.
						</li><li class="listitem"><p class="simpara">
							Add the customized hieradata to the file. For example, add the following snippet to modify the Compute (nova) service parameter <code class="literal">force_raw_images</code> from the default value of <code class="literal">True</code> to <code class="literal">False</code>:
						</p><pre class="screen">nova::compute::force_raw_images: False</pre><p class="simpara">
							If there is no Puppet implementation for the parameter you want to set, then use the following method to configure the parameter:
						</p><pre class="screen">nova::config::nova_config:
  DEFAULT/&lt;parameter_name&gt;:
    value: &lt;parameter_value&gt;</pre><p class="simpara">
							For example:
						</p><pre class="screen">nova::config::nova_config:
  DEFAULT/network_allocate_retries:
    value: 20
  ironic/serial_console_state_timeout:
    value: 15</pre></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">hieradata_override</code> parameter in the <code class="literal">undercloud.conf</code> file to the path of the new <code class="literal">/home/stack/hieradata.yaml</code> file:
						</p><pre class="screen">hieradata_override = /home/stack/hieradata.yaml</pre></li></ol></div></section><section class="section" id="configuring-the-undercloud-for-bare-metal-provisioning-over-ipv6"><div class="titlepage"><div><div><h2 class="title">4.6. Configuring the undercloud for bare metal provisioning over IPv6</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					If you have IPv6 nodes and infrastructure, you can configure the undercloud and the provisioning network to use IPv6 instead of IPv4 so that director can provision and deploy Red Hat OpenStack Platform onto IPv6 nodes. However, there are some considerations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Dual stack IPv4/6 is not available.
						</li><li class="listitem">
							Tempest validations might not perform correctly.
						</li><li class="listitem">
							IPv4 to IPv6 migration is not available during upgrades.
						</li></ul></div><p>
					Modify the <code class="literal">undercloud.conf</code> file to enable IPv6 provisioning in Red Hat OpenStack Platform.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							An IPv6 address on the undercloud. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/ipv6_networking_for_the_overcloud/index#sect-pre-Configuring_an_IPv6_on_the_Undercloud">Configuring an IPv6 address on the undercloud</a> in the <span class="emphasis"><em>IPv6 Networking for the Overcloud</em></span> guide.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Copy the sample <code class="literal">undercloud.conf</code> file, or modify your existing <code class="literal">undercloud.conf</code> file.
						</li><li class="listitem"><p class="simpara">
							Set the following parameter values in the <code class="literal">undercloud.conf</code> file:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Set <code class="literal">ipv6_address_mode</code> to <code class="literal">dhcpv6-stateless</code> or <code class="literal">dhcpv6-stateful</code> if your NIC supports stateful DHCPv6 with Red Hat OpenStack Platform.
								</li><li class="listitem">
									Set <code class="literal">enable_routed_networks</code> to <code class="literal">true</code> if you do not want the undercloud to create a router on the provisioning network. In this case, the data center router must provide router advertisements. Otherwise, set this value to <code class="literal">false</code>.
								</li><li class="listitem">
									Set <code class="literal">local_ip</code> to the IPv6 address of the undercloud.
								</li><li class="listitem">
									Use IPv6 addressing for the undercloud interface parameters <code class="literal">undercloud_public_host</code> and <code class="literal">undercloud_admin_host</code>.
								</li><li class="listitem"><p class="simpara">
									Optional. If you want to use stateful DHCPv6, use the <code class="literal">ironic_enabled_network_interfaces</code> parameter to specify the neutron interface. You can also use the <code class="literal">ironic_default_network_interface</code> parameter to set the neutron interface as the default network interface for bare metal nodes:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">ironic_enabled_network_interfaces = neutron,flat</code>
										</li><li class="listitem">
											<code class="literal">ironic_default_network_interface = neutron</code>
										</li></ul></div></li><li class="listitem"><p class="simpara">
									In the <code class="literal">[ctlplane-subnet]</code> section, use IPv6 addressing in the following parameters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">cidr</code>
										</li><li class="listitem">
											<code class="literal">dhcp_start</code>
										</li><li class="listitem">
											<code class="literal">dhcp_end</code>
										</li><li class="listitem">
											<code class="literal">gateway</code>
										</li><li class="listitem">
											<code class="literal">inspection_iprange</code>
										</li></ul></div></li><li class="listitem"><p class="simpara">
									In the <code class="literal">[ctlplane-subnet]</code> section, set an IPv6 nameserver for the subnet in the <code class="literal">dns_nameservers</code> parameter.
								</p><pre class="screen">[DEFAULT]
ipv6_address_mode = dhcpv6-stateless
enable_routed_networks: false
local_ip = &lt;ipv6-address&gt;
ironic_enabled_network_interfaces = neutron,flat
ironic_default_network_interface = neutron
undercloud_admin_host = &lt;ipv6-address&gt;
undercloud_public_host = &lt;ipv6-address&gt;

[ctlplane-subnet]
cidr = &lt;ipv6-address&gt;::&lt;ipv6-mask&gt;
dhcp_start = &lt;ipv6-address&gt;
dhcp_end = &lt;ipv6-address&gt;
dns_nameservers = &lt;ipv6-dns&gt;
gateway = &lt;ipv6-address&gt;
inspection_iprange = &lt;ipv6-address&gt;,&lt;ipv6-address&gt;</pre></li></ol></div></li></ol></div></section><section class="section" id="installing-director"><div class="titlepage"><div><div><h2 class="title">4.7. Installing director</h2></div></div></div><p>
					Complete the following steps to install director and perform some basic post-installation tasks.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to install director on the undercloud:
						</p><pre class="screen">[stack@director ~]$ openstack undercloud install</pre><p class="simpara">
							This command launches the director configuration script. Director installs additional packages and configures its services according to the configuration in the <code class="literal">undercloud.conf</code>. This script takes several minutes to complete.
						</p><p class="simpara">
							The script generates two files:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">undercloud-passwords.conf</code> - A list of all passwords for the director services.
								</li><li class="listitem">
									<code class="literal">stackrc</code> - A set of initialization variables to help you access the director command line tools.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							The script also starts all OpenStack Platform service containers automatically. You can check the enabled containers with the following command:
						</p><pre class="screen">[stack@director ~]$ sudo podman ps</pre></li><li class="listitem"><p class="simpara">
							To initialize the <code class="literal">stack</code> user to use the command line tools, run the following command:
						</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre><p class="simpara">
							The prompt now indicates that OpenStack commands authenticate and execute against the undercloud;
						</p><pre class="screen">(undercloud) [stack@director ~]$</pre></li></ol></div><p>
					The director installation is complete. You can now use the director command line tools.
				</p></section><section class="section" id="sect-Obtaining_Images_for_Overcloud_Nodes"><div class="titlepage"><div><div><h2 class="title">4.8. Obtaining images for overcloud nodes</h2></div></div></div><p>
					Director requires several disk images to provision overcloud nodes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							An introspection kernel and ramdisk for bare metal system introspection over PXE boot.
						</li><li class="listitem">
							A deployment kernel and ramdisk for system provisioning and deployment.
						</li><li class="listitem">
							An overcloud kernel, ramdisk, and full image. which form a base overcloud system that is written to the hard disk of the node.
						</li></ul></div><p>
					The following procedure shows how to obtain and install these images.
				</p><section class="section" id="single_cpu_architecture_overclouds"><div class="titlepage"><div><div><h3 class="title">4.8.1. Single CPU architecture overclouds</h3></div></div></div><p>
						These images and procedures are necessary for deployment of the overcloud with the default CPU architecture, x86-64.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Source the <code class="literal">stackrc</code> file to enable the director command line tools:
							</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
								Install the <code class="literal">rhosp-director-images</code> and <code class="literal">rhosp-director-images-ipa</code> packages:
							</p><pre class="screen">(undercloud) [stack@director ~]$ sudo dnf install rhosp-director-images rhosp-director-images-ipa</pre></li><li class="listitem"><p class="simpara">
								Extract the images archives to the <code class="literal">images</code> directory in the home directory of the <code class="literal">stack</code> user (<code class="literal">/home/stack/images</code>):
							</p><pre class="screen">(undercloud) [stack@director ~]$ cd ~/images
(undercloud) [stack@director images]$ for i in /usr/share/rhosp-director-images/overcloud-full-latest-16.1.tar /usr/share/rhosp-director-images/ironic-python-agent-latest-16.1.tar; do tar -xvf $i; done</pre></li><li class="listitem"><p class="simpara">
								Import these images into director:
							</p><pre class="screen">(undercloud) [stack@director images]$ openstack overcloud image upload --image-path /home/stack/images/</pre><p class="simpara">
								This script uploads the following images into director:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">overcloud-full</code>
									</li><li class="listitem">
										<code class="literal">overcloud-full-initrd</code>
									</li><li class="listitem">
										<code class="literal">overcloud-full-vmlinuz</code>
									</li></ul></div><p class="simpara">
								The script also installs the introspection images on the director PXE server.
							</p></li><li class="listitem"><p class="simpara">
								Verify that the images uploaded successfully:
							</p><pre class="screen">(undercloud) [stack@director images]$ openstack image list
+--------------------------------------+------------------------+
| ID                                   | Name                   |
+--------------------------------------+------------------------+
| ef793cd0-e65c-456a-a675-63cd57610bd5 | overcloud-full         |
| 9a51a6cb-4670-40de-b64b-b70f4dd44152 | overcloud-full-initrd  |
| 4f7e33f4-d617-47c1-b36f-cbe90f132e5d | overcloud-full-vmlinuz |
+--------------------------------------+------------------------+</pre><p class="simpara">
								This list does not show the introspection PXE images. Director copies these files to <code class="literal">/var/lib/ironic/httpboot</code>.
							</p><pre class="screen">(undercloud) [stack@director images]$ ls -l /var/lib/ironic/httpboot
total 417296
-rwxr-xr-x. 1 root  root    6639920 Jan 29 14:48 agent.kernel
-rw-r--r--. 1 root  root  420656424 Jan 29 14:48 agent.ramdisk
-rw-r--r--. 1 42422 42422       758 Jan 29 14:29 boot.ipxe
-rw-r--r--. 1 42422 42422       488 Jan 29 14:16 inspector.ipxe</pre></li></ol></div></section><section class="section" id="multiple_cpu_architecture_overclouds"><div class="titlepage"><div><div><h3 class="title">4.8.2. Multiple CPU architecture overclouds</h3></div></div></div><p>
						These are the images and procedures that are necessary to deploy the overcloud to enable support of additional CPU architectures.
					</p><p>
						The following example procedure uses the ppc64le image.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Source the <code class="literal">stackrc</code> file to enable the director command line tools:
							</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
								Install the <code class="literal">rhosp-director-images-all</code> package:
							</p><pre class="screen">(undercloud) [stack@director ~]$ sudo dnf install rhosp-director-images-all</pre></li><li class="listitem"><p class="simpara">
								Extract the archives to an architecture specific directory in the <code class="literal">images</code> directory in the home directory of the <code class="literal">stack</code> user (<code class="literal">/home/stack/images</code>):
							</p><pre class="screen">(undercloud) [stack@director ~]$ cd ~/images
(undercloud) [stack@director images]$ for arch in x86_64 ppc64le ; do mkdir $arch ; done
(undercloud) [stack@director images]$ for arch in x86_64 ppc64le ; do for i in /usr/share/rhosp-director-images/overcloud-full-latest-16.1-${arch}.tar /usr/share/rhosp-director-images/ironic-python-agent-latest-16.1-${arch}.tar ; do tar -C $arch -xf $i ; done ; done</pre></li><li class="listitem"><p class="simpara">
								Import these images into director:
							</p><pre class="screen">(undercloud) [stack@director ~]$ cd ~/images
(undercloud) [stack@director images]$ openstack overcloud image upload --local --image-path ~/images/ppc64le --architecture ppc64le --whole-disk --http-boot /var/lib/ironic/tftpboot/ppc64le
(undercloud) [stack@director images]$ openstack overcloud image upload --local --image-path ~/images/x86_64/ --http-boot /var/lib/ironic/tftpboot</pre><p class="simpara">
								These commands import the following images into director:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">overcloud-full</code>
									</li><li class="listitem">
										<code class="literal">overcloud-full-initrd</code>
									</li><li class="listitem">
										<code class="literal">overcloud-full-vmlinuz</code>
									</li><li class="listitem">
										<code class="literal">ppc64le-bm-deploy-kernel</code>
									</li><li class="listitem">
										<code class="literal">ppc64le-bm-deploy-ramdisk</code>
									</li><li class="listitem"><p class="simpara">
										<code class="literal">ppc64le-overcloud-full</code>
									</p><p class="simpara">
										The script also installs the introspection images on the director PXE server.
									</p></li></ul></div></li><li class="listitem"><p class="simpara">
								Verify that the images uploaded successfully:
							</p><pre class="screen">(undercloud) [stack@director images]$ openstack image list
+--------------------------------------+---------------------------+--------+
| ID                                   | Name                      | Status |
+--------------------------------------+---------------------------+--------+
| 6a6096ba-8f79-4343-b77c-4349f7b94960 | overcloud-full            | active |
| de2a1bde-9351-40d2-bbd7-7ce9d6eb50d8 | overcloud-full-initrd     | active |
| 67073533-dd2a-4a95-8e8b-0f108f031092 | overcloud-full-vmlinuz    | active |
| 69a9ffe5-06dc-4d81-a122-e5d56ed46c98 | ppc64le-bm-deploy-kernel  | active |
| 464dd809-f130-4055-9a39-cf6b63c1944e | ppc64le-bm-deploy-ramdisk | active |
| f0fedcd0-3f28-4b44-9c88-619419007a03 | ppc64le-overcloud-full    | active |
+--------------------------------------+---------------------------+--------+</pre><p class="simpara">
								This list does not show the introspection PXE images. Director copies these files to <code class="literal">/tftpboot</code>.
							</p><pre class="screen">(undercloud) [stack@director images]$ ls -l /var/lib/ironic/tftpboot /var/lib/ironic/tftpboot/ppc64le/
/var/lib/ironic/tftpboot:
total 422624
-rwxr-xr-x. 1 root   root     6385968 Aug  8 19:35 agent.kernel
-rw-r--r--. 1 root   root   425530268 Aug  8 19:35 agent.ramdisk
-rwxr--r--. 1 ironic ironic     20832 Aug  8 02:08 chain.c32
-rwxr--r--. 1 ironic ironic    715584 Aug  8 02:06 ipxe.efi
-rw-r--r--. 1 root   root          22 Aug  8 02:06 map-file
drwxr-xr-x. 2 ironic ironic        62 Aug  8 19:34 ppc64le
-rwxr--r--. 1 ironic ironic     26826 Aug  8 02:08 pxelinux.0
drwxr-xr-x. 2 ironic ironic        21 Aug  8 02:06 pxelinux.cfg
-rwxr--r--. 1 ironic ironic     69631 Aug  8 02:06 undionly.kpxe

/var/lib/ironic/tftpboot/ppc64le/:
total 457204
-rwxr-xr-x. 1 root             root              19858896 Aug  8 19:34 agent.kernel
-rw-r--r--. 1 root             root             448311235 Aug  8 19:34 agent.ramdisk
-rw-r--r--. 1 ironic-inspector ironic-inspector       336 Aug  8 02:06 default</pre></li></ol></div></section><section class="section" id="sect-obtaining-overcloud-minimal-image"><div class="titlepage"><div><div><h3 class="title">4.8.3. Minimal overcloud image</h3></div></div></div><p>
						You can use the <code class="literal">overcloud-minimal</code> image to provision a bare OS where you do not want to run any other Red Hat OpenStack Platform services or consume one of your subscription entitlements.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Source the <code class="literal">stackrc</code> file to enable the director command line tools:
							</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
								Install the <code class="literal">overcloud-minimal</code> package:
							</p><pre class="screen">(undercloud) [stack@director ~]$ sudo dnf install rhosp-director-images-minimal</pre></li><li class="listitem"><p class="simpara">
								Extract the images archives to the <code class="literal">images</code> directory in the home directory of the <code class="literal">stack</code> user (<code class="literal">/home/stack/images</code>):
							</p><pre class="screen">(undercloud) [stack@director ~]$ cd ~/images
(undercloud) [stack@director images]$ tar xf /usr/share/rhosp-director-images/overcloud-minimal-latest-16.1.tar</pre></li><li class="listitem"><p class="simpara">
								Import the images into director:
							</p><pre class="screen">(undercloud) [stack@director images]$ openstack overcloud image upload --image-path /home/stack/images/ --os-image-name overcloud-minimal.qcow2</pre><p class="simpara">
								This script uploads the following images into director:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">overcloud-minimal</code>
									</li><li class="listitem">
										<code class="literal">overcloud-minimal-initrd</code>
									</li><li class="listitem">
										<code class="literal">overcloud-minimal-vmlinuz</code>
									</li></ul></div></li><li class="listitem"><p class="simpara">
								Verify that the images uploaded successfully:
							</p><pre class="screen">(undercloud) [stack@director images]$ openstack image list
+--------------------------------------+---------------------------+
| ID                                   | Name                      |
+--------------------------------------+---------------------------+
| ef793cd0-e65c-456a-a675-63cd57610bd5 | overcloud-full            |
| 9a51a6cb-4670-40de-b64b-b70f4dd44152 | overcloud-full-initrd     |
| 4f7e33f4-d617-47c1-b36f-cbe90f132e5d | overcloud-full-vmlinuz    |
| 32cf6771-b5df-4498-8f02-c3bd8bb93fdd | overcloud-minimal         |
| 600035af-dbbb-4985-8b24-a4e9da149ae5 | overcloud-minimal-initrd  |
| d45b0071-8006-472b-bbcc-458899e0d801 | overcloud-minimal-vmlinuz |
+--------------------------------------+---------------------------+</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The default <code class="literal">overcloud-full.qcow2</code> image is a flat partition image. However, you can also import and use whole disk images. For more information, see <a class="xref" href="index.html#creating-whole-disk-images" title="Chapter 23. Creating whole disk images">Chapter 23, <em>Creating whole disk images</em></a>.
						</p></div></div></section></section><section class="section" id="sect-Setting_a_Nameserver_on_the_Underclouds_Neutron_Subnet"><div class="titlepage"><div><div><h2 class="title">4.9. Setting a nameserver for the control plane</h2></div></div></div><p>
					If you intend for the overcloud to resolve external hostnames, such as <code class="literal">cdn.redhat.com</code>, set a nameserver on the overcloud nodes. For a standard overcloud without network isolation, the nameserver is defined using the undercloud control plane subnet. Complete the following procedure to define nameservers for the environment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file to enable the director command line tools:
						</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Set the nameservers for the <code class="literal">ctlplane-subnet</code> subnet:
						</p><pre class="screen">(undercloud) [stack@director images]$ openstack subnet set --dns-nameserver [nameserver1-ip] --dns-nameserver [nameserver2-ip] ctlplane-subnet</pre><p class="simpara">
							Use the <code class="literal">--dns-nameserver</code> option for each nameserver.
						</p></li><li class="listitem"><p class="simpara">
							View the subnet to verify the nameserver:
						</p><pre class="screen">(undercloud) [stack@director images]$ openstack subnet show ctlplane-subnet
+-------------------+-----------------------------------------------+
| Field             | Value                                         |
+-------------------+-----------------------------------------------+
| ...               |                                               |
| dns_nameservers   | 8.8.8.8                                       |
| ...               |                                               |
+-------------------+-----------------------------------------------+</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						If you aim to isolate service traffic onto separate networks, the overcloud nodes use the <code class="literal">DnsServers</code> parameter in your network environment files.
					</p></div></div></section><section class="section" id="installing-the-undercloud-configuration"><div class="titlepage"><div><div><h2 class="title">4.10. Updating the undercloud configuration</h2></div></div></div><p>
					If you need to change the undercloud configuration to suit new requirements, you can make changes to your undercloud configuration after installation, edit the relevant configuration files and re-run the <code class="literal">openstack undercloud install</code> command.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Modify the undercloud configuration files. For example, edit the <code class="literal">undercloud.conf</code> file and add the <code class="literal">idrac</code> hardware type to the list of enabled hardware types:
						</p><pre class="screen">enabled_hardware_types = ipmi,redfish,idrac</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack undercloud install</code> command to refresh your undercloud with the new changes:
						</p><pre class="screen">[stack@director ~]$ openstack undercloud install</pre><p class="simpara">
							Wait until the command runs to completion.
						</p></li><li class="listitem"><p class="simpara">
							Initialize the <code class="literal">stack</code> user to use the command line tools,:
						</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre><p class="simpara">
							The prompt now indicates that OpenStack commands authenticate and execute against the undercloud:
						</p><pre class="screen">(undercloud) [stack@director ~]$</pre></li><li class="listitem"><p class="simpara">
							Verify that director has applied the new configuration. For this example, check the list of enabled hardware types:
						</p><pre class="screen">(undercloud) [stack@director ~]$ openstack baremetal driver list
+---------------------+----------------+
| Supported driver(s) | Active host(s) |
+---------------------+----------------+
| idrac               | unused         |
| ipmi                | unused         |
| redfish             | unused         |
+---------------------+----------------+</pre></li></ol></div><p>
					The undercloud re-configuration is complete.
				</p></section><section class="section" id="undercloud-container-registry"><div class="titlepage"><div><div><h2 class="title">4.11. Undercloud container registry</h2></div></div></div><p>
					Red Hat Enterprise Linux 8.2 no longer includes the <code class="literal">docker-distribution</code> package, which installed a Docker Registry v2. To maintain the compatibility and the same level of feature, the director installation creates an Apache web server with a vhost called <code class="literal">image-serve</code> to provide a registry. This registry also uses port 8787/TCP with SSL disabled. The Apache-based registry is not containerized, which means that you must run the following command to restart the registry:
				</p><pre class="screen">$ sudo systemctl restart httpd</pre><p>
					You can find the container registry logs in the following locations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							/var/log/httpd/image_serve_access.log
						</li><li class="listitem">
							/var/log/httpd/image_serve_error.log.
						</li></ul></div><p>
					The image content is served from <code class="literal">/var/lib/image-serve</code>. This location uses a specific directory layout and <code class="literal">apache</code> configuration to implement the pull function of the registry REST API.
				</p><p>
					The Apache-based registry does not support <code class="literal">podman push</code> nor <code class="literal">buildah push</code> commands, which means that you cannot push container images using traditional methods. To modify images during deployment, use the container preparation workflow, such as the <code class="literal">ContainerImagePrepare</code> parameter. To manage container images, use the container management commands:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">sudo openstack tripleo container image list</span></dt><dd>
								Lists all images stored on the registry.
							</dd><dt><span class="term">sudo openstack tripleo container image show</span></dt><dd>
								Show metadata for a specific image on the registry.
							</dd><dt><span class="term">sudo openstack tripleo container image push</span></dt><dd>
								Push an image from a remote registry to the undercloud registry.
							</dd><dt><span class="term">sudo openstack tripleo container image delete</span></dt><dd>
								Delete an image from the registry.
							</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You must run all container image management commands with <code class="literal">sudo</code> level permissions.
					</p></div></div></section><section class="section" id="next_steps"><div class="titlepage"><div><div><h2 class="title">4.12. Next steps</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Install an undercloud minion to scale undercloud services. See <a class="xref" href="index.html#installing-undercloud-minions" title="Chapter 5. Installing undercloud minions">Chapter 5, <em>Installing undercloud minions</em></a>.
						</li><li class="listitem">
							Perform basic overcloud configuration, including registering nodes, inspecting them, and then tagging them into various node roles. For more information, see <a class="xref" href="index.html#creating-a-basic-overcloud-with-cli-tools" title="Chapter 7. Configuring a basic overcloud with CLI tools">Chapter 7, <em>Configuring a basic overcloud with CLI tools</em></a>.
						</li></ul></div></section></section><section class="chapter" id="installing-undercloud-minions"><div class="titlepage"><div><div><h2 class="title">Chapter 5. Installing undercloud minions</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p></div></div><section class="section" id="undercloud-minion"><div class="titlepage"><div><div><h2 class="title">5.1. Undercloud minion</h2></div></div></div><p>
					An undercloud minion provides additional <code class="literal">heat-engine</code> and <code class="literal">ironic-conductor</code> services on a separate host. These additional services support the undercloud with orchestration and provisioning operations. The distribution of undercloud operations across multiple hosts provides more resources to run an overcloud deployment, which can result in potentially faster and larger deployments.
				</p></section><section class="section" id="undercloud-minion-requirements"><div class="titlepage"><div><div><h2 class="title">5.2. Undercloud minion requirements</h2></div></div></div><p>
					The scaled <code class="literal">heat-engine</code> and <code class="literal">ironic-conductor</code> services on an undercloud minion use a set of workers. Each worker performs operations specific to that service. Multiple workers provide simultaneous operations. The default number of workers on the minion is determined by halving the total CPU thread count of the minion host. In this instance, total thread count is the number of CPU cores multiplied by the hyper-threading value. For example, if your minion has a CPU with 16 threads, then the minion spawns 8 workers for each service by default. The minion also uses a set of minimum and maximum caps by default:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301182927264" scope="col">Service</th><th align="left" valign="top" id="idm140301182926176" scope="col">Minimum</th><th align="left" valign="top" id="idm140301182925088" scope="col">Maximum</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301182927264"> <p>
									<code class="literal">heat-engine</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182926176"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm140301182925088"> <p>
									24
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182927264"> <p>
									<code class="literal">ironic-conductor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182926176"> <p>
									2
								</p>
								 </td><td align="left" valign="top" headers="idm140301182925088"> <p>
									12
								</p>
								 </td></tr></tbody></table></div><p>
					An undercloud minion has the following minimum CPU and memory requirements:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							An 8-thread 64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions. This processor provides 4 workers for each undercloud service.
						</li><li class="listitem">
							A minimum of 16 GB of RAM.
						</li></ul></div><p>
					To use a larger number of workers, increase the vCPUs and memory count on the undercloud using a ratio of 2 GB of RAM for each CPU thread. For example, a machine with 48 threads must have 96 GB of RAM. This provides coverage for 24 <code class="literal">heat-engine</code> workers and 12 <code class="literal">ironic-conductor</code> workers.
				</p></section><section class="section" id="preparing-the-minion"><div class="titlepage"><div><div><h2 class="title">5.3. Preparing a minion</h2></div></div></div><p>
					Before you can install a minion, you must complete some basic configuration on the host machine:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A non-root user to execute commands.
						</li><li class="listitem">
							A resolvable hostname
						</li><li class="listitem">
							A Red Hat subscription
						</li><li class="listitem">
							The command line tools for image preparation and minion installation
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the minion host as the <code class="literal">root</code> user.
						</li><li class="listitem"><p class="simpara">
							Create the <code class="literal">stack</code> user:
						</p><pre class="screen">[root@minion ~]# useradd stack</pre></li><li class="listitem"><p class="simpara">
							Set a password for the <code class="literal">stack</code> user:
						</p><pre class="screen">[root@minion ~]# passwd stack</pre></li><li class="listitem"><p class="simpara">
							Disable password requirements when using <code class="literal">sudo</code>:
						</p><pre class="screen">[root@minion ~]# echo "stack ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/stack
[root@minion ~]# chmod 0440 /etc/sudoers.d/stack</pre></li><li class="listitem"><p class="simpara">
							Switch to the new <code class="literal">stack</code> user:
						</p><pre class="screen">[root@minion ~]# su - stack
[stack@minion ~]$</pre></li><li class="listitem"><p class="simpara">
							Check the base and full hostname of the minion:
						</p><pre class="screen">[stack@minion ~]$ hostname
[stack@minion ~]$ hostname -f</pre><p class="simpara">
							If either of the previous commands do not report the correct fully-qualified hostname or report an error, use <code class="literal">hostnamectl</code> to set a hostname:
						</p><pre class="screen">[stack@minion ~]$ sudo hostnamectl set-hostname minion.example.com
[stack@minion ~]$ sudo hostnamectl set-hostname --transient minion.example.com</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">/etc/hosts</code> file and include an entry for the system hostname. For example, if the system is named <code class="literal">minion.example.com</code> and uses the IP address <code class="literal">10.0.0.1</code>, add the following line to the <code class="literal">/etc/hosts</code> file:
						</p><pre class="screen">10.0.0.1  minion.example.com manager</pre></li><li class="listitem"><p class="simpara">
							Register your system either with the Red Hat Content Delivery Network or Red Hat Satellite. For example, run the following command to register the system to the Content Delivery Network. Enter your Customer Portal user name and password when prompted:
						</p><pre class="screen">[stack@minion ~]$ sudo subscription-manager register</pre></li><li class="listitem"><p class="simpara">
							Find the entitlement pool ID for Red Hat OpenStack Platform (RHOSP) director:
						</p><pre class="screen">[stack@minion ~]$ sudo subscription-manager list --available --all --matches="Red Hat OpenStack"
Subscription Name:   Name of SKU
Provides:            Red Hat Single Sign-On
                     Red Hat Enterprise Linux Workstation
                     Red Hat CloudForms
                     Red Hat OpenStack
                     Red Hat Software Collections (for RHEL Workstation)
                     Red Hat Virtualization
SKU:                 SKU-Number
Contract:            Contract-Number
Pool ID:             Valid-Pool-Number-123456
Provides Management: Yes
Available:           1
Suggested:           1
Service Level:       Support-level
Service Type:        Service-Type
Subscription Type:   Sub-type
Ends:                End-date
System Type:         Physical</pre></li><li class="listitem"><p class="simpara">
							Locate the <code class="literal">Pool ID</code> value and attach the Red Hat OpenStack Platform 16.1 entitlement:
						</p><pre class="screen">[stack@minion ~]$ sudo subscription-manager attach --pool=Valid-Pool-Number-123456</pre></li><li class="listitem"><p class="simpara">
							Disable all default repositories, and then enable the required Red Hat Enterprise Linux repositories:
						</p><pre class="screen">[stack@minion ~]$ sudo subscription-manager repos --disable=*
[stack@minion ~]$ sudo subscription-manager repos --enable=rhel-8-for-x86_64-baseos-eus-rpms --enable=rhel-8-for-x86_64-appstream-eus-rpms --enable=rhel-8-for-x86_64-highavailability-eus-rpms --enable=ansible-2.9-for-rhel-8-x86_64-rpms --enable=openstack-16.1-for-rhel-8-x86_64-rpms --enable=fast-datapath-for-rhel-8-x86_64-rpms</pre><p class="simpara">
							These repositories contain packages that the minion installation requires.
						</p></li><li class="listitem"><p class="simpara">
							Perform an update on your system to ensure that you have the latest base system packages:
						</p><pre class="screen">[stack@minion ~]$ sudo dnf update -y
[stack@minion ~]$ sudo reboot</pre></li><li class="listitem"><p class="simpara">
							Install the command line tools for minion installation and configuration:
						</p><pre class="screen">[stack@minion ~]$ sudo dnf install -y python3-tripleoclient</pre></li></ol></div></section><section class="section" id="copying-the-undercloud-configuration-files-to-the-minion"><div class="titlepage"><div><div><h2 class="title">5.4. Copying the undercloud configuration files to the minion</h2></div></div></div><p>
					The minion requires some configuration files from the undercloud so that the minion installation can configure the minion services and register them with director:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">tripleo-undercloud-outputs.yaml</code>
						</li><li class="listitem">
							<code class="literal">tripleo-undercloud-passwords.yaml</code>
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Copy the files from the undercloud to the minion:
						</p><pre class="screen">$ scp ~/tripleo-undercloud-outputs.yaml ~/tripleo-undercloud-passwords.yaml stack@&lt;minion-host&gt;:~/.</pre><p class="simpara">
							Replace <code class="literal">&lt;minion-host&gt;</code> with the hostname or IP address of the minion.
						</p></li></ol></div></section><section class="section" id="copying-the-undercloud-certificate-authority"><div class="titlepage"><div><div><h2 class="title">5.5. Copying the undercloud certificate authority</h2></div></div></div><p>
					If the undercloud uses SSL/TLS for endpoint encryption, the minion host must contain the certificate authority that signed the undercloud SSL/TLS certificates. Depending on your undercloud configuration, this certificate authority is one of the following:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							An external certificate authority whose certificate is preloaded on the minion host. No action is required.
						</li><li class="listitem">
							A director-generated self-signed certificate authority, which the director creates at <code class="literal">/etc/pki/ca-trust/source/anchors/cm-local-ca.pem</code>. Copy this file to the minion host and include the file as a part of the trusted certificate authorities for the minion host. This procedure uses this file as an example.
						</li><li class="listitem">
							A custom self-signed certificate authority, which you create with OpenSSL. Examples in this document refer to this file as <code class="literal">ca.crt.pem</code>. Copy this file to the minion host and include the file as a part of the trusted certificate authorities for the minion host.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the minion host as the <code class="literal">root</code> user.
						</li><li class="listitem"><p class="simpara">
							Copy the certificate authority file from the undercloud to the minion:
						</p><pre class="screen">[root@minion ~]# scp \
    root@&lt;undercloud-host&gt;:/etc/pki/ca-trust/source/anchors/cm-local-ca.pem \
    /etc/pki/ca-trust/source/anchors/undercloud-ca.pem</pre><p class="simpara">
							Replace <code class="literal">&lt;undercloud-host&gt;</code> with the hostname or IP address of the undercloud.
						</p></li><li class="listitem"><p class="simpara">
							Update the trusted certificate authorities for the minion host:
						</p><pre class="screen">[root@minion ~]# update-ca-trust enable
[root@minion ~]# update-ca-trust extract</pre></li></ol></div></section><section class="section" id="configuring-the-minion"><div class="titlepage"><div><div><h2 class="title">5.6. Configuring the minion</h2></div></div></div><p>
					The minion installation process requires certain settings in the <code class="literal">minion.conf</code> configuration file, which the minion reads from the home directory of the <code class="literal">stack</code> user. Complete the following steps to use the default template as a foundation for your configuration.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the minion host as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Copy the default template to the home directory of the <code class="literal">stack</code> user:
						</p><pre class="screen">[stack@minion ~]$ cp \
  /usr/share/python-tripleoclient/minion.conf.sample \
  ~/minion.conf</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">minion.conf</code> file. This file contains settings to configure your minion. If you omit or comment out a parameter, the minion installation uses the default value. Review the following recommended parameters:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">minion_hostname</code>, which you set to the hostname of the minion.
								</li><li class="listitem">
									<code class="literal">minion_local_interface</code>, which you set to the interface that connects to the undercloud through the Provisioning Network.
								</li><li class="listitem">
									<code class="literal">minion_local_ip</code>, which you set to a free IP address on the Provisioning Network.
								</li><li class="listitem">
									<code class="literal">minion_nameservers</code>, which you set to the DNS nameservers so that the minion can resolve hostnames.
								</li><li class="listitem">
									<code class="literal">enable_ironic_conductor</code>, which defines whether to enable the <code class="literal">ironic-conductor</code> service.
								</li><li class="listitem">
									<code class="literal">enable_heat_engine</code>, which defines whether to enable the <code class="literal">heat-engine</code> service.
								</li></ul></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The default <code class="literal">minion.conf</code> file enables only the <code class="literal">heat-engine</code> service on the minion. To enable the <code class="literal">ironic-conductor</code> service, set the <code class="literal">enable_ironic_conductor</code> parameter to <code class="literal">true</code>.
					</p></div></div></section><section class="section" id="minion-configuration-parameters"><div class="titlepage"><div><div><h2 class="title">5.7. Minion configuration parameters</h2></div></div></div><p>
					The following list contains information about parameters for configuring the <code class="literal">minion.conf</code> file. Keep all parameters within their relevant sections to avoid errors.
				</p><div class="formalpara"><p class="title"><strong>Defaults</strong></p><p>
						The following parameters are defined in the <code class="literal">[DEFAULT]</code> section of the <code class="literal">minion.conf</code> file:
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">cleanup</span></dt><dd>
								Cleanup temporary files. Set this parmaeter to <code class="literal">False</code> to leave the temporary files used during deployment in place after the command is run. This is useful for debugging the generated files or if errors occur.
							</dd><dt><span class="term">container_cli</span></dt><dd>
								The CLI tool for container management. Leave this parameter set to <code class="literal">podman</code>. Red Hat Enterprise Linux 8.2 only supports <code class="literal">podman</code>.
							</dd><dt><span class="term">container_healthcheck_disabled</span></dt><dd>
								Disables containerized service health checks. Red Hat recommends that you enable health checks and leave this option set to <code class="literal">false</code>.
							</dd><dt><span class="term">container_images_file</span></dt><dd><p class="simpara">
								Heat environment file with container image information. This file can contain the following entries:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Parameters for all required container images
									</li><li class="listitem">
										The <code class="literal">ContainerImagePrepare</code> parameter to drive the required image preparation. Usually the file that contains this parameter is named <code class="literal">containers-prepare-parameter.yaml</code>.
									</li></ul></div></dd><dt><span class="term">container_insecure_registries</span></dt><dd>
								A list of insecure registries for <code class="literal">podman</code> to use. Use this parameter if you want to pull images from another source, such as a private container registry. In most cases, <code class="literal">podman</code> has the certificates to pull container images from either the Red Hat Container Catalog or from your Satellite server if the minion is registered to Satellite.
							</dd><dt><span class="term">container_registry_mirror</span></dt><dd>
								An optional <code class="literal">registry-mirror</code> configured that <code class="literal">podman</code> uses.
							</dd><dt><span class="term">custom_env_files</span></dt><dd>
								Additional environment file that you want to add to the minion installation.
							</dd><dt><span class="term">deployment_user</span></dt><dd>
								The user who installs the minion. Leave this parameter unset to use the current default user <code class="literal">stack</code>.
							</dd><dt><span class="term">enable_heat_engine</span></dt><dd>
								Defines whether to install the heat engine on the minion. The default is <code class="literal">true</code>.
							</dd><dt><span class="term">enable_ironic_conductor</span></dt><dd>
								Defines whether to install the ironic conductor service on the minion. The default value is <code class="literal">false</code>. Set this value to <code class="literal">true</code> to enable the ironic conductor service.
							</dd><dt><span class="term">heat_container_image</span></dt><dd>
								URL for the heat container image that you want to use. Leave unset.
							</dd><dt><span class="term">heat_native</span></dt><dd>
								Use native heat templates. Leave as <code class="literal">true</code>.
							</dd><dt><span class="term">hieradata_override</span></dt><dd>
								Path to <code class="literal">hieradata</code> override file that configures Puppet hieradata on the director, providing custom configuration to services beyond the <code class="literal">minion.conf</code> parameters. If set, the minion installation copies this file to the <code class="literal">/etc/puppet/hieradata</code> directory and sets it as the first file in the hierarchy.
							</dd><dt><span class="term">minion_debug</span></dt><dd>
								Set this value to <code class="literal">true</code> to enable the <code class="literal">DEBUG</code> log level for minion services.
							</dd><dt><span class="term">minion_enable_selinux</span></dt><dd>
								Enable or disable SELinux during the deployment. It is highly recommended to leave this value set to <code class="literal">true</code> unless you are debugging an issue.
							</dd><dt><span class="term">minion_enable_validations</span></dt><dd>
								Enable validation services on the minion.
							</dd><dt><span class="term">minion_hostname</span></dt><dd>
								Defines the fully qualified host name for the minion. If set, the minion installation configures all system host name settings. If left unset, the minion uses the current host name, but you must configure all system host name settings appropriately.
							</dd><dt><span class="term">minion_local_interface</span></dt><dd><p class="simpara">
								The chosen interface for the Provisioning NIC on the undercloud. This is also the device that the minion uses for DHCP and PXE boot services. Change this value to your chosen device. To see which device is connected, use the <code class="literal">ip addr</code> command. For example, this is the result of an <code class="literal">ip addr</code> command:
							</p><pre class="screen">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:75:24:09 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.178/24 brd 192.168.122.255 scope global dynamic eth0
       valid_lft 3462sec preferred_lft 3462sec
    inet6 fe80::5054:ff:fe75:2409/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noop state DOWN
    link/ether 42:0b:c2:a5:c1:26 brd ff:ff:ff:ff:ff:ff</pre><p class="simpara">
								In this example, the External NIC uses <code class="literal">eth0</code> and the Provisioning NIC uses <code class="literal">eth1</code>, which is currently not configured. In this case, set the <code class="literal">local_interface</code> to <code class="literal">eth1</code>. The configuration script attaches this interface to a custom bridge defined with the <code class="literal">inspection_interface</code> parameter.
							</p></dd><dt><span class="term">minion_local_ip</span></dt><dd>
								The IP address defined for the Provisioning NIC on the undercloud. This is also the IP address that the minion uses for DHCP and PXE boot services. Leave this value as the default <code class="literal">192.168.24.1/24</code> unless you use a different subnet for the Provisioning network, for example, if the default IP address conflicts with an existing IP address or subnet in your environment.
							</dd><dt><span class="term">minion_local_mtu</span></dt><dd>
								The maximum transmission unit (MTU) that you want to use for the <code class="literal">local_interface</code>. Do not exceed 1500 for the minion.
							</dd><dt><span class="term">minion_log_file</span></dt><dd>
								The path to a log file where you want to store the minion install and upgrade logs. By default, the log file is <code class="literal">install-minion.log</code> in the home directory. For example, <code class="literal">/home/stack/install-minion.log</code>.
							</dd><dt><span class="term">minion_nameservers</span></dt><dd>
								A list of DNS nameservers to use for the minion hostname resolution.
							</dd><dt><span class="term">minion_ntp_servers</span></dt><dd>
								A list of network time protocol servers to help synchronize the minion date and time.
							</dd><dt><span class="term">minion_password_file</span></dt><dd>
								The file that contains the passwords for the minion to connect to undercloud services. Leave this parameter set to the <code class="literal">tripleo-undercloud-passwords.yaml</code> file copied from the undercloud.
							</dd><dt><span class="term">minion_service_certificate</span></dt><dd>
								The location and filename of the certificate for OpenStack SSL/TLS communication. Ideally, you obtain this certificate from a trusted certificate authority. Otherwise, generate your own self-signed certificate.
							</dd><dt><span class="term">minion_timezone</span></dt><dd>
								Host timezone for the minion. If you do not specify a timezone, the minion uses the existing timezone configuration.
							</dd><dt><span class="term">minion_undercloud_output_file</span></dt><dd>
								The file that contains undercloud configuration information that the minion can use to connect to undercloud services. Leave this parameter set to the <code class="literal">tripleo-undercloud-outputs.yaml</code> file copied from the undercloud.
							</dd><dt><span class="term">net_config_override</span></dt><dd>
								The path to a network configuration override template. If you set this parameter, the minion uses a JSON format template to configure the networking with <code class="literal">os-net-config</code> and ignores the network parameters set in <code class="literal">minion.conf</code>. See <code class="literal">/usr/share/python-tripleoclient/minion.conf.sample</code> for an example.
							</dd><dt><span class="term">networks_file</span></dt><dd>
								Networks file to override for <code class="literal">heat</code>.
							</dd><dt><span class="term">output_dir</span></dt><dd>
								Directory to output state, processed heat templates, and Ansible deployment files.
							</dd><dt><span class="term">roles_file</span></dt><dd>
								The roles file that you want to use to override the default roles file for minion installation. It is highly recommended to leave this parameter unset so that the minion installation uses the default roles file.
							</dd><dt><span class="term">templates</span></dt><dd>
								Heat templates file to override.
							</dd></dl></div></section><section class="section" id="installing-the-minion"><div class="titlepage"><div><div><h2 class="title">5.8. Installing the minion</h2></div></div></div><p>
					Complete the following steps to install the minion.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the minion host as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Run the following command to install the minion:
						</p><pre class="screen">[stack@minion ~]$ openstack undercloud minion install</pre><p class="simpara">
							This command launches the configuration script for the minion, installs additional packages, and configures minion services according to the configuration in the <code class="literal">minion.conf</code> file. This script takes several minutes to complete.
						</p></li></ol></div></section><section class="section" id="verifying-the-minion-installation"><div class="titlepage"><div><div><h2 class="title">5.9. Verifying the minion installation</h2></div></div></div><p>
					Complete the following steps to confirm a successful minion installation.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to your undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">[stack@director ~]$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							If you enabled the heat engine service on the minion, verify that the <code class="literal">heat-engine</code> service from the minion appears on the undercloud service list:
						</p><pre class="screen">[stack@director ~]$ $ openstack orchestration service list</pre><p class="simpara">
							The command output displays a table with <code class="literal">heat-engine</code> workers for both the undercloud and any minions.
						</p></li><li class="listitem"><p class="simpara">
							If you enabled the ironic conductor service on the minion, verify that the <code class="literal">ironic-conductor</code> service from the minion appears on the undercloud service list:
						</p><pre class="screen">[stack@director ~]$ $ openstack baremetal conductor list</pre><p class="simpara">
							The command output displays a table with <code class="literal">ironic-conductor</code> services for both the undercloud and any minions.
						</p></li></ol></div></section><section class="section" id="next_steps_2"><div class="titlepage"><div><div><h2 class="title">5.10. Next Steps</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Perform basic overcloud configuration, including registering nodes, inspecting nodes, and tagging nodes into various node roles. For more information, see <a class="xref" href="index.html#creating-a-basic-overcloud-with-cli-tools" title="Chapter 7. Configuring a basic overcloud with CLI tools">Chapter 7, <em>Configuring a basic overcloud with CLI tools</em></a>.
						</li></ul></div></section></section></div><div class="part" id="basic_overcloud_deployment"><div class="titlepage"><div><div><h1 class="title">Part II. Basic overcloud deployment</h1></div></div></div><section class="chapter" id="planning-your-Overcloud"><div class="titlepage"><div><div><h2 class="title">Chapter 6. Planning your overcloud</h2></div></div></div><p>
				The following section contains some guidelines for planning various aspects of your Red Hat OpenStack Platform (RHOSP) environment. This includes defining node roles, planning your network topology, and storage.
			</p><section class="section" id="node-roles"><div class="titlepage"><div><div><h2 class="title">6.1. Node roles</h2></div></div></div><p>
					Director includes the following default node types to build your overcloud:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Controller</span></dt><dd><p class="simpara">
								Provides key services for controlling your environment. This includes the dashboard (horizon), authentication (keystone), image storage (glance), networking (neutron), orchestration (heat), and high availability services. A Red Hat OpenStack Platform (RHOSP) environment requires three Controller nodes for a highly available production-level environment.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Use environments with one Controller node only for testing purposes, not for production. Environments with two Controller nodes or more than three Controller nodes are not supported.
								</p></div></div></dd><dt><span class="term">Compute</span></dt><dd>
								A physical server that acts as a hypervisor and contains the processing capabilities required to run virtual machines in the environment. A basic RHOSP environment requires at least one Compute node.
							</dd><dt><span class="term">Ceph Storage</span></dt><dd>
								A host that provides Red Hat Ceph Storage. Additional Ceph Storage hosts scale into a cluster. This deployment role is optional.
							</dd><dt><span class="term">Swift Storage</span></dt><dd>
								A host that provides external object storage to the OpenStack Object Storage (swift) service. This deployment role is optional.
							</dd></dl></div><p>
					The following table contains some examples of different overclouds and defines the node types for each scenario.
				</p><div class="table" id="idm140301178521312"><p class="title"><strong>Table 6.1. Node Deployment Roles for Scenarios</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 17%; " class="col_1"><!--Empty--></col><col style="width: 17%; " class="col_2"><!--Empty--></col><col style="width: 17%; " class="col_3"><!--Empty--></col><col style="width: 17%; " class="col_4"><!--Empty--></col><col style="width: 17%; " class="col_5"><!--Empty--></col><col style="width: 17%; " class="col_6"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top"> </td><td align="left" valign="top"> <p>
									Controller
								</p>
								 </td><td align="left" valign="top"> <p>
									Compute
								</p>
								 </td><td align="left" valign="top"> <p>
									Ceph Storage
								</p>
								 </td><td align="left" valign="top"> <p>
									Swift Storage
								</p>
								 </td><td align="left" valign="top"> <p>
									Total
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									Small overcloud
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									1
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									4
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									Medium overcloud
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									6
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									Medium overcloud with additional object storage
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									9
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									Medium overcloud with Ceph Storage cluster
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									3
								</p>
								 </td><td align="left" valign="top"> <p>
									-
								</p>
								 </td><td align="left" valign="top"> <p>
									9
								</p>
								 </td></tr></tbody></table></div></div><p>
					In addition, consider whether to split individual services into custom roles. For more information about the composable roles architecture, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/#Roles">"Composable Services and Custom Roles"</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
				</p></section><section class="section" id="overcloud-networks"><div class="titlepage"><div><div><h2 class="title">6.2. Overcloud networks</h2></div></div></div><p>
					It is important to plan the networking topology and subnets in your environment so that you can map roles and services to communicate with each other correctly. Red Hat OpenStack Platform (RHOSP) uses the Openstack Networking (neutron) service, which operates autonomously and manages software-based networks, static and floating IP addresses, and DHCP.
				</p><p>
					By default, director configures nodes to use the <span class="strong strong"><strong>Provisioning / Control Plane</strong></span> for connectivity. However, it is possible to isolate network traffic into a series of composable networks, that you can customize and assign services.
				</p><p>
					In a typical RHOSP installation, the number of network types often exceeds the number of physical network links. To connect all the networks to the proper hosts, the overcloud uses VLAN tagging to deliver more than one network on each interface. Most of the networks are isolated subnets but some networks require a Layer 3 gateway to provide routing for Internet access or infrastructure network connectivity. If you use VLANs to isolate your network traffic types, you must use a switch that supports 802.1Q standards to provide tagged VLANs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is recommended that you deploy a project network (tunneled with GRE or VXLAN) even if you intend to use a neutron VLAN mode with tunneling disabled at deployment time. This requires minor customization at deployment time and leaves the option available to use tunnel networks as utility networks or virtualization networks in the future. You still create Tenant networks using VLANs, but you can also create VXLAN tunnels for special-use networks without consuming tenant VLANs. It is possible to add VXLAN capability to a deployment with a Tenant VLAN, but it is not possible to add a Tenant VLAN to an existing overcloud without causing disruption.
					</p></div></div><p>
					Director also includes a set of templates that you can use to configure NICs with isolated composable networks. The following configurations are the default configurations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Single NIC configuration - One NIC for the Provisioning network on the native VLAN and tagged VLANs that use subnets for the different overcloud network types.
						</li><li class="listitem">
							Bonded NIC configuration - One NIC for the Provisioning network on the native VLAN and two NICs in a bond for tagged VLANs for the different overcloud network types.
						</li><li class="listitem">
							Multiple NIC configuration - Each NIC uses a subnet for a different overcloud network type.
						</li></ul></div><p>
					You can also create your own templates to map a specific NIC configuration.
				</p><p>
					The following details are also important when you consider your network configuration:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							During the overcloud creation, you refer to NICs using a single name across all overcloud machines. Ideally, you should use the same NIC on each overcloud node for each respective network to avoid confusion. For example, use the primary NIC for the Provisioning network and the secondary NIC for the OpenStack services.
						</li><li class="listitem">
							Set all overcloud systems to PXE boot off the Provisioning NIC, and disable PXE boot on the External NIC and any other NICs on the system. Also ensure that the Provisioning NIC has PXE boot at the top of the boot order, ahead of hard disks and CD/DVD drives.
						</li><li class="listitem">
							All overcloud bare metal systems require a supported power management interface, such as an Intelligent Platform Management Interface (IPMI), so that director can control the power management of each node.
						</li><li class="listitem">
							Make a note of the following details for each overcloud system: the MAC address of the Provisioning NIC, the IP address of the IPMI NIC, IPMI username, and IPMI password. This information is useful later when you configure the overcloud nodes.
						</li><li class="listitem">
							If an instance must be accessible from the external internet, you can allocate a floating IP address from a public network and associate the floating IP with an instance. The instance retains its private IP but network traffic uses NAT to traverse through to the floating IP address. Note that a floating IP address can be assigned only to a single instance rather than multiple private IP addresses. However, the floating IP address is reserved for use only by a single tenant, which means that the tenant can associate or disassociate the floating IP address with a particular instance as required. This configuration exposes your infrastructure to the external internet and you must follow suitable security practices.
						</li><li class="listitem">
							To mitigate the risk of network loops in Open vSwitch, only a single interface or a single bond can be a member of a given bridge. If you require multiple bonds or interfaces, you can configure multiple bridges.
						</li><li class="listitem">
							Red Hat recommends using DNS hostname resolution so that your overcloud nodes can connect to external services, such as the Red Hat Content Delivery Network and network time servers.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can virtualize the overcloud control plane if you are using Red Hat Virtualization (RHV). For more information, see <a class="link" href="index.html#creating-virtualized-control-planes" title="Chapter 25. Creating virtualized control planes">Creating virtualized control planes</a>.
					</p></div></div></section><section class="section" id="overcloud-storage"><div class="titlepage"><div><div><h2 class="title">6.3. Overcloud storage</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Using LVM on a guest instance that uses a back end cinder-volume of any driver or back-end type results in issues with performance, volume visibility and availability, and data corruption. Use an LVM filter to mitigate these issues. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/storage_guide/#ch-backends">section 2.1 Back Ends</a> in the <span class="emphasis"><em>Storage Guide</em></span> and KCS article 3213311, <a class="link" href="https://access.redhat.com/solutions/3213311">"Using LVM on a cinder volume exposes the data to the compute host."</a>
					</p></div></div><p>
					Director includes different storage options for the overcloud environment:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Ceph Storage nodes</span></dt><dd><p class="simpara">
								Director creates a set of scalable storage nodes using Red Hat Ceph Storage. The overcloud uses these nodes for the following storage types:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<span class="strong strong"><strong>Images</strong></span> - The Image service (glance) manages images for virtual machines. Images are immutable. OpenStack treats images as binary blobs and downloads them accordingly. You can use the Image service (glance) to store images in a Ceph Block Device.
									</li><li class="listitem">
										<span class="strong strong"><strong>Volumes</strong></span> - OpenStack manages volumes with the Block Storage service (cinder). The Block Storage service (cinder) volumes are block devices. OpenStack uses volumes to boot virtual machines, or to attach volumes to running virtual machines. You can use the Block Storage serivce to boot a virtual machine using a copy-on-write clone of an image.
									</li><li class="listitem">
										<span class="strong strong"><strong>File Systems</strong></span> - Openstack manages shared file systems with the Shared File Systems service (manila). Shares are backed by file systems. You can use manila to manage shares backed by a CephFS file system with data on the Ceph Storage nodes.
									</li><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong>Guest Disks</strong></span> - Guest disks are guest operating system disks. By default, when you boot a virtual machine with the Compute service (nova), the virtual machine disk appears as a file on the filesystem of the hypervisor (usually under <code class="literal">/var/lib/nova/instances/&lt;uuid&gt;/</code>). Every virtual machine inside Ceph can be booted without using the Block Storage service (cinder). As a result, you can perform maintenance operations easily with the live-migration process. Additionally, if your hypervisor fails, it is also convenient to trigger <code class="literal">nova evacuate</code> and run the virtual machine elsewhere.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											For information about supported image formats, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/ch-image-service">Image Service</a> chapter in the <span class="emphasis"><em>Instances and Images Guide</em></span>.
										</p></div></div><p class="simpara">
										For more information about Ceph Storage, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/architecture_guide/index">Red Hat Ceph Storage Architecture Guide</a>.
									</p></li></ul></div></dd><dt><span class="term">Swift Storage nodes</span></dt><dd>
								Director creates an external object storage node. This is useful in situations where you need to scale or replace Controller nodes in your overcloud environment but need to retain object storage outside of a high availability cluster.
							</dd></dl></div></section><section class="section" id="overcloud-security"><div class="titlepage"><div><div><h2 class="title">6.4. Overcloud security</h2></div></div></div><p>
					Your OpenStack Platform implementation is only as secure as your environment. Follow good security principles in your networking environment to ensure that you control network access properly:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use network segmentation to mitigate network movement and isolate sensitive data. A flat network is much less secure.
						</li><li class="listitem">
							Restrict services access and ports to a minimum.
						</li><li class="listitem">
							Enforce proper firewall rules and password usage.
						</li><li class="listitem">
							Ensure that SELinux is enabled.
						</li></ul></div><p>
					For more information about securing your system, see the following Red Hat guides:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/">Security Hardening</a> for Red Hat Enterprise Linux 8
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/using_selinux/">Using SELinux</a> for Red Hat Enterprise Linux 8
						</li></ul></div></section><section class="section" id="sect-planning-HA"><div class="titlepage"><div><div><h2 class="title">6.5. Overcloud high availability</h2></div></div></div><p>
					To deploy a highly-available overcloud, director configures multiple Controller, Compute and Storage nodes to work together as a single cluster. In case of node failure, an automated fencing and re-spawning process is triggered based on the type of node that failed. For more information about overcloud high availability architecture and services, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/high_availability_deployment_and_usage/">High Availability Deployment and Usage</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Deploying a highly available overcloud without STONITH is not supported. You must configure a STONITH device for each node that is a part of the Pacemaker cluster in a highly available overcloud. For more information on STONITH and Pacemaker, see <a class="link" href="https://access.redhat.com/solutions/15575">Fencing in a Red Hat High Availability Cluster</a> and <a class="link" href="https://access.redhat.com/articles/2881341">Support Policies for RHEL High Availability Clusters</a>.
					</p></div></div><p>
					You can also configure high availability for Compute instances with director (Instance HA). This high availability mechanism automates evacuation and re-spawning of instances on Compute nodes in case of node failure. The requirements for Instance HA are the same as the general overcloud requirements, but you must perform a few additional steps to prepare your environment for the deployment. For more information about Instance HA and installation instructions, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/high_availability_for_compute_instances/">High Availability for Compute Instances</a> guide.
				</p></section><section class="section" id="controller-node-requirements"><div class="titlepage"><div><div><h2 class="title">6.6. Controller node requirements</h2></div></div></div><p>
					Controller nodes host the core services in a Red Hat OpenStack Platform environment, such as the Dashboard (horizon), the back-end database server, the Identity service (keystone) authentication, and high availability services.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Processor</span></dt><dd>
								64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions.
							</dd><dt><span class="term">Memory</span></dt><dd><p class="simpara">
								The minimum amount of memory is 32 GB. However, the amount of recommended memory depends on the number of vCPUs, which is based on the number of CPU cores multiplied by hyper-threading value. Use the following calculations to determine your RAM requirements:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong>Controller RAM minimum calculation:</strong></span>
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												Use 1.5 GB of memory for each vCPU. For example, a machine with 48 vCPUs should have 72 GB of RAM.
											</li></ul></div></li><li class="listitem"><p class="simpara">
										<span class="strong strong"><strong>Controller RAM recommended calculation:</strong></span>
									</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
												Use 3 GB of memory for each vCPU. For example, a machine with 48 vCPUs should have 144 GB of RAM
											</li></ul></div></li></ul></div><p class="simpara">
								For more information about measuring memory requirements, see <a class="link" href="https://access.redhat.com/articles/2431181">"Red Hat OpenStack Platform Hardware Requirements for Highly Available Controllers"</a> on the Red Hat Customer Portal.
							</p></dd><dt><span class="term">Disk Storage and layout</span></dt><dd><p class="simpara">
								A minimum amount of 40 GB storage is required if the Object Storage service (swift) is not running on the Controller nodes. However, the Telemetry and Object Storage services are both installed on the Controllers, with both configured to use the root disk. These defaults are suitable for deploying small overclouds built on commodity hardware. These environments are typical of proof-of-concept and test environments. You can use these defaults to deploy overclouds with minimal planning, but they offer little in terms of workload capacity and performance.
							</p><p class="simpara">
								In an enterprise environment, however, the defaults could cause a significant bottleneck because Telemetry accesses storage constantly. This results in heavy disk I/O usage, which severely impacts the performance of all other Controller services. In this type of environment, you must plan your overcloud and configure it accordingly.
							</p><p class="simpara">
								Red Hat provides several configuration recommendations for both Telemetry and Object Storage. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/deployment_recommendations_for_specific_red_hat_openStack_platform_services">Deployment Recommendations for Specific Red Hat OpenStack Platform Services</a>.
							</p></dd><dt><span class="term">Network Interface Cards</span></dt><dd>
								A minimum of 2 x 1 Gbps Network Interface Cards. Use additional network interface cards for bonded interfaces or to delegate tagged VLAN traffic.
							</dd><dt><span class="term">Power management</span></dt><dd>
								Each Controller node requires a supported power management interface, such as an Intelligent Platform Management Interface (IPMI) functionality, on the server motherboard.
							</dd><dt><span class="term">Virtualization support</span></dt><dd>
								Red Hat supports virtualized Controller nodes only on Red Hat Virtualization platforms. For more information, see <a class="link" href="index.html#creating-virtualized-control-planes" title="Chapter 25. Creating virtualized control planes">Virtualized control planes</a>.
							</dd></dl></div></section><section class="section" id="compute-node-requirements"><div class="titlepage"><div><div><h2 class="title">6.7. Compute node requirements</h2></div></div></div><p>
					Compute nodes are responsible for running virtual machine instances after they are launched. Compute nodes must support hardware virtualization. Compute nodes must also have enough memory and disk space to support the requirements of the virtual machine instances that they host.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Processor</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions, and the AMD-V or Intel VT hardware virtualization extensions enabled. It is recommended that this processor has a minimum of 4 cores.
									</li><li class="listitem">
										IBM POWER 8 processor.
									</li></ul></div></dd><dt><span class="term">Memory</span></dt><dd>
								A minimum of 6 GB of RAM. Add additional RAM to this requirement based on the amount of memory that you intend to make available to virtual machine instances.
							</dd><dt><span class="term">Disk space</span></dt><dd>
								A minimum of 40 GB of available disk space.
							</dd><dt><span class="term">Network Interface Cards</span></dt><dd>
								A minimum of one 1 Gbps Network Interface Cards, although it is recommended to use at least two NICs in a production environment. Use additional network interface cards for bonded interfaces or to delegate tagged VLAN traffic.
							</dd><dt><span class="term">Power management</span></dt><dd>
								Each Compute node requires a supported power management interface, such as an Intelligent Platform Management Interface (IPMI) functionality, on the server motherboard.
							</dd></dl></div></section><section class="section" id="ceph-storage-node-requirements"><div class="titlepage"><div><div><h2 class="title">6.8. Ceph Storage node requirements</h2></div></div></div><p>
					Ceph Storage nodes are responsible for providing object storage in a Red Hat OpenStack Platform environment.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Placement Groups (PGs)</span></dt><dd>
								Ceph uses placement groups to facilitate dynamic and efficient object tracking at scale. In the case of OSD failure or cluster rebalancing, Ceph can move or replicate a placement group and its contents, which means a Ceph cluster can re-balance and recover efficiently. The default placement group count that director creates is not always optimal so it is important to calculate the correct placement group count according to your requirements. You can use the placement group calculator to calculate the correct count: <a class="link" href="https://access.redhat.com/labs/cephpgc/">Placement Groups (PGs) per Pool Calculator</a>
							</dd><dt><span class="term">Processor</span></dt><dd>
								64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions.
							</dd><dt><span class="term">Memory</span></dt><dd>
								Red Hat typically recommends a baseline of 16 GB of RAM per OSD host, with an additional 2 GB of RAM per OSD daemon.
							</dd><dt><span class="term">Disk layout</span></dt><dd><p class="simpara">
								Sizing is dependent on your storage requirements. Red Hat recommends that your Ceph Storage node configuration includes three or more disks in a layout similar to the following example:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">/dev/sda</code> - The root disk. The director copies the main overcloud image to the disk. Ensure that the disk has a minimum of 40 GB of available disk space.
									</li><li class="listitem">
										<code class="literal">/dev/sdb</code> - The journal disk. This disk divides into partitions for Ceph OSD journals. For example, <code class="literal">/dev/sdb1</code>, <code class="literal">/dev/sdb2</code>, and <code class="literal">/dev/sdb3</code>. The journal disk is usually a solid state drive (SSD) to aid with system performance.
									</li><li class="listitem"><p class="simpara">
										<code class="literal">/dev/sdc</code> and onward - The OSD disks. Use as many disks as necessary for your storage requirements.
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Red Hat OpenStack Platform director uses <code class="literal">ceph-ansible</code>, which does not support installing the OSD on the root disk of Ceph Storage nodes. This means that you need at least two disks for a supported Ceph Storage node.
										</p></div></div></li></ul></div></dd><dt><span class="term">Network Interface Cards</span></dt><dd>
								A minimum of one 1 Gbps Network Interface Cards, although Red Hat recommends that you use at least two NICs in a production environment. Use additional network interface cards for bonded interfaces or to delegate tagged VLAN traffic. Red Hat recommends that you use a 10 Gbps interface for storage nodes, especially if you want to create an OpenStack Platform environment that serves a high volume of traffic.
							</dd><dt><span class="term">Power management</span></dt><dd>
								Each Controller node requires a supported power management interface, such as Intelligent Platform Management Interface (IPMI) functionality on the motherboard of the server.
							</dd></dl></div><p>
					For more information about installing an overcloud with a Ceph Storage cluster, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/deploying_an_overcloud_with_containerized_red_hat_ceph/">Deploying an Overcloud with Containerized Red Hat Ceph</a> guide.
				</p></section><section class="section" id="object-storage-node-requirements"><div class="titlepage"><div><div><h2 class="title">6.9. Object Storage node requirements</h2></div></div></div><p>
					Object Storage nodes provide an object storage layer for the overcloud. The Object Storage proxy is installed on Controller nodes. The storage layer requires bare metal nodes with multiple disks on each node.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Processor</span></dt><dd>
								64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions.
							</dd><dt><span class="term">Memory</span></dt><dd>
								Memory requirements depend on the amount of storage space. Use at minimum 1 GB of memory for each 1 TB of hard disk space. For optimal performance, it is recommended to use 2 GB for each 1 TB of hard disk space, especially for workloads with files smaller than 100GB.
							</dd><dt><span class="term">Disk space</span></dt><dd><p class="simpara">
								Storage requirements depend on the capacity needed for the workload. It is recommended to use SSD drives to store the account and container data. The capacity ratio of account and container data to objects is approximately 1 per cent. For example, for every 100TB of hard drive capacity, provide 1TB of SSD capacity for account and container data.
							</p><p class="simpara">
								However, this depends on the type of stored data. If you want to store mostly small objects, provide more SSD space. For large objects (videos, backups), use less SSD space.
							</p></dd><dt><span class="term">Disk layout</span></dt><dd><p class="simpara">
								The recommended node configuration requires a disk layout similar to the following example:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">/dev/sda</code> - The root disk. Director copies the main overcloud image to the disk.
									</li><li class="listitem">
										<code class="literal">/dev/sdb</code> - Used for account data.
									</li><li class="listitem">
										<code class="literal">/dev/sdc</code> - Used for container data.
									</li><li class="listitem">
										<code class="literal">/dev/sdd</code> and onward - The object server disks. Use as many disks as necessary for your storage requirements.
									</li></ul></div></dd><dt><span class="term">Network Interface Cards</span></dt><dd>
								A minimum of 2 x 1 Gbps Network Interface Cards. Use additional network interface cards for bonded interfaces or to delegate tagged VLAN traffic.
							</dd><dt><span class="term">Power management</span></dt><dd>
								Each Controller node requires a supported power management interface, such as an Intelligent Platform Management Interface (IPMI) functionality, on the server motherboard.
							</dd></dl></div></section><section class="section" id="overcloud-repositories"><div class="titlepage"><div><div><h2 class="title">6.10. Overcloud repositories</h2></div></div></div><p>
					Red Hat OpenStack Platform 16.1 runs on Red Hat Enterprise Linux 8.2. As a result, you must lock the content from these repositories to the respective Red Hat Enterprise Linux version.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you synchronize repositories with Red Hat Satellite, you can enable specific versions of the Red Hat Enterprise Linux repositories. However, the repository remains the same despite the version you choose. For example, you can enable the 8.2 version of the BaseOS repository, but the repository name is still <code class="literal">rhel-8-for-x86_64-baseos-eus-rpms</code> despite the specific version you choose.
					</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Any repositories outside the ones specified here are not supported. Unless recommended, do not enable any other products or repositories outside the ones listed in the following tables or else you might encounter package dependency issues. Do not enable Extra Packages for Enterprise Linux (EPEL).
					</p></div></div><div class="formalpara"><p class="title"><strong>Core repositories</strong></p><p>
						The following table lists core repositories for installing the overcloud.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301178993216" scope="col">Name</th><th align="left" valign="top" id="idm140301178992128" scope="col">Repository</th><th align="left" valign="top" id="idm140301178991040" scope="col">Description of requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs) Extended Update Support (EUS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">rhel-8-for-x86_64-baseos-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Base operating system repository for x86_64 systems.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">rhel-8-for-x86_64-appstream-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Contains Red Hat OpenStack Platform dependencies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - High Availability (RPMs) Extended Update Support (EUS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">rhel-8-for-x86_64-highavailability-eus-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									High availability tools for Red Hat Enterprise Linux.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Ansible Engine 2.9 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">ansible-2.9-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Ansible Engine for Red Hat Enterprise Linux. Used to provide the latest version of Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Advanced Virtualization for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">advanced-virt-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Provides virtualization packages for OpenStack Platform.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Satellite Tools for RHEL 8 Server RPMs x86_64
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">satellite-tools-6.5-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Tools for managing hosts with Red Hat Satellite 6.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat OpenStack Platform 16.1 for RHEL 8 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">openstack-16.1-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Core Red Hat OpenStack Platform repository.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178993216"> <p>
									Red Hat Fast Datapath for RHEL 8 (RPMS)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178992128"> <p>
									<code class="literal">fast-datapath-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178991040"> <p>
									Provides Open vSwitch (OVS) packages for OpenStack Platform.
								</p>
								 </td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Ceph repositories</strong></p><p>
						The following table lists Ceph Storage related repositories for the overcloud.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301182718672" scope="col">Name</th><th align="left" valign="top" id="idm140301182717696" scope="col">Repository</th><th align="left" valign="top" id="idm140301182716608" scope="col">Description of Requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhel-8-for-x86_64-baseos-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									Base operating system repository for x86_64 systems.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhel-8-for-x86_64-appstream-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									Contains Red Hat OpenStack Platform dependencies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - High Availability (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhel-8-for-x86_64-highavailability-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									High availability tools for Red Hat Enterprise Linux.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Ansible Engine 2.9 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">ansible-2.9-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									Ansible Engine for Red Hat Enterprise Linux. Used to provide the latest version of Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat OpenStack Platform 16.1 Director Deployment Tools for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">openstack-16.1-deployment-tools-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									Packages to help director configure Ceph Storage nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Ceph Storage OSD 4 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhceph-4-osd-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									(For Ceph Storage Nodes) Repository for Ceph Storage Object Storage daemon. Installed on Ceph Storage nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Ceph Storage MON 4 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhceph-4-mon-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									(For Ceph Storage Nodes) Repository for Ceph Storage Monitor daemon. Installed on Controller nodes in OpenStack environments using Ceph Storage nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301182718672"> <p>
									Red Hat Ceph Storage Tools 4 for RHEL 8 x86_64 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301182717696"> <p>
									<code class="literal">rhceph-4-tools-for-rhel-8-x86_64-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301182716608"> <p>
									Provides tools for nodes to communicate with the Ceph Storage cluster. This repository should be enabled for all nodes when deploying an overcloud with a Ceph Storage cluster.
								</p>
								 </td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>Real Time repositories</strong></p><p>
						The following table lists repositories for Real Time Compute (RTC) functionality.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301176535696" scope="col">Name</th><th align="left" valign="top" id="idm140301173804752" scope="col">Repository</th><th align="left" valign="top" id="idm140301173803664" scope="col">Description of requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301176535696"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - Real Time (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301173804752"> <p>
									<code class="literal">rhel-8-for-x86_64-rt-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173803664"> <p>
									Repository for Real Time KVM (RT-KVM). Contains packages to enable the real time kernel. Enable this repository for all Compute nodes targeted for RT-KVM. NOTE: You need a separate subscription to a <code class="literal">Red Hat OpenStack Platform for Real Time</code> SKU to access this repository.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301176535696"> <p>
									Red Hat Enterprise Linux 8 for x86_64 - Real Time for NFV (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301173804752"> <p>
									<code class="literal">rhel-8-for-x86_64-nfv-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301173803664"> <p>
									Repository for Real Time KVM (RT-KVM) for NFV. Contains packages to enable the real time kernel. Enable this repository for all NFV Compute nodes targeted for RT-KVM. NOTE: You need a separate subscription to a <code class="literal">Red Hat OpenStack Platform for Real Time</code> SKU to access this repository.
								</p>
								 </td></tr></tbody></table></div><div class="formalpara"><p class="title"><strong>IBM POWER repositories</strong></p><p>
						The following table lists repositories for Openstack Platform on POWER PC architecture. Use these repositories in place of equivalents in the Core repositories.
					</p></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301178233008" scope="col">Name</th><th align="left" valign="top" id="idm140301178231920" scope="col">Repository</th><th align="left" valign="top" id="idm140301178230832" scope="col">Description of requirement</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301178233008"> <p>
									Red Hat Enterprise Linux for IBM Power, little endian - BaseOS (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178231920"> <p>
									<code class="literal">rhel-8-for-ppc64le-baseos-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178230832"> <p>
									Base operating system repository for ppc64le systems.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178233008"> <p>
									Red Hat Enterprise Linux 8 for IBM Power, little endian - AppStream (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178231920"> <p>
									<code class="literal">rhel-8-for-ppc64le-appstream-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178230832"> <p>
									Contains Red Hat OpenStack Platform dependencies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178233008"> <p>
									Red Hat Enterprise Linux 8 for IBM Power, little endian - High Availability (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178231920"> <p>
									<code class="literal">rhel-8-for-ppc64le-highavailability-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178230832"> <p>
									High availability tools for Red Hat Enterprise Linux. Used for Controller node high availability.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178233008"> <p>
									Red Hat Ansible Engine 2.8 for RHEL 8 IBM Power, little endian (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178231920"> <p>
									<code class="literal">ansible-2.8-for-rhel-8-ppc64le-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178230832"> <p>
									Ansible Engine for Red Hat Enterprise Linux. Used to provide the latest version of Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301178233008"> <p>
									Red Hat OpenStack Platform 16.1 for RHEL 8 (RPMs)
								</p>
								 </td><td align="left" valign="top" headers="idm140301178231920"> <p>
									<code class="literal">openstack-16.1-for-rhel-8-ppc64le-rpms</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301178230832"> <p>
									Core Red Hat OpenStack Platform repository for ppc64le systems.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="Provisioning-methods"><div class="titlepage"><div><div><h2 class="title">6.11. Provisioning methods</h2></div></div></div><p>
					There are three main methods that you can use to provision the nodes for your Red Hat OpenStack Platform environment:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Provisioning with director</span></dt><dd>
								Red Hat OpenStack Platform director is the standard provisioning method. In this scenario, the <code class="literal">openstack overcloud deploy</code> command performs both the provisioning and the configuration of your deployment. For more information about the standard provisioning and deployment method, see <a class="xref" href="index.html#creating-a-basic-overcloud-with-cli-tools" title="Chapter 7. Configuring a basic overcloud with CLI tools">Chapter 7, <em>Configuring a basic overcloud with CLI tools</em></a>.
							</dd><dt><span class="term">Provisioning with the OpenStack Bare Metal (ironic) service</span></dt><dd><p class="simpara">
								In this scenario, you can separate the provisioning and configuration stages of the standard director deployment into two distinct processes. This is useful if you want to mitigate some of the risk involved with the standard director deployment and identify points of failure more efficiently. For more information about this scenario, see <a class="xref" href="index.html#provisioning-bare-metal-nodes-before-deploying-the-overcloud" title="Chapter 8. Provisioning bare metal nodes before deploying the overcloud">Chapter 8, <em>Provisioning bare metal nodes before deploying the overcloud</em></a>.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
								</p></div></div></dd><dt><span class="term">Provisioning with an external tool</span></dt><dd><p class="simpara">
								In this scenario, director controls the overcloud configuration on nodes that you pre-provision with an external tool. This is useful if you want to create an overcloud without power management control, use networks that have DHCP/PXE boot restrictions, or if you want to use nodes that have a custom partitioning layout that does not rely on the QCOW2 <code class="literal">overcloud-full</code> image. This scenario does not use the OpenStack Compute (nova), OpenStack Bare Metal (ironic), or OpenStack Image (glance) services for managing nodes.
							</p><p class="simpara">
								For more information about this scenario, see <a class="xref" href="index.html#configuring-a-basic-overcloud-with-pre-provisioned-nodes" title="Chapter 9. Configuring a basic overcloud with pre-provisioned nodes">Chapter 9, <em>Configuring a basic overcloud with pre-provisioned nodes</em></a>.
							</p></dd></dl></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						You cannot combine pre-provisioned nodes with director-provisioned nodes.
					</p></div></div></section></section><section class="chapter" id="creating-a-basic-overcloud-with-cli-tools"><div class="titlepage"><div><div><h2 class="title">Chapter 7. Configuring a basic overcloud with CLI tools</h2></div></div></div><p>
				This chapter contains basic configuration procedures to deploy an OpenStack Platform environment using the CLI tools. An overcloud with a basic configuration contains no custom features. However, you can add advanced configuration options to this basic overcloud and customize it to your specifications using the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/">Advanced Overcloud Customization</a> guide.
			</p><section class="section" id="sect-Registering_Nodes_for_the_Overcloud-basic"><div class="titlepage"><div><div><h2 class="title">7.1. Registering nodes for the overcloud</h2></div></div></div><p>
					Director requires a node definition template, which you create manually. This template uses a JSON or YAML format, and contains the hardware and power management details for your nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a template that lists your nodes. Use the following JSON and YAML template examples to understand how to structure your node definition template:
						</p><div class="formalpara"><p class="title"><strong>Example JSON template</strong></p><p>
								
<pre class="screen">{
    "nodes":[
        {
            "mac":[
                "bb:bb:bb:bb:bb:bb"
            ],
            "name":"node01",
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.168.24.205"
        },
        {
            "mac":[
                "cc:cc:cc:cc:cc:cc"
            ],
            "name":"node02",
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.168.24.206"
        }
    ]
}</pre>

							</p></div><div class="formalpara"><p class="title"><strong>Example YAML template</strong></p><p>
								
<pre class="screen">nodes:
  - mac:
      - "bb:bb:bb:bb:bb:bb"
    name: "node01"
    cpu: 4
    memory: 6144
    disk: 40
    arch: "x86_64"
    pm_type: "ipmi"
    pm_user: "admin"
    pm_password: "p@55w0rd!"
    pm_addr: "192.168.24.205"
  - mac:
      - cc:cc:cc:cc:cc:cc
    name: "node02"
    cpu: 4
    memory: 6144
    disk: 40
    arch: "x86_64"
    pm_type: "ipmi"
    pm_user: "admin"
    pm_password: "p@55w0rd!"
    pm_addr: "192.168.24.206"</pre>

							</p></div><p class="simpara">
							This template contains the following attributes:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">name</span></dt><dd>
										The logical name for the node.
									</dd><dt><span class="term">pm_type</span></dt><dd><p class="simpara">
										The power management driver that you want to use. This example uses the IPMI driver (<code class="literal">ipmi</code>).
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											IPMI is the preferred supported power management driver. For more information about supported power management types and their options, see <a class="xref" href="index.html#appe-Power_Management_Drivers" title="Appendix A. Power management drivers">Appendix A, <em>Power management drivers</em></a>. If these power management drivers do not work as expected, use IPMI for your power management.
										</p></div></div></dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
										The IPMI username and password.
									</dd><dt><span class="term">pm_addr</span></dt><dd>
										The IP address of the IPMI device.
									</dd><dt><span class="term">pm_port (Optional)</span></dt><dd>
										The port to access the specific IPMI device.
									</dd><dt><span class="term">mac</span></dt><dd>
										(Optional) A list of MAC addresses for the network interfaces on the node. Use only the MAC address for the Provisioning NIC of each system.
									</dd><dt><span class="term">cpu</span></dt><dd>
										(Optional) The number of CPUs on the node.
									</dd><dt><span class="term">memory</span></dt><dd>
										(Optional) The amount of memory in MB.
									</dd><dt><span class="term">disk</span></dt><dd>
										(Optional) The size of the hard disk in GB.
									</dd><dt><span class="term">arch</span></dt><dd><p class="simpara">
										(Optional) The system architecture.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											When building a multi-architecture cloud, the <code class="literal">arch</code> key is mandatory to distinguish nodes using <code class="literal">x86_64</code> and <code class="literal">ppc64le</code> architectures.
										</p></div></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							After you create the template, run the following commands to verify the formatting and syntax:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud node import --validate-only ~/nodes.json</pre></li><li class="listitem"><p class="simpara">
							Save the file to the home directory of the <code class="literal">stack</code> user (<code class="literal">/home/stack/nodes.json</code>), then run the following commands to import the template to director:
						</p><pre class="screen">(undercloud) $ openstack overcloud node import ~/nodes.json</pre><p class="simpara">
							This command registers each node from the template into director.
						</p></li><li class="listitem"><p class="simpara">
							Wait for the node registration and configuration to complete. When complete, confirm that director has successfully registered the nodes:
						</p><pre class="screen">(undercloud) $ openstack baremetal node list</pre></li></ol></div></section><section class="section" id="validating-the-introspection-requirements"><div class="titlepage"><div><div><h2 class="title">7.2. Validating the introspection requirements</h2></div></div></div><p>
					Run the <span class="literal literal">pre-introspection</span> validation group to check the introspection requirements.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file.
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack tripleo validator run</code> command with the <span class="literal literal">--group pre-introspection</span> option:
						</p><pre class="screen">$ openstack tripleo validator run --group pre-introspection</pre></li><li class="listitem"><p class="simpara">
							Review the results of the validation report. To view detailed output from a specific validation, run the <code class="literal">openstack tripleo validator show run</code> command against the UUID of the specific validation from the report:
						</p><pre class="screen">$ openstack tripleo validator show run &lt;UUID&gt;</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						A <code class="literal">FAILED</code> validation does not prevent you from deploying or running Red Hat OpenStack Platform. However, a <code class="literal">FAILED</code> validation can indicate a potential issue with a production environment.
					</p></div></div></section><section class="section" id="inspecting-the-hardware-of-nodes-basic"><div class="titlepage"><div><div><h2 class="title">7.3. Inspecting the hardware of nodes</h2></div></div></div><p>
					Director can run an introspection process on each node. This process boots an introspection agent over PXE on each node. The introspection agent collects hardware data from the node and sends the data back to director. Director then stores this introspection data in the OpenStack Object Storage (swift) service running on director. Director uses hardware information for various purposes such as profile tagging, benchmarking, and manual root disk assignment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to inspect the hardware attributes of each node:
						</p><pre class="screen">(undercloud) $ openstack overcloud node introspect --all-manageable --provide</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Use the <code class="literal">--all-manageable</code> option to introspect only the nodes that are in a managed state. In this example, all nodes are in a managed state.
								</li><li class="listitem">
									Use the <code class="literal">--provide</code> option to reset all nodes to an <code class="literal">available</code> state after introspection.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Monitor the introspection progress logs in a separate terminal window:
						</p><pre class="screen">(undercloud) $ sudo tail -f /var/log/containers/ironic-inspector/ironic-inspector.log</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Ensure that this process runs to completion. This process usually takes 15 minutes for bare metal nodes.
							</p></div></div></li></ol></div><p>
					After the introspection completes, all nodes change to an <code class="literal">available</code> state.
				</p></section><section class="section" id="tagging-nodes-into-profiles"><div class="titlepage"><div><div><h2 class="title">7.4. Tagging nodes into profiles</h2></div></div></div><p>
					After you register and inspect the hardware of each node, tag the nodes into specific profiles. These profile tags match your nodes to flavors, which assigns the flavors to deployment roles. The following example shows the relationships across roles, flavors, profiles, and nodes for Controller nodes:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301173556144" scope="col">Type</th><th align="left" valign="top" id="idm140301175901552" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301173556144"> <p>
									Role
								</p>
								 </td><td align="left" valign="top" headers="idm140301175901552"> <p>
									The <code class="literal">Controller</code> role defines how director configures Controller nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301173556144"> <p>
									Flavor
								</p>
								 </td><td align="left" valign="top" headers="idm140301175901552"> <p>
									The <code class="literal">control</code> flavor defines the hardware profile for nodes to use as controllers. You assign this flavor to the <code class="literal">Controller</code> role so that director can decide which nodes to use.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301173556144"> <p>
									Profile
								</p>
								 </td><td align="left" valign="top" headers="idm140301175901552"> <p>
									The <code class="literal">control</code> profile is a tag you apply to the <code class="literal">control</code> flavor. This defines the nodes that belong to the flavor.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301173556144"> <p>
									Node
								</p>
								 </td><td align="left" valign="top" headers="idm140301175901552"> <p>
									You also apply the <code class="literal">control</code> profile tag to individual nodes, which groups them to the <code class="literal">control</code> flavor and, as a result, director configures them using the <code class="literal">Controller</code> role.
								</p>
								 </td></tr></tbody></table></div><p>
					Default profile flavors <code class="literal">compute</code>, <code class="literal">control</code>, <code class="literal">swift-storage</code>, <code class="literal">ceph-storage</code>, and <code class="literal">block-storage</code> are created during undercloud installation and are usable without modification in most environments.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To tag a node into a specific profile, add a <code class="literal">profile</code> option to the <code class="literal">properties/capabilities</code> parameter for each node. For example, to tag your nodes to use Controller and Compute profiles respectively, use the following commands:
						</p><pre class="screen">(undercloud) $ openstack baremetal node set --property capabilities='profile:control,boot_option:local' 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0
(undercloud) $ openstack baremetal node set --property capabilities='profile:compute,boot_option:local' 58c3d07e-24f2-48a7-bbb6-6843f0e8ee13</pre><p class="simpara">
							The addition of the <code class="literal">profile:control</code> and <code class="literal">profile:compute</code> options tag the two nodes into each respective profiles.
						</p><p class="simpara">
							These commands also set the <code class="literal">boot_option:local</code> parameter, which defines how each node boots.
						</p></li><li class="listitem"><p class="simpara">
							After you complete node tagging, check the assigned profiles or possible profiles:
						</p><pre class="screen">(undercloud) $ openstack overcloud profiles list</pre></li></ol></div></section><section class="section" id="setting-uefi-boot-mode"><div class="titlepage"><div><div><h2 class="title">7.5. Setting UEFI boot mode</h2></div></div></div><p>
					The default boot mode is the legacy BIOS mode. Newer systems might require UEFI boot mode instead of the legacy BIOS mode. Complete the following steps to change the boot mode to UEFI mode.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Set the following parameters in your <code class="literal">undercloud.conf</code> file:
						</p><pre class="screen">ipxe_enabled = True
inspection_enable_uefi = True</pre></li><li class="listitem"><p class="simpara">
							Save the <code class="literal">undercloud.conf</code> file and run the undercloud installation:
						</p><pre class="screen">$ openstack undercloud install</pre><p class="simpara">
							Wait until the installation script completes.
						</p></li><li class="listitem"><p class="simpara">
							Set the boot mode to <code class="literal">uefi</code> for each registered node. For example, to add or replace the existing <code class="literal">boot_mode</code> parameters in the <code class="literal">capabilities</code> property, run the following command:
						</p><pre class="screen">$ NODE=&lt;NODE NAME OR ID&gt; ; openstack baremetal node set --property capabilities="boot_mode:uefi,$(openstack baremetal node show $NODE -f json -c properties | jq -r .properties.capabilities | sed "s/boot_mode:[^,]*,//g")" $NODE</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Check that you have retained the <code class="literal">profile</code> and <code class="literal">boot_option</code> capabilities:
							</p><pre class="screen">$ openstack baremetal node show r530-12 -f json -c properties | jq -r .properties.capabilities</pre></div></div></li><li class="listitem"><p class="simpara">
							Set the boot mode to <code class="literal">uefi</code> for each flavor:
						</p><pre class="screen">$ openstack flavor set --property capabilities:boot_mode='uefi' control</pre></li></ol></div></section><section class="section" id="enabling-virtual-media-boot_basic"><div class="titlepage"><div><div><h2 class="title">7.6. Enabling virtual media boot</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					You can use Redfish virtual media boot to supply a boot image to the Baseboard Management Controller (BMC) of a node so that the BMC can insert the image into one of the virtual drives. The node can then boot from the virtual drive into the operating system that exists in the image.
				</p><p>
					Redfish hardware types support booting deploy, rescue, and user images over virtual media. The Bare Metal service (ironic) uses kernel and ramdisk images associated with a node to build bootable ISO images for UEFI or BIOS boot modes at the moment of node deployment. The major advantage of virtual media boot is that you can eliminate the TFTP image transfer phase of PXE and use HTTP GET, or other methods, instead.
				</p><p>
					To boot a node with the <code class="literal">redfish</code> hardware type over virtual media, set the boot interface to <code class="literal">redfish-virtual-media</code> and, for UEFI nodes, define the EFI System Partition (ESP) image. Then configure an enrolled node to use Redfish virtual media boot.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Redfish driver enabled in the <code class="literal">enabled_hardware_types</code> parameter in the <code class="literal">undercloud.conf</code> file.
						</li><li class="listitem">
							A bare metal node registered and enrolled.
						</li><li class="listitem">
							IPA and instance images in the Image Service (glance).
						</li><li class="listitem">
							For UEFI nodes, you must also have an EFI system partition image (ESP) available in the Image Service (glance).
						</li><li class="listitem">
							A bare metal flavor.
						</li><li class="listitem">
							A network for cleaning and provisioning.
						</li><li class="listitem"><p class="simpara">
							Sushy library installed:
						</p><pre class="screen">$ sudo yum install sushy</pre></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Set the Bare Metal service (ironic) boot interface to <code class="literal">redfish-virtual-media</code>:
						</p><pre class="screen">$ openstack baremetal node set --boot-interface redfish-virtual-media $NODE_NAME</pre><p class="simpara">
							Replace <code class="literal">$NODE_NAME</code> with the name of the node.
						</p></li><li class="listitem"><p class="simpara">
							For UEFI nodes, set the boot mode to <code class="literal">uefi</code>:
						</p><pre class="screen">NODE=&lt;NODE NAME OR ID&gt; ; openstack baremetal node set --property capabilities="boot_mode:uefi,$(openstack baremetal node show $NODE -f json -c properties | jq -r .properties.capabilities | sed "s/boot_mode:[^,]*,//g")" $NODE</pre><p class="simpara">
							Replace <code class="literal">$NODE</code> with the name of the node.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For BIOS nodes, do not complete this step.
							</p></div></div></li><li class="listitem"><p class="simpara">
							For UEFI nodes, define the EFI System Partition (ESP) image:
						</p><pre class="screen">$ openstack baremetal node set --driver-info bootloader=$ESP $NODE_NAME</pre><p class="simpara">
							Replace <code class="literal">$ESP</code> with the glance image UUID or URL for the ESP image, and replace <code class="literal">$NODE_NAME</code> with the name of the node.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For BIOS nodes, do not complete this step.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create a port on the bare metal node and associate the port with the MAC address of the NIC on the bare metal node:
						</p><pre class="screen">$ openstack baremetal port create --pxe-enabled True --node $UUID $MAC_ADDRESS</pre><p class="simpara">
							Replace <code class="literal">$UUID</code> with the UUID of the bare metal node, and replace <code class="literal">$MAC_ADDRESS</code> with the MAC address of the NIC on the bare metal node.
						</p></li></ol></div></section><section class="section" id="defining-the-root-disk"><div class="titlepage"><div><div><h2 class="title">7.7. Defining the root disk for multi-disk clusters</h2></div></div></div><p>
					Director must identify the root disk during provisioning in the case of nodes with multiple disks. For example, most Ceph Storage nodes use multiple disks. By default, director writes the overcloud image to the root disk during the provisioning process
				</p><p>
					There are several properties that you can define to help director identify the root disk:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">model</code> (String): Device identifier.
						</li><li class="listitem">
							<code class="literal">vendor</code> (String): Device vendor.
						</li><li class="listitem">
							<code class="literal">serial</code> (String): Disk serial number.
						</li><li class="listitem">
							<code class="literal">hctl</code> (String): Host:Channel:Target:Lun for SCSI.
						</li><li class="listitem">
							<code class="literal">size</code> (Integer): Size of the device in GB.
						</li><li class="listitem">
							<code class="literal">wwn</code> (String): Unique storage identifier.
						</li><li class="listitem">
							<code class="literal">wwn_with_extension</code> (String): Unique storage identifier with the vendor extension appended.
						</li><li class="listitem">
							<code class="literal">wwn_vendor_extension</code> (String): Unique vendor storage identifier.
						</li><li class="listitem">
							<code class="literal">rotational</code> (Boolean): True for a rotational device (HDD), otherwise false (SSD).
						</li><li class="listitem">
							<code class="literal">name</code> (String): The name of the device, for example: /dev/sdb1.
						</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Use the <code class="literal">name</code> property only for devices with persistent names. Do not use <code class="literal">name</code> to set the root disk for any other devices because this value can change when the node boots.
					</p></div></div><p>
					Complete the following steps to specify the root device using its serial number.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the disk information from the hardware introspection of each node. Run the following command to display the disk information of a node:
						</p><pre class="screen">(undercloud) $ openstack baremetal introspection data save 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0 | jq ".inventory.disks"</pre><p class="simpara">
							For example, the data for one node might show three disks:
						</p><pre class="screen">[
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sda",
    "wwn_vendor_extension": "0x1ea4dcc412a9632b",
    "wwn_with_extension": "0x61866da04f3807001ea4dcc412a9632b",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f380700",
    "serial": "61866da04f3807001ea4dcc412a9632b"
  }
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sdb",
    "wwn_vendor_extension": "0x1ea4e13c12e36ad6",
    "wwn_with_extension": "0x61866da04f380d001ea4e13c12e36ad6",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f380d00",
    "serial": "61866da04f380d001ea4e13c12e36ad6"
  }
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sdc",
    "wwn_vendor_extension": "0x1ea4e31e121cfb45",
    "wwn_with_extension": "0x61866da04f37fc001ea4e31e121cfb45",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f37fc00",
    "serial": "61866da04f37fc001ea4e31e121cfb45"
  }
]</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack baremetal node set --property root_device=</code> command to set the root disk for a node. Include the most appropriate hardware attribute value to define the root disk.
						</p><pre class="screen">(undercloud) $ openstack baremetal node set --property root_device=’{“serial”:”&lt;serial_number&gt;”}' &lt;node-uuid&gt;</pre><p class="simpara">
							For example, to set the root device to disk 2, which has the serial number <code class="literal">61866da04f380d001ea4e13c12e36ad6</code> run the following command:
						</p></li></ol></div><pre class="screen">(undercloud) $ openstack baremetal node set --property root_device='{"serial": "61866da04f380d001ea4e13c12e36ad6"}' 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0</pre><p>
					+
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Ensure that you configure the BIOS of each node to include booting from the root disk that you choose. Configure the boot order to boot from the network first, then to boot from the root disk.
					</p></div></div><p>
					Director identifies the specific disk to use as the root disk. When you run the <code class="literal">openstack overcloud deploy</code> command, director provisions and writes the overcloud image to the root disk.
				</p></section><section class="section" id="using-the-overcloud-minimal-image-to-avoid-using-a-Red-Hat-subscription-entitlement"><div class="titlepage"><div><div><h2 class="title">7.8. Using the overcloud-minimal image to avoid using a Red Hat subscription entitlement</h2></div></div></div><p>
					By default, director writes the QCOW2 <code class="literal">overcloud-full</code> image to the root disk during the provisioning process. The <code class="literal">overcloud-full</code> image uses a valid Red Hat subscription. However, you can also use the <code class="literal">overcloud-minimal</code> image, for example, to provision a bare OS where you do not want to run any other OpenStack services and consume your subscription entitlements.
				</p><p>
					A common use case for this occurs when you want to provision nodes with only Ceph daemons. For this and similar use cases, you can use the <code class="literal">overcloud-minimal</code> image option to avoid reaching the limit of your paid Red Hat subscriptions. For information about how to obtain the <code class="literal">overcloud-minimal</code> image, see <a class="link" href="index.html#sect-Obtaining_Images_for_Overcloud_Nodes">Obtaining images for overcloud nodes</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						A Red Hat OpenStack Platform subscription contains Open vSwitch (OVS), but core services, such as OVS, are not available when you use the <code class="literal">overcloud-minimal</code> image. OVS is not required to deploy Ceph Storage nodes. Instead of using <span class="emphasis"><em>ovs_bond</em></span> to define bonds, use <span class="emphasis"><em>linux_bond</em></span>. For more information about <code class="literal">linux_bond</code>, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#linux-bonding-options">Linux bonding options</a>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To configure director to use the <code class="literal">overcloud-minimal</code> image, create an environment file that contains the following image definition:
						</p><pre class="screen">parameter_defaults:
  &lt;roleName&gt;Image: overcloud-minimal</pre></li><li class="listitem"><p class="simpara">
							Replace <code class="literal">&lt;roleName&gt;</code> with the name of the role and append <code class="literal">Image</code> to the name of the role. The following example shows an <code class="literal">overcloud-minimal</code> image for Ceph storage nodes:
						</p><pre class="screen">parameter_defaults:
  CephStorageImage: overcloud-minimal</pre></li><li class="listitem">
							Pass the environment file to the <code class="literal">openstack overcloud deploy</code> command.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">overcloud-minimal</code> image supports only standard Linux bridges and not OVS because OVS is an OpenStack service that requires a Red Hat OpenStack Platform subscription entitlement.
					</p></div></div></section><section class="section" id="creating-architecture-specific-roles"><div class="titlepage"><div><div><h2 class="title">7.9. Creating architecture specific roles</h2></div></div></div><p>
					When building a multi-architecture cloud, you must add any architecture specific roles to the <code class="literal">roles_data.yaml</code> file. The following example includes the <code class="literal">ComputePPC64LE</code> role along with the default roles:
				</p><pre class="screen">openstack overcloud roles generate \
    --roles-path /usr/share/openstack-tripleo-heat-templates/roles -o ~/templates/roles_data.yaml \
    Controller Compute ComputePPC64LE BlockStorage ObjectStorage CephStorage</pre><p>
					The <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/#sect-Creating_a_Custom_Roles_File">Creating a Custom Role File</a> section has information on roles.
				</p></section><section class="section" id="environment-files"><div class="titlepage"><div><div><h2 class="title">7.10. Environment files</h2></div></div></div><p>
					The undercloud includes a set of heat templates that form the plan for your overcloud creation. You can customize aspects of the overcloud with environment files, which are YAML-formatted files that override parameters and resources in the core heat template collection. You can include as many environment files as necessary. However, the order of the environment files is important because the parameters and resources that you define in subsequent environment files take precedence. Use the following list as an example of the environment file order:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The number of nodes and the flavors for each role. It is vital to include this information for overcloud creation.
						</li><li class="listitem">
							The location of the container images for containerized OpenStack services.
						</li><li class="listitem"><p class="simpara">
							Any network isolation files, starting with the initialization file (<code class="literal">environments/network-isolation.yaml</code>) from the heat template collection, then your custom NIC configuration file, and finally any additional network configurations. For more information, see the following chapters in the Advanced Overcloud Customization guide:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/basic-network-isolation">"Basic network isolation"</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/custom-composable-networks">"Custom composable networks"</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/custom-network-interface-templates">"Custom network interface templates"</a>
								</li></ul></div></li><li class="listitem">
							Any external load balancing environment files if you are using an external load balancer. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/external_load_balancing_for_the_overcloud/index">External Load Balancing for the Overcloud</a>.
						</li><li class="listitem">
							Any storage environment files such as Ceph Storage, NFS, or iSCSI.
						</li><li class="listitem">
							Any environment files for Red Hat CDN or Satellite registration.
						</li><li class="listitem">
							Any other custom environment files.
						</li></ul></div><p>
					Red Hat recommends that you organize your custom environment files in a separate directory, such as the <code class="literal">templates</code> directory.
				</p><p>
					For more information about customizing advanced features for your overcloud, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/">Advanced Overcloud Customization</a> guide.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						A basic overcloud uses local LVM storage for block storage, which is not a supported configuration. It is recommended to use an external storage solution, such as Red Hat Ceph Storage, for block storage.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The environment file extension must be <code class="literal">.yaml</code> or <code class="literal">.template</code>, or it will not be treated as a custom template resource.
					</p></div></div><p>
					The next few sections contain information about creating some environment files necessary for your overcloud.
				</p></section><section class="section" id="creating-an-environment-file-that-defines-node-counts-and-flavors"><div class="titlepage"><div><div><h2 class="title">7.11. Creating an environment file that defines node counts and flavors</h2></div></div></div><p>
					By default, director deploys an overcloud with 1 Controller node and 1 Compute node using the <code class="literal">baremetal</code> flavor. However, this is only suitable for a proof-of-concept deployment. You can override the default configuration by specifying different node counts and flavors. For a small-scale production environment, deploy at least 3 Controller nodes and 3 Compute nodes, and assign specific flavors to ensure that the nodes have the appropriate resource specifications. Complete the following steps to create an environment file named <code class="literal">node-info.yaml</code> that stores the node counts and flavor assignments.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a <code class="literal">node-info.yaml</code> file in the <code class="literal">/home/stack/templates/</code> directory:
						</p><pre class="screen">(undercloud) $ touch /home/stack/templates/node-info.yaml</pre></li><li class="listitem"><p class="simpara">
							Edit the file to include the node counts and flavors that you need. This example contains 3 Controller nodes and 3 Compute nodes:
						</p><pre class="screen">parameter_defaults:
  OvercloudControllerFlavor: control
  OvercloudComputeFlavor: compute
  ControllerCount: 3
  ComputeCount: 3</pre></li></ol></div></section><section class="section" id="creating-an-environment-file-for-undercloud-ca-trust"><div class="titlepage"><div><div><h2 class="title">7.12. Creating an environment file for undercloud CA trust</h2></div></div></div><p>
					If your undercloud uses TLS and the Certificate Authority (CA) is not publicly trusted, you can use the CA for SSL endpoint encryption that the undercloud operates. To ensure that the undercloud endpoints are accessible to the rest of your deployment, configure your overcloud nodes to trust the undercloud CA.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For this approach to work, your overcloud nodes must have a network route to the public endpoint on the undercloud. It is likely that you must apply this configuration for deployments that rely on spine-leaf networking.
					</p></div></div><p>
					There are two types of custom certificates you can use in the undercloud:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>User-provided certificates</strong></span> - This definition applies when you have provided your own certificate. This can be from your own CA, or it can be self-signed. This is passed using the <code class="literal">undercloud_service_certificate</code> option. In this case, you must either trust the self-signed certificate, or the CA (depending on your deployment).
						</li><li class="listitem">
							<span class="strong strong"><strong>Auto-generated certificates</strong></span> - This definition applies when you use <code class="literal">certmonger</code> to generate the certificate using its own local CA. Enable auto-generated certificates with the <code class="literal">generate_service_certificate</code> option in the <code class="literal">undercloud.conf</code> file. In this case, director generates a CA certificate at <code class="literal">/etc/pki/ca-trust/source/anchors/cm-local-ca.pem</code> and the director configures the undercloud’s HAProxy instance to use a server certificate. Add the CA certificate to the <code class="literal">inject-trust-anchor-hiera.yaml</code> file to present the certificate to OpenStack Platform.
						</li></ul></div><p>
					This example uses a self-signed certificate located in <code class="literal">/home/stack/ca.crt.pem</code>. If you use auto-generated certificates, use <code class="literal">/etc/pki/ca-trust/source/anchors/cm-local-ca.pem</code> instead.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Open the certificate file and copy only the certificate portion. Do not include the key:
						</p><pre class="screen">$ vi /home/stack/ca.crt.pem</pre><p class="simpara">
							The certificate portion you need looks similar to this shortened example:
						</p><pre class="screen">-----BEGIN CERTIFICATE-----
MIIDlTCCAn2gAwIBAgIJAOnPtx2hHEhrMA0GCSqGSIb3DQEBCwUAMGExCzAJBgNV
BAYTAlVTMQswCQYDVQQIDAJOQzEQMA4GA1UEBwwHUmFsZWlnaDEQMA4GA1UECgwH
UmVkIEhhdDELMAkGA1UECwwCUUUxFDASBgNVBAMMCzE5Mi4xNjguMC4yMB4XDTE3
-----END CERTIFICATE-----</pre></li><li class="listitem"><p class="simpara">
							Create a new YAML file called <code class="literal">/home/stack/inject-trust-anchor-hiera.yaml</code> with the following contents, and include the certificate you copied from the PEM file:
						</p><pre class="screen">parameter_defaults:
  CAMap:
    undercloud-ca:
      content: |
        -----BEGIN CERTIFICATE-----
        MIIDlTCCAn2gAwIBAgIJAOnPtx2hHEhrMA0GCSqGSIb3DQEBCwUAMGExCzAJBgNV
        BAYTAlVTMQswCQYDVQQIDAJOQzEQMA4GA1UEBwwHUmFsZWlnaDEQMA4GA1UECgwH
        UmVkIEhhdDELMAkGA1UECwwCUUUxFDASBgNVBAMMCzE5Mi4xNjguMC4yMB4XDTE3
        -----END CERTIFICATE-----</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The certificate string must follow the PEM format.
					</p></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">CAMap</code> parameter might contain other certificates relevant to SSL/TLS configuration.
					</p></div></div><p>
					Director copies the CA certificate to each overcloud node during the overcloud deployment. As a result, each node trusts the encryption presented by the undercloud’s SSL endpoints. For more information about environment files, see <a class="xref" href="index.html#sect-Including_Environment_Files_in_Overcloud_Creation" title="7.15. Including environment files in an overcloud deployment">Section 7.15, “Including environment files in an overcloud deployment”</a>.
				</p></section><section class="section" id="deployment-command"><div class="titlepage"><div><div><h2 class="title">7.13. Deployment command</h2></div></div></div><p>
					The final stage in creating your OpenStack environment is to run the <code class="literal">openstack overcloud deploy</code> command to create the overcloud. Before you run this command, familiarize yourself with key options and how to include custom environment files.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Do not run <code class="literal">openstack overcloud deploy</code> as a background process. The overcloud creation might hang mid-deployment if you run it as a background process.
					</p></div></div></section><section class="section" id="deployment-command-options"><div class="titlepage"><div><div><h2 class="title">7.14. Deployment command options</h2></div></div></div><p>
					The following table lists the additional parameters for the <code class="literal">openstack overcloud deploy</code> command.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Some options are available in this release as a <span class="emphasis"><em>Technology Preview</em></span> and therefore are not fully supported by Red Hat. They should only be used for testing and should not be used in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><div class="table" id="idm140301179604928"><p class="title"><strong>Table 7.1. Deployment command options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301179165968" scope="col">Parameter</th><th align="left" valign="top" id="idm140301179164880" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--templates [TEMPLATES]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The directory that contains the heat templates that you want to deploy. If blank, the deployment command uses the default template location at <code class="literal">/usr/share/openstack-tripleo-heat-templates/</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--stack STACK</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The name of the stack that you want to create or update
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">-t [TIMEOUT]</code>, <code class="literal">--timeout [TIMEOUT]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The deployment timeout duration in minutes
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--libvirt-type [LIBVIRT_TYPE]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The virtualization type that you want to use for hypervisors
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--ntp-server [NTP_SERVER]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The Network Time Protocol (NTP) server that you want to use to synchronize time. You can also specify multiple NTP servers in a comma-separated list, for example: <code class="literal">--ntp-server 0.centos.pool.org,1.centos.pool.org</code>. For a high availability cluster deployment, it is essential that your Controller nodes are consistently referring to the same time source. Note that a typical environment might already have a designated NTP time source with established practices.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--no-proxy [NO_PROXY]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines custom values for the environment variable <code class="literal">no_proxy</code>, which excludes certain host names from proxy communication.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--overcloud-ssh-user OVERCLOUD_SSH_USER</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the SSH user to access the overcloud nodes. Normally SSH access occurs through the <code class="literal">heat-admin</code> user.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--overcloud-ssh-key OVERCLOUD_SSH_KEY</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the key path for SSH access to overcloud nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--overcloud-ssh-network OVERCLOUD_SSH_NETWORK</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the network name that you want to use for SSH access to overcloud nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">-e [EXTRA HEAT TEMPLATE]</code>, <code class="literal">--extra-template [EXTRA HEAT TEMPLATE]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Extra environment files that you want to pass to the overcloud deployment. You can specify this option more than once. Note that the order of environment files that you pass to the <code class="literal">openstack overcloud deploy</code> command is important. For example, parameters from each sequential environment file override the same parameters from earlier environment files.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--environment-directory</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									A directory that contains environment files that you want to include in deployment. The deployment command processes these environment files in numerical order, then alphabetical order.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">-r ROLES_FILE</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the roles file and overrides the default <code class="literal">roles_data.yaml</code> in the <code class="literal">--templates</code> directory. The file location can be an absolute path or the path relative to <code class="literal">--templates</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">-n NETWORKS_FILE</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the networks file and overrides the default network_data.yaml in the <code class="literal">--templates</code> directory. The file location can be an absolute path or the path relative to <code class="literal">--templates</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">-p PLAN_ENVIRONMENT_FILE</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Defines the plan Environment file and overrides the default <code class="literal">plan-environment.yaml</code> in the <code class="literal">--templates</code> directory. The file location can be an absolute path or the path relative to <code class="literal">--templates</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--no-cleanup</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you do not want to delete temporary files after deployment, and log their location.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--update-plan-only</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to update the plan without performing the actual deployment.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--validation-errors-nonfatal</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The overcloud creation process performs a set of pre-deployment checks. This option exits if any non-fatal errors occur from the pre-deployment checks. It is advisable to use this option as any errors can cause your deployment to fail.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--validation-warnings-fatal</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The overcloud creation process performs a set of pre-deployment checks. This option exits if any non-critical warnings occur from the pre-deployment checks. openstack-tripleo-validations
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--dry-run</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to perform a validation check on the overcloud without creating the overcloud.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--run-validations</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to run external validations from the <code class="literal">openstack-tripleo-validations</code> package.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--skip-postconfig</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to skip the overcloud post-deployment configuration.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--force-postconfig</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to force the overcloud post-deployment configuration.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--skip-deploy-identifier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you do not want the deployment command to generate a unique identifier for the <code class="literal">DeployIdentifier</code> parameter. The software configuration deployment steps only trigger if there is an actual change to the configuration. Use this option with caution and only if you are confident that you do not need to run the software configuration, such as scaling out certain roles.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--answers-file ANSWERS_FILE</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The path to a YAML file with arguments and parameters.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--disable-password-generation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to disable password generation for the overcloud services.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--deployed-server</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to deploy pre-provisioned overcloud nodes. Used in conjunction with <code class="literal">--disable-validations</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--no-config-download, --stack-only</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to disable the <code class="literal">config-download</code> workflow and create only the stack and associated OpenStack resources. This command applies no software configuration to the overcloud.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--config-download-only</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option if you want to disable the overcloud stack creation and only run the <code class="literal">config-download</code> workflow to apply the software configuration.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--output-dir OUTPUT_DIR</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The directory that you want to use for saved <code class="literal">config-download</code> output. The directory must be writeable by the mistral user. When not specified, director uses the default, which is <code class="literal">/var/lib/mistral/overcloud</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--override-ansible-cfg OVERRIDE_ANSIBLE_CFG</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The path to an Ansible configuration file. The configuration in the file overrides any configuration that <code class="literal">config-download</code> generates by default.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--config-download-timeout CONFIG_DOWNLOAD_TIMEOUT</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The timeout duration in minutes that you want to use for <code class="literal">config-download</code> steps. If unset, director sets the default to the amount of time remaining from the <code class="literal">--timeout</code> parameter after the stack deployment operation.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--limit NODE1,NODE2</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									<span class="strong strong"><strong>(Technology Preview)</strong></span> Use this option with a comma-separated list of nodes to limit the config-download playbook execution to a specific node or set of nodes. For example, the <code class="literal">--limit</code> option can be useful for scale-up operations, when you want to run config-download only on new nodes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--tags TAG1,TAG2</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									<span class="strong strong"><strong>(Technology Preview)</strong></span> Use this option with a comma-separated list of tags from the config-download playbook to run the deployment with a specific set of config-download tasks.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--skip-tags TAG1,TAG2</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									<span class="strong strong"><strong>(Technology Preview)</strong></span> Use this option with a comma-separated list of tags that you want to skip from the config-download playbook.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--rhel-reg</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to register overcloud nodes to the Customer Portal or Satellite 6.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--reg-method</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to define the registration method that you want to use for the overcloud nodes. <code class="literal">satellite</code> for Red Hat Satellite 6 or Red Hat Satellite 5, <code class="literal">portal</code> for Customer Portal.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--reg-org [REG_ORG]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The organization that you want to use for registration.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--reg-force</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to register the system even if it is already registered.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--reg-sat-url [REG_SAT_URL]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									The base URL of the Satellite server to register overcloud nodes. Use the Satellite HTTP URL and not the HTTPS URL for this parameter. For example, use <a class="link" href="http://satellite.example.com">http://satellite.example.com</a> and not <a class="link" href="https://satellite.example.com">https://satellite.example.com</a>. The overcloud creation process uses this URL to determine whether the server is a Red Hat Satellite 5 or Red Hat Satellite 6 server. If the server is a Red Hat Satellite 6 server, the overcloud obtains the <code class="literal">katello-ca-consumer-latest.noarch.rpm</code> file, registers with <code class="literal">subscription-manager</code>, and installs <code class="literal">katello-agent</code>. If the server is a Red Hat Satellite 5 server, the overcloud obtains the <code class="literal">RHN-ORG-TRUSTED-SSL-CERT</code> file and registers with <code class="literal">rhnreg_ks</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179165968"> <p>
									<code class="literal">--reg-activation-key [REG_ACTIVATION_KEY]</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179164880"> <p>
									Use this option to define the activation key that you want to use for registration.
								</p>
								 </td></tr></tbody></table></div></div><p>
					Run the following command to view a full list of options:
				</p><pre class="screen">(undercloud) $ openstack help overcloud deploy</pre><p>
					Some command line parameters are outdated or deprecated in favor of using heat template parameters, which you include in the <code class="literal">parameter_defaults</code> section in an environment file. The following table maps deprecated parameters to their heat template equivalents.
				</p><div class="table" id="idm140301179214816"><p class="title"><strong>Table 7.2. Mapping deprecated CLI parameters to heat template parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301179209040" scope="col">Parameter</th><th align="left" valign="top" id="idm140301179207952" scope="col">Description</th><th align="left" valign="top" id="idm140301179206864" scope="col">Heat template parameter</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--control-scale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The number of Controller nodes to scale out
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">ControllerCount</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--compute-scale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The number of Compute nodes to scale out
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">ComputeCount</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--ceph-storage-scale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The number of Ceph Storage nodes to scale out
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">CephStorageCount</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--block-storage-scale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The number of Block Storage (cinder) nodes to scale out
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">BlockStorageCount</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--swift-storage-scale</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The number of Object Storage (swift) nodes to scale out
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">ObjectStorageCount</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--control-flavor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The flavor that you want to use for Controller nodes
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">OvercloudControllerFlavor</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--compute-flavor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The flavor that you want to use for Compute nodes
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">OvercloudComputeFlavor</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--ceph-storage-flavor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The flavor that you want to use for Ceph Storage nodes
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">OvercloudCephStorageFlavor</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--block-storage-flavor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The flavor that you want to use for Block Storage (cinder) nodes
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">OvercloudBlockStorageFlavor</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--swift-storage-flavor</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The flavor that you want to use for Object Storage (swift) nodes
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									<code class="literal">OvercloudSwiftStorageFlavor</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--validation-errors-fatal</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									The overcloud creation process performs a set of pre-deployment checks. This option exits if any fatal errors occur from the pre-deployment checks. It is advisable to use this option because any errors can cause your deployment to fail.
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									No parameter mapping
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--disable-validations</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									Disable the pre-deployment validations entirely. These validations were built-in pre-deployment validations, which have been replaced with external validations from the <code class="literal">openstack-tripleo-validations</code> package.
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									No parameter mapping
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301179209040"> <p>
									<code class="literal">--config-download</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301179207952"> <p>
									Run deployment using the <code class="literal">config-download</code> mechanism. This is now the default and this CLI options may be removed in the future.
								</p>
								 </td><td align="left" valign="top" headers="idm140301179206864"> <p>
									No parameter mapping
								</p>
								 </td></tr></tbody></table></div></div><p>
					These parameters are scheduled for removal in a future version of Red Hat OpenStack Platform.
				</p></section><section class="section" id="sect-Including_Environment_Files_in_Overcloud_Creation"><div class="titlepage"><div><div><h2 class="title">7.15. Including environment files in an overcloud deployment</h2></div></div></div><p>
					Use the <code class="literal">-e</code> option to include an environment file to customize your overcloud. You can include as many environment files as necessary. However, the order of the environment files is important because the parameters and resources that you define in subsequent environment files take precedence. Use the following list as an example of the environment file order:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The number of nodes and the flavors for each role. It is vital to include this information for overcloud creation.
						</li><li class="listitem">
							The location of the container images for containerized OpenStack services.
						</li><li class="listitem"><p class="simpara">
							Any network isolation files, starting with the initialization file (<code class="literal">environments/network-isolation.yaml</code>) from the heat template collection, then your custom NIC configuration file, and finally any additional network configurations. For more information, see the following chapters in the Advanced Overcloud Customization guide:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/basic-network-isolation">"Basic network isolation"</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/custom-composable-networks">"Custom composable networks"</a>
								</li><li class="listitem">
									<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/custom-network-interface-templates">"Custom network interface templates"</a>
								</li></ul></div></li><li class="listitem">
							Any external load balancing environment files if you are using an external load balancer. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/external_load_balancing_for_the_overcloud/index">External Load Balancing for the Overcloud</a>.
						</li><li class="listitem">
							Any storage environment files such as Ceph Storage, NFS, or iSCSI.
						</li><li class="listitem">
							Any environment files for Red Hat CDN or Satellite registration.
						</li><li class="listitem">
							Any other custom environment files.
						</li></ul></div><p>
					Any environment files that you add to the overcloud using the <code class="literal">-e</code> option become part of the stack definition of the overcloud.
				</p><p>
					The following command is an example of how to start the overcloud creation using environment files defined earlier in this scenario:
				</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e /home/stack/templates/node-info.yaml\
  -e /home/stack/containers-prepare-parameter.yaml \
  -e /home/stack/inject-trust-anchor-hiera.yaml
  -r /home/stack/templates/roles_data.yaml \</pre><p>
					This command contains the following additional options:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">--templates</span></dt><dd>
								Creates the overcloud using the heat template collection in <code class="literal">/usr/share/openstack-tripleo-heat-templates</code> as a foundation.
							</dd><dt><span class="term">-e /home/stack/templates/node-info.yaml</span></dt><dd>
								Adds an environment file to define how many nodes and which flavors to use for each role.
							</dd><dt><span class="term">-e /home/stack/containers-prepare-parameter.yaml</span></dt><dd>
								Adds the container image preparation environment file. You generated this file during the undercloud installation and can use the same file for your overcloud creation.
							</dd><dt><span class="term">-e /home/stack/inject-trust-anchor-hiera.yaml</span></dt><dd>
								Adds an environment file to install a custom certificate in the undercloud.
							</dd><dt><span class="term">-r /home/stack/templates/roles_data.yaml</span></dt><dd>
								(Optional) The generated roles data if you use custom roles or want to enable a multi architecture cloud. For more information, see <a class="xref" href="index.html#creating-architecture-specific-roles" title="7.9. Creating architecture specific roles">Section 7.9, “Creating architecture specific roles”</a>.
							</dd></dl></div><p>
					Director requires these environment files for re-deployment and post-deployment functions. Failure to include these files can result in damage to your overcloud.
				</p><p>
					To modify the overcloud configuration at a later stage, perform the following actions:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Modify parameters in the custom environment files and heat templates.
						</li><li class="listitem">
							Run the <code class="literal">openstack overcloud deploy</code> command again with the same environment files.
						</li></ol></div><p>
					Do not edit the overcloud configuration directly because director overrides any manual configuration when you update the overcloud stack.
				</p></section><section class="section" id="validating-the-deployment-requirements"><div class="titlepage"><div><div><h2 class="title">7.16. Validating the deployment requirements</h2></div></div></div><p>
					Run the <span class="literal literal">pre-deployment</span> validation group to check the deployment requirements.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file.
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							This validation requires a copy of your overcloud plan. Upload your overcloud plan with all necessary environment files. To upload your plan only, run the <code class="literal">openstack overcloud deploy</code> command with the <code class="literal">--update-plan-only</code> option:
						</p><pre class="screen">$ openstack overcloud deploy --templates \
    -e environment-file1.yaml \
    -e environment-file2.yaml \
    ...
    --update-plan-only</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack tripleo validator run</code> command with the <span class="literal literal">--group pre-deployment</span> option:
						</p><pre class="screen">$ openstack tripleo validator run --group pre-deployment</pre></li><li class="listitem"><p class="simpara">
							If the overcloud uses a plan name that is different to the default <code class="literal">overcloud</code> name, set the plan name with the --<code class="literal">plan</code> option:
						</p><pre class="screen">$ openstack tripleo validator run --group pre-deployment \
    --plan myovercloud</pre></li><li class="listitem"><p class="simpara">
							Review the results of the validation report. To view detailed output from a specific validation, run the <code class="literal">openstack tripleo validator show run</code> command against the UUID of the specific validation from the report:
						</p><pre class="screen">$ openstack tripleo validator show run &lt;UUID&gt;</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						A <code class="literal">FAILED</code> validation does not prevent you from deploying or running Red Hat OpenStack Platform. However, a <code class="literal">FAILED</code> validation can indicate a potential issue with a production environment.
					</p></div></div></section><section class="section" id="overcloud-deployment-output-basic"><div class="titlepage"><div><div><h2 class="title">7.17. Overcloud deployment output</h2></div></div></div><p>
					When the overcloud creation completes, director provides a recap of the Ansible plays that were executed to configure the overcloud:
				</p><pre class="screen">PLAY RECAP *************************************************************
overcloud-compute-0     : ok=160  changed=67   unreachable=0    failed=0
overcloud-controller-0  : ok=210  changed=93   unreachable=0    failed=0
undercloud              : ok=10   changed=7    unreachable=0    failed=0

Tuesday 15 October 2018  18:30:57 +1000 (0:00:00.107) 1:06:37.514 ******
========================================================================</pre><p>
					Director also provides details to access your overcloud.
				</p><pre class="screen">Ansible passed.
Overcloud configuration completed.
Overcloud Endpoint: http://192.168.24.113:5000
Overcloud Horizon Dashboard URL: http://192.168.24.113:80/dashboard
Overcloud rc file: /home/stack/overcloudrc
Overcloud Deployed</pre></section><section class="section" id="accessing-the-overcloud-basic"><div class="titlepage"><div><div><h2 class="title">7.18. Accessing the overcloud</h2></div></div></div><p>
					The director generates a script to configure and help authenticate interactions with your overcloud from the undercloud. The director saves this file, <code class="literal">overcloudrc</code>, in the home directory of the <code class="literal">stack</code> user. Run the following command to use this file:
				</p><pre class="screen">(undercloud) $ source ~/overcloudrc</pre><p>
					This command loads the environment variables that are necessary to interact with your overcloud from the undercloud CLI. The command prompt changes to indicate this:
				</p><pre class="screen">(overcloud) $</pre><p>
					To return to interacting with the undercloud, run the following command:
				</p><pre class="screen">(overcloud) $ source ~/stackrc
(undercloud) $</pre><p>
					Each node in the overcloud also contains a <code class="literal">heat-admin</code> user. The <code class="literal">stack</code> user has SSH access to this user on each node. To access a node over SSH, find the IP address of the node that you want to access:
				</p><pre class="screen">(undercloud) $ openstack server list</pre><p>
					Then connect to the node using the <code class="literal">heat-admin</code> user and the IP address of the node:
				</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.24.23</pre></section><section class="section" id="validating-the-post-deployment-state"><div class="titlepage"><div><div><h2 class="title">7.19. Validating the post-deployment state</h2></div></div></div><p>
					Run the <span class="literal literal">post-deployment</span> validation group to check the post-deployment state.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file.
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack tripleo validator run</code> command with the <span class="literal literal">--group post-deployment</span> option:
						</p><pre class="screen">$ openstack tripleo validator run --group post-deployment</pre></li><li class="listitem"><p class="simpara">
							If the overcloud uses a plan name that is different to the default <code class="literal">overcloud</code> name, set the plan name with the --<code class="literal">plan</code> option:
						</p><pre class="screen">$ openstack tripleo validator run --group post-deployment \
    --plan myovercloud</pre></li><li class="listitem"><p class="simpara">
							Review the results of the validation report. To view detailed output from a specific validation, run the <code class="literal">openstack tripleo validator show run</code> command against the UUID of the specific validation from the report:
						</p><pre class="screen">$ openstack tripleo validator show run &lt;UUID&gt;</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						A <code class="literal">FAILED</code> validation does not prevent you from deploying or running Red Hat OpenStack Platform. However, a <code class="literal">FAILED</code> validation can indicate a potential issue with a production environment.
					</p></div></div></section><section class="section" id="next_steps_3"><div class="titlepage"><div><div><h2 class="title">7.20. Next steps</h2></div></div></div><p>
					This concludes the creation of the overcloud using the command line tools. For more information about post-creation functions, see <a class="xref" href="index.html#performing-overcloud-post-installation-tasks" title="Chapter 11. Performing overcloud post-installation tasks">Chapter 11, <em>Performing overcloud post-installation tasks</em></a>.
				</p></section></section><section class="chapter" id="provisioning-bare-metal-nodes-before-deploying-the-overcloud"><div class="titlepage"><div><div><h2 class="title">Chapter 8. Provisioning bare metal nodes before deploying the overcloud</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p></div></div><p>
				The overcloud deployment process contains two primary operations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Provisioning nodes
					</li><li class="listitem">
						Deploying the overcloud
					</li></ul></div><p>
				You can mitigate some of the risk involved with this process and identify points of failure more efficiently if you separate these operations into distinct processes:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Provision your bare metal nodes.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Create a node definition file in yaml format.
							</li><li class="listitem">
								Run the provisioning command, including the node definition file.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Deploy your overcloud.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Run the deployment command, including the heat environment file that the provisioning command generates.
							</li></ol></div></li></ol></div><p>
				The provisioning process provisions your nodes and generates a heat environment file that contains various node specifications, including node count, predictive node placement, custom images, and custom NICs. When you deploy your overcloud, include this file in the deployment command.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You cannot combine pre-provisioned nodes with director-provisioned nodes.
				</p></div></div><section class="section" id="sect-Registering_Nodes_for_the_Overcloud-novaless-provisioning"><div class="titlepage"><div><div><h2 class="title">8.1. Registering nodes for the overcloud</h2></div></div></div><p>
					Director requires a node definition template, which you create manually. This template uses a JSON or YAML format, and contains the hardware and power management details for your nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a template that lists your nodes. Use the following JSON and YAML template examples to understand how to structure your node definition template:
						</p><div class="formalpara"><p class="title"><strong>Example JSON template</strong></p><p>
								
<pre class="screen">{
    "nodes":[
        {
            "mac":[
                "bb:bb:bb:bb:bb:bb"
            ],
            "name":"node01",
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.168.24.205"
        },
        {
            "mac":[
                "cc:cc:cc:cc:cc:cc"
            ],
            "name":"node02",
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.168.24.206"
        }
    ]
}</pre>

							</p></div><div class="formalpara"><p class="title"><strong>Example YAML template</strong></p><p>
								
<pre class="screen">nodes:
  - mac:
      - "bb:bb:bb:bb:bb:bb"
    name: "node01"
    cpu: 4
    memory: 6144
    disk: 40
    arch: "x86_64"
    pm_type: "ipmi"
    pm_user: "admin"
    pm_password: "p@55w0rd!"
    pm_addr: "192.168.24.205"
  - mac:
      - cc:cc:cc:cc:cc:cc
    name: "node02"
    cpu: 4
    memory: 6144
    disk: 40
    arch: "x86_64"
    pm_type: "ipmi"
    pm_user: "admin"
    pm_password: "p@55w0rd!"
    pm_addr: "192.168.24.206"</pre>

							</p></div><p class="simpara">
							This template contains the following attributes:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">name</span></dt><dd>
										The logical name for the node.
									</dd><dt><span class="term">pm_type</span></dt><dd><p class="simpara">
										The power management driver that you want to use. This example uses the IPMI driver (<code class="literal">ipmi</code>).
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											IPMI is the preferred supported power management driver. For more information about supported power management types and their options, see <a class="xref" href="index.html#appe-Power_Management_Drivers" title="Appendix A. Power management drivers">Appendix A, <em>Power management drivers</em></a>. If these power management drivers do not work as expected, use IPMI for your power management.
										</p></div></div></dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
										The IPMI username and password.
									</dd><dt><span class="term">pm_addr</span></dt><dd>
										The IP address of the IPMI device.
									</dd><dt><span class="term">pm_port (Optional)</span></dt><dd>
										The port to access the specific IPMI device.
									</dd><dt><span class="term">mac</span></dt><dd>
										(Optional) A list of MAC addresses for the network interfaces on the node. Use only the MAC address for the Provisioning NIC of each system.
									</dd><dt><span class="term">cpu</span></dt><dd>
										(Optional) The number of CPUs on the node.
									</dd><dt><span class="term">memory</span></dt><dd>
										(Optional) The amount of memory in MB.
									</dd><dt><span class="term">disk</span></dt><dd>
										(Optional) The size of the hard disk in GB.
									</dd><dt><span class="term">arch</span></dt><dd><p class="simpara">
										(Optional) The system architecture.
									</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
											When building a multi-architecture cloud, the <code class="literal">arch</code> key is mandatory to distinguish nodes using <code class="literal">x86_64</code> and <code class="literal">ppc64le</code> architectures.
										</p></div></div></dd></dl></div></li><li class="listitem"><p class="simpara">
							After you create the template, run the following commands to verify the formatting and syntax:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud node import --validate-only ~/nodes.json</pre></li><li class="listitem"><p class="simpara">
							Save the file to the home directory of the <code class="literal">stack</code> user (<code class="literal">/home/stack/nodes.json</code>), then run the following commands to import the template to director:
						</p><pre class="screen">(undercloud) $ openstack overcloud node import ~/nodes.json</pre><p class="simpara">
							This command registers each node from the template into director.
						</p></li><li class="listitem"><p class="simpara">
							Wait for the node registration and configuration to complete. When complete, confirm that director has successfully registered the nodes:
						</p><pre class="screen">(undercloud) $ openstack baremetal node list</pre></li></ol></div></section><section class="section" id="inspecting-the-hardware-of-nodes-novaless-provisioning"><div class="titlepage"><div><div><h2 class="title">8.2. Inspecting the hardware of nodes</h2></div></div></div><p>
					Director can run an introspection process on each node. This process boots an introspection agent over PXE on each node. The introspection agent collects hardware data from the node and sends the data back to director. Director then stores this introspection data in the OpenStack Object Storage (swift) service running on director. Director uses hardware information for various purposes such as profile tagging, benchmarking, and manual root disk assignment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to inspect the hardware attributes of each node:
						</p><pre class="screen">(undercloud) $ openstack overcloud node introspect --all-manageable --provide</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Use the <code class="literal">--all-manageable</code> option to introspect only the nodes that are in a managed state. In this example, all nodes are in a managed state.
								</li><li class="listitem">
									Use the <code class="literal">--provide</code> option to reset all nodes to an <code class="literal">available</code> state after introspection.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Monitor the introspection progress logs in a separate terminal window:
						</p><pre class="screen">(undercloud) $ sudo tail -f /var/log/containers/ironic-inspector/ironic-inspector.log</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Ensure that this process runs to completion. This process usually takes 15 minutes for bare metal nodes.
							</p></div></div></li></ol></div><p>
					After the introspection completes, all nodes change to an <code class="literal">available</code> state.
				</p></section><section class="section" id="provisioning-bare-metal-nodes"><div class="titlepage"><div><div><h2 class="title">8.3. Provisioning bare metal nodes</h2></div></div></div><p>
					Create a new YAML file <code class="literal">~/overcloud-baremetal-deploy.yaml</code>, define the quantity and attributes of the bare metal nodes that you want to deploy, and assign overcloud roles to these nodes. The provisioning process creates a heat environment file that you can include in your <code class="literal">openstack overcloud deploy</code> command.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
						</li><li class="listitem">
							Bare metal nodes introspected and available for provisioning and deployment. For more information, see <a class="xref" href="index.html#sect-Registering_Nodes_for_the_Overcloud-novaless-provisioning" title="8.1. Registering nodes for the overcloud">Section 8.1, “Registering nodes for the overcloud”</a> and <a class="xref" href="index.html#inspecting-the-hardware-of-nodes-novaless-provisioning" title="8.2. Inspecting the hardware of nodes">Section 8.2, “Inspecting the hardware of nodes”</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> undercloud credential file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Create a new <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file and define the node count for each role that you want to provision. For example, to provision three Controller nodes and three Compute nodes, use the following syntax:
						</p><pre class="screen">- name: Controller
  count: 3
- name: Compute
  count: 3</pre></li><li class="listitem"><p class="simpara">
							In the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file, define any predictive node placements, custom images, custom NICs, or other attributes that you want to assign to your nodes. For example, use the following example syntax to provision three Controller nodes on nodes <code class="literal">node00</code>, <code class="literal">node01</code>, and <code class="literal">node02</code>, and three Compute nodes on <code class="literal">node04</code>, <code class="literal">node05</code>, and <code class="literal">node06</code>:
						</p><pre class="screen">- name: Controller
  count: 3
  instances:
  - hostname: overcloud-controller-0
    name: node00
  - hostname: overcloud-controller-1
    name: node01
  - hostname: overcloud-controller-2
    name: node02
- name: Compute
  count: 3
  instances:
  - hostname: overcloud-novacompute-0
    name: node04
  - hostname: overcloud-novacompute-1
    name: node05
  - hostname: overcloud-novacompute-2
    name: node06</pre><p class="simpara">
							By default, the provisioning process uses the <code class="literal">overcloud-full</code> image. You can use the <code class="literal">image</code> attribute in the <code class="literal">instances</code> parameter to define a custom image:
						</p><pre class="screen">- name: Controller
  count: 3
  instances:
  - hostname: overcloud-controller-0
    name: node00
    image:
      href: overcloud-custom</pre><p class="simpara">
							You can also override the default parameter values with the <code class="literal">defaults</code> parameter to avoid manual node definitions for each node entry:
						</p><pre class="screen">- name: Controller
  count: 3
  defaults:
    image:
      href: overcloud-custom
  instances:
  - hostname :overcloud-controller-0
    name: node00
  - hostname: overcloud-controller-1
    name: node01
  - hostname: overcloud-controller-2
    name: node02</pre><p class="simpara">
							For more information about the parameters, attributes, and values that you can use in your node definition file, see <a class="xref" href="index.html#bare-metal-node-provisioning-attributes" title="8.6. Bare metal node provisioning attributes">Section 8.6, “Bare metal node provisioning attributes”</a>.
						</p></li><li class="listitem"><p class="simpara">
							Run the provisioning command, specifying the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file and defining an output file with the <code class="literal">--output</code> option:
						</p><pre class="screen">(undercloud) $ sudo openstack overcloud node provision \
--stack stack \
--output ~/overcloud-baremetal-deployed.yaml \
~/overcloud-baremetal-deploy.yaml</pre><p class="simpara">
							The provisioning process generates a heat environment file with the name that you specify in the <code class="literal">--output</code> option. This file contains your node definitions. When you deploy the overcloud, include this file in the deployment command.
						</p></li><li class="listitem"><p class="simpara">
							In a separate terminal, monitor your nodes to verify that they provision successfully. The provisioning process changes the node state from <code class="literal">available</code> to <code class="literal">active</code>:
						</p><pre class="screen">(undercloud) $ watch openstack baremetal node list</pre><p class="simpara">
							Use the <code class="literal">metalsmith</code> tool to obtain a unified view of your nodes, including allocations and neutron ports:
						</p><pre class="screen">(undercloud) $ metalsmith list</pre><p class="simpara">
							You can also use the <code class="literal">openstack baremetal allocation</code> command to verify association of nodes to hostnames, and to obtain IP addresses for the provisioned nodes:
						</p><pre class="screen">(undercloud) $ openstack baremetal allocation list</pre></li></ol></div><p>
					When your nodes are provisioned successfully, you can deploy the overcloud. For more information, see <a class="xref" href="index.html#configuring-a-basic-overcloud-with-pre-provisioned-nodes" title="Chapter 9. Configuring a basic overcloud with pre-provisioned nodes">Chapter 9, <em>Configuring a basic overcloud with pre-provisioned nodes</em></a>.
				</p></section><section class="section" id="scaling-up-bare-metal-nodes"><div class="titlepage"><div><div><h2 class="title">8.4. Scaling up bare metal nodes</h2></div></div></div><p>
					To increase the count of bare metal nodes in an existing overcloud, increment the node count in the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file and redeploy the overcloud.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
						</li><li class="listitem">
							A successful overcloud deployment. For more information, see <a class="xref" href="index.html#configuring-a-basic-overcloud-with-pre-provisioned-nodes" title="Chapter 9. Configuring a basic overcloud with pre-provisioned nodes">Chapter 9, <em>Configuring a basic overcloud with pre-provisioned nodes</em></a>.
						</li><li class="listitem">
							Bare metal nodes introspected and available for provisioning and deployment. For more information, see <a class="xref" href="index.html#sect-Registering_Nodes_for_the_Overcloud-novaless-provisioning" title="8.1. Registering nodes for the overcloud">Section 8.1, “Registering nodes for the overcloud”</a> and <a class="xref" href="index.html#inspecting-the-hardware-of-nodes-novaless-provisioning" title="8.2. Inspecting the hardware of nodes">Section 8.2, “Inspecting the hardware of nodes”</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> undercloud credential file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file that you used to provision your bare metal nodes, and increment the <code class="literal">count</code> parameter for the roles that you want to scale up. For example, if your overcloud contains three Compute nodes, use the following snippet to increase the Compute node count to 10:
						</p><pre class="screen">- name: Controller
  count: 3
- name: Compute
  count: 10</pre><p class="simpara">
							You can also add predictive node placement with the <code class="literal">instances</code> parameter. For more information about the parameters and attributes that are available, see <a class="xref" href="index.html#bare-metal-node-provisioning-attributes" title="8.6. Bare metal node provisioning attributes">Section 8.6, “Bare metal node provisioning attributes”</a>.
						</p></li><li class="listitem"><p class="simpara">
							Run the provisioning command, specifying the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file and defining an output file with the <code class="literal">--output</code> option:
						</p><pre class="screen">(undercloud) $ sudo openstack overcloud node provision \
--stack stack \
--output ~/overcloud-baremetal-deployed.yaml \
~/overcloud-baremetal-deploy.yaml</pre></li><li class="listitem">
							Monitor the provisioning progress with the <code class="literal">openstack baremetal node list</code> command.
						</li><li class="listitem"><p class="simpara">
							Deploy the overcloud, including the <code class="literal">~/overcloud-baremetal-deployed.yaml</code> file that the provisioning command generates, along with any other environment files relevant to your deployment:
						</p><pre class="screen">(undercloud) $ openstack overcloud deploy \
  ...
  -e /usr/share/openstack-tripleo-heat-templates/environments/deployed-server-environment.yaml \
  -e ~/overcloud-baremetal-deployed.yaml \
  --deployed-server \
  --disable-validations \
  ...</pre></li></ol></div></section><section class="section" id="scaling-down-bare-metal-nodes"><div class="titlepage"><div><div><h2 class="title">8.5. Scaling down bare metal nodes</h2></div></div></div><p>
					Tag the nodes that you want to delete from the stack in the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file, redeploy the overcloud, and then include this file in the <code class="literal">openstack overcloud node delete</code> command with the <code class="literal">--baremetal-deployment</code> option.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
						</li><li class="listitem">
							A successful overcloud deployment. For more information, see <a class="xref" href="index.html#configuring-a-basic-overcloud-with-pre-provisioned-nodes" title="Chapter 9. Configuring a basic overcloud with pre-provisioned nodes">Chapter 9, <em>Configuring a basic overcloud with pre-provisioned nodes</em></a>.
						</li><li class="listitem">
							At least one bare metal node that you want to remove from the stack.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> undercloud credential file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file that you used to provision your bare metal nodes, and decrement the <code class="literal">count</code> parameter for the roles that you want to scale down. You must also define the following attributes for each node that you want to remove from the stack:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The name of the node.
								</li><li class="listitem">
									The hostname that is associated with the node.
								</li><li class="listitem"><p class="simpara">
									The attribute <code class="literal">provisioned: false</code>.
								</p><p class="simpara">
									For example, to remove the node <code class="literal">overcloud-controller-1</code> from the stack, include the following snippet in your <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file:
								</p><pre class="screen">- name: Controller
  count: 2
  instances:
  - hostname: overcloud-controller-0
    name: node00
  - hostname: overcloud-controller-1
    name: node01
    # Removed from cluster due to disk failure
    provisioned: false
  - hostname: overcloud-controller-2
    name: node02</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Run the provisioning command, specifying the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file and defining an output file with the <code class="literal">--output</code> option:
						</p><pre class="screen">(undercloud) $ sudo openstack overcloud node provision \
--stack stack \
--output ~/overcloud-baremetal-deployed.yaml \
~/overcloud-baremetal-deploy.yaml</pre></li><li class="listitem"><p class="simpara">
							Redeploy the overcloud and include the <code class="literal">~/overcloud-baremetal-deployed.yaml</code> file that the provisioning command generates, along with any other environment files relevant to your deployment:
						</p><pre class="screen">(undercloud) $ openstack overcloud deploy \
  ...
  -e /usr/share/openstack-tripleo-heat-templates/environments/deployed-server-environment.yaml \
  -e ~/overcloud-baremetal-deployed.yaml \
  --deployed-server \
  --disable-validations \
  ...</pre><p class="simpara">
							After you redeploy the overcloud, the nodes that you define with the <code class="literal">provisioned: false</code> attribute are no longer present in the stack. However, these nodes are still running in a provisioned state.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you want to remove a node from the stack temporarily, you can deploy the overcloud with the attribute <code class="literal">provisioned: false</code> and then redeploy the overcloud with the attribute <code class="literal">provisioned: true</code> to return the node to the stack.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack overcloud node delete</code> command, including the <code class="literal">~/overcloud-baremetal-deploy.yaml</code> file with the <code class="literal">--baremetal-deployment</code> option.
						</p><pre class="screen">(undercloud) $ sudo openstack overcloud node delete \
--stack stack \
--baremetal-deployment ~/overcloud-baremetal-deploy.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Do not include the nodes that you want to remove from the stack as command arguments in the <code class="literal">openstack overcloud node delete</code> command.
							</p></div></div></li></ol></div></section><section class="section" id="bare-metal-node-provisioning-attributes"><div class="titlepage"><div><div><h2 class="title">8.6. Bare metal node provisioning attributes</h2></div></div></div><p>
					Use the following tables to understand the parameters, attributes, and values that are available for you to use when you provision bare metal nodes with the <code class="literal">openstack baremetal node provision</code> command.
				</p><div class="table" id="idm140301174110288"><p class="title"><strong>Table 8.1. Role parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301177410240" scope="col">Parameter</th><th align="left" valign="top" id="idm140301177409152" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301177410240"> <p>
									name
								</p>
								 </td><td align="left" valign="top" headers="idm140301177409152"> <p>
									Mandatory role name
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177410240"> <p>
									count
								</p>
								 </td><td align="left" valign="top" headers="idm140301177409152"> <p>
									The number of nodes that you want to provision for this role. The default value is <code class="literal">1</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177410240"> <p>
									defaults
								</p>
								 </td><td align="left" valign="top" headers="idm140301177409152"> <p>
									A dictionary of default values for <code class="literal">instances</code> entry properties. An <code class="literal">instances</code> entry property overrides any defaults that you specify in the <code class="literal">defaults</code> parameter.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177410240"> <p>
									instances
								</p>
								 </td><td align="left" valign="top" headers="idm140301177409152"> <p>
									A dictionary of values that you can use to specify attributes for specific nodes. For more information about supported properties in the <code class="literal">instances</code> parameter, see <a class="xref" href="index.html#instances-and-defaults-properties" title="Table 8.2. instances and defaults parameters">Table 8.2, “<code class="literal">instances</code> and <code class="literal">defaults</code> parameters”</a>. The length of this list must not be greater than the value of the <code class="literal">count</code> parameter.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301177410240"> <p>
									hostname_format
								</p>
								 </td><td align="left" valign="top" headers="idm140301177409152"> <p>
									Overrides the default hostname format for this role. The default format uses the lower case role name. For example, the default format for the Controller role is <code class="literal">%stackname%-controller-%index%</code>. Only the Compute role does not follow the role name rule. The Compute default format is <code class="literal">%stackname%-novacompute-%index%</code>
								</p>
								 </td></tr></tbody></table></div></div><h5 id="example_syntax">Example syntax</h5><p>
					In the following example, the <code class="literal">name</code> refers to the logical name of the node, and the <code class="literal">hostname</code> refers to the generated hostname which is derived from the overcloud stack name, the role, and an incrementing index. All Controller servers use a default custom image <code class="literal">overcloud-full-custom</code> and are on predictive nodes. One of the Compute servers is placed predictively on <code class="literal">node04</code> with custom host name <code class="literal">overcloud-compute-special</code>, and the other 99 Compute servers are on nodes allocated automatically from the pool of available nodes:
				</p><pre class="screen">- name: Controller
  count: 3
  defaults:
    image:
      href: file:///var/lib/ironic/images/overcloud-full-custom.qcow2
  instances:
  - hostname: overcloud-controller-0
    name: node00
  - hostname: overcloud-controller-1
    name: node01
  - hostname: overcloud-controller-2
    name: node02
- name: Compute
  count: 100
  instances:
  - hostname: overcloud-compute-special
    name: node04</pre><div class="table" id="instances-and-defaults-properties"><p class="title"><strong>Table 8.2. <code class="literal">instances</code> and <code class="literal">defaults</code> parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301166853376" scope="col">Parameter</th><th align="left" valign="top" id="idm140301166852288" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									hostname
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									If the hostname complies with the <code class="literal">hostname_format</code> pattern then other properties apply to the node allocated to this hostname. Otherwise, you can use a custom hostname for this node.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									name
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									The name of the node that you want to provision.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									image
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Details of the image that you want to provision onto the node. For more information about supported properties in the <code class="literal">image</code> parameter, see <a class="xref" href="index.html#image-properties" title="Table 8.3. image parameters">Table 8.3, “<code class="literal">image</code> parameters”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									capabilities
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Selection criteria to match the node capabilities.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									nics
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									List of dictionaries that represent requested NICs. For more information about supported properties in the <code class="literal">nics</code> parameter, see <a class="xref" href="index.html#nic-properties" title="Table 8.4. nic parameters">Table 8.4, “<code class="literal">nic</code> parameters”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									profile
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Selection criteria to use Advanced Profile Matching.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									provisioned
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Boolean to determine whether this node is provisioned or unprovisioned. The default value is <code class="literal">true</code>. Use <code class="literal">false</code> to unprovision a node. For more information, see <a class="xref" href="index.html#scaling-down-bare-metal-nodes" title="8.5. Scaling down bare metal nodes">Section 8.5, “Scaling down bare metal nodes”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									resource_class
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Selection criteria to match the resource class of the node. The default value is <code class="literal">baremetal</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									root_size_gb
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Size of the root partition in GiB. The default value is <code class="literal">49</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									swap_size_mb
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									Size of the swap partition in MiB.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166853376"> <p>
									traits
								</p>
								 </td><td align="left" valign="top" headers="idm140301166852288"> <p>
									A list of traits as selection criteria to match the node traits.
								</p>
								 </td></tr></tbody></table></div></div><h5 id="example_syntax_2">Example syntax</h5><p>
					In the following example, all Controller servers use a custom default overcloud image <code class="literal">overcloud-full-custom</code>. The Controller server <code class="literal">overcloud-controller-0</code> is placed predictively on <code class="literal">node00</code> and has custom root and swap sizes. The other two Controller servers are on nodes allocated automatically from the pool of available nodes, and have default root and swap sizes:
				</p><pre class="screen">- name: Controller
  count: 3
  defaults:
    image:
      href: file:///var/lib/ironic/images/overcloud-full-custom.qcow2
  instances:
  - hostname: overcloud-controller-0
    name: node00
    root_size_gb: 140
    swap_size_mb: 600</pre><div class="table" id="image-properties"><p class="title"><strong>Table 8.3. <code class="literal">image</code> parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301186809760" scope="col">Parameter</th><th align="left" valign="top" id="idm140301186808672" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301186809760"> <p>
									href
								</p>
								 </td><td align="left" valign="top" headers="idm140301186808672"> <p>
									Glance image reference or URL of the root partition or whole disk image. URL schemes supported are <code class="literal">file://</code>, <code class="literal">http://</code>, and <code class="literal">https://</code>. If the value is not a valid URL, this value must be a valid glance image reference.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301186809760"> <p>
									checksum
								</p>
								 </td><td align="left" valign="top" headers="idm140301186808672"> <p>
									When the href is a URL, this value must be the SHA512 checksum of the root partition or whole disk image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301186809760"> <p>
									kernel
								</p>
								 </td><td align="left" valign="top" headers="idm140301186808672"> <p>
									Glance image reference or URL of the kernel image. Use this property only for partition images.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301186809760"> <p>
									ramdisk
								</p>
								 </td><td align="left" valign="top" headers="idm140301186808672"> <p>
									Glance image reference or URL of the ramdisk image. Use this property only for partition images.
								</p>
								 </td></tr></tbody></table></div></div><h5 id="example_syntax_3">Example syntax</h5><p>
					In the following example, all three Controller servers are on nodes allocated automatically from the pool of available nodes. All Controller servers in this environment use a default custom image <code class="literal">overcloud-full-custom</code>:
				</p><pre class="screen">- name: Controller
  count: 3
  defaults:
    image:
      href: file:///var/lib/ironic/images/overcloud-full-custom.qcow2
      checksum: 1582054665
      kernel: file:///var/lib/ironic/images/overcloud-full-custom.vmlinuz
      ramdisk: file:///var/lib/ironic/images/overcloud-full-custom.initrd</pre><div class="table" id="nic-properties"><p class="title"><strong>Table 8.4. <code class="literal">nic</code> parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301164925024" scope="col">Parameter</th><th align="left" valign="top" id="idm140301164923936" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301164925024"> <p>
									fixed_ip
								</p>
								 </td><td align="left" valign="top" headers="idm140301164923936"> <p>
									The specific IP address that you want to use for this NIC.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164925024"> <p>
									network
								</p>
								 </td><td align="left" valign="top" headers="idm140301164923936"> <p>
									The neutron network where you want to create the port for this NIC.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164925024"> <p>
									subnet
								</p>
								 </td><td align="left" valign="top" headers="idm140301164923936"> <p>
									The neutron subnet where you want to create the port for this NIC.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164925024"> <p>
									port
								</p>
								 </td><td align="left" valign="top" headers="idm140301164923936"> <p>
									Existing Neutron port to use instead of creating a new port.
								</p>
								 </td></tr></tbody></table></div></div><h5 id="example_syntax_4">Example syntax</h5><p>
					In the following example, all three Controller servers are on nodes allocated automatically from the pool of available nodes. All Controller servers in this environment use a default custom image <code class="literal">overcloud-full-custom</code> and have specific networking requirements:
				</p><pre class="screen">- name: Controller
  count: 3
  defaults:
    image:
      href: file:///var/lib/ironic/images/overcloud-full-custom.qcow2
      nics:
        network: custom-network
        subnet: custom-subnet</pre></section></section><section class="chapter" id="configuring-a-basic-overcloud-with-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">Chapter 9. Configuring a basic overcloud with pre-provisioned nodes</h2></div></div></div><p>
				This chapter contains basic configuration procedures that you can use to configure a Red Hat OpenStack Platform (RHOSP) environment with pre-provisioned nodes. This scenario differs from the standard overcloud creation scenarios in several ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						You can provision nodes with an external tool and let the director control the overcloud configuration only.
					</li><li class="listitem">
						You can use nodes without relying on the director provisioning methods. This is useful if you want to create an overcloud without power management control, or use networks with DHCP/PXE boot restrictions.
					</li><li class="listitem">
						The director does not use OpenStack Compute (nova), OpenStack Bare Metal (ironic), or OpenStack Image (glance) to manage nodes.
					</li><li class="listitem">
						Pre-provisioned nodes can use a custom partitioning layout that does not rely on the QCOW2 overcloud-full image.
					</li></ul></div><p>
				This scenario includes only basic configuration with no custom features. However, you can add advanced configuration options to this basic overcloud and customize it to your specifications with the instructions in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index">Advanced Overcloud Customization</a> guide.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					You cannot combine pre-provisioned nodes with director-provisioned nodes.
				</p></div></div><section class="section" id="pre-provisioned-node-requirements"><div class="titlepage"><div><div><h2 class="title">9.1. Pre-provisioned node requirements</h2></div></div></div><p>
					Before you begin deploying an overcloud with pre-provisioned nodes, ensure that the following configuration is present in your environment:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The director node that you created in <a class="xref" href="index.html#installing-the-undercloud" title="Chapter 4. Installing director">Chapter 4, <em>Installing director</em></a>.
						</li><li class="listitem">
							A set of bare metal machines for your nodes. The number of nodes required depends on the type of overcloud you intend to create. These machines must comply with the requirements set for each node type. These nodes require Red Hat Enterprise Linux 8.2 or later installed as the host operating system. Red Hat recommends using the latest version available.
						</li><li class="listitem">
							One network connection for managing the pre-provisioned nodes. This scenario requires uninterrupted SSH access to the nodes for orchestration agent configuration.
						</li><li class="listitem"><p class="simpara">
							One network connection for the Control Plane network. There are two main scenarios for this network:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p class="simpara">
									Using the Provisioning Network as the Control Plane, which is the default scenario. This network is usually a layer-3 (L3) routable network connection from the pre-provisioned nodes to director. The examples for this scenario use following IP address assignments:
								</p><div class="table" id="idm140301166270928"><p class="title"><strong>Table 9.1. Provisioning Network IP assignments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301168419536" scope="col">Node name</th><th align="left" valign="top" id="idm140301168418448" scope="col">IP address</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301168419536"> <p>
													Director
												</p>
												 </td><td align="left" valign="top" headers="idm140301168418448"> <p>
													192.168.24.1
												</p>
												 </td></tr><tr><td align="left" valign="top" headers="idm140301168419536"> <p>
													Controller 0
												</p>
												 </td><td align="left" valign="top" headers="idm140301168418448"> <p>
													192.168.24.2
												</p>
												 </td></tr><tr><td align="left" valign="top" headers="idm140301168419536"> <p>
													Compute 0
												</p>
												 </td><td align="left" valign="top" headers="idm140301168418448"> <p>
													192.168.24.3
												</p>
												 </td></tr></tbody></table></div></div></li><li class="listitem">
									Using a separate network. In situations where the director’s Provisioning network is a private non-routable network, you can define IP addresses for nodes from any subnet and communicate with director over the Public API endpoint. For more information about the requirements for this scenario, see <a class="xref" href="index.html#using-a-separate-network-for-pre-provisioned-nodes" title="9.6. Using a separate network for pre-provisioned nodes">Section 9.6, “Using a separate network for pre-provisioned nodes”</a>.
								</li></ul></div></li><li class="listitem">
							All other network types in this example also use the Control Plane network for OpenStack services. However, you can create additional networks for other network traffic types.
						</li><li class="listitem">
							If any nodes use Pacemaker resources, the service user <code class="literal">hacluster</code> and the service group <code class="literal">haclient</code> must have a UID/GID of <span class="strong strong"><strong>189</strong></span>. This is due to <a class="link" href="https://access.redhat.com/security/cve/CVE-2018-16877">CVE-2018-16877</a>. If you installed Pacemaker together with the operating system, the installation creates these IDs automatically. If the ID values are set incorrectly, follow the steps in the article <a class="link" href="https://access.redhat.com/solutions/4669581">OpenStack minor update / fast-forward upgrade can fail on the controller nodes at pacemaker step with "Could not evaluate: backup_cib"</a> to change the ID values.
						</li><li class="listitem">
							To prevent some services from binding to an incorrect IP address and causing deployment failures, make sure that the <code class="literal">/etc/hosts</code> file does not include the <code class="literal">node-name=127.0.0.1</code> mapping.
						</li></ul></div></section><section class="section" id="creating-a-user-on-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">9.2. Creating a user on pre-provisioned nodes</h2></div></div></div><p>
					When you configure an overcloud with pre-provisioned nodes, director requires SSH access to the overcloud nodes as the <code class="literal">stack</code> user. To create the <code class="literal">stack</code> user, complete the following steps.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On each overcloud node, create the <code class="literal">stack</code> user and set a password. For example, run the following commands on the Controller node:
						</p><pre class="screen">[root@controller-0 ~]# useradd stack
[root@controller-0 ~]# passwd stack  # specify a password</pre></li><li class="listitem"><p class="simpara">
							Disable password requirements for this user when using <code class="literal">sudo</code>:
						</p><pre class="screen">[root@controller-0 ~]# echo "stack ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/stack
[root@controller-0 ~]# chmod 0440 /etc/sudoers.d/stack</pre></li><li class="listitem"><p class="simpara">
							After you create and configure the <code class="literal">stack</code> user on all pre-provisioned nodes, copy the <code class="literal">stack</code> user’s public SSH key from the director node to each overcloud node. For example, to copy the director’s public SSH key to the Controller node, run the following command:
						</p><pre class="screen">[stack@director ~]$ ssh-copy-id stack@192.168.24.2</pre></li></ol></div></section><section class="section" id="registering-the-operating-system-for-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">9.3. Registering the operating system for pre-provisioned nodes</h2></div></div></div><p>
					Each node requires access to a Red Hat subscription. Complete the following steps on each node to register your nodes with the Red Hat Content Delivery Network.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the registration command and enter your Customer Portal user name and password when prompted:
						</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager register</pre></li><li class="listitem"><p class="simpara">
							Find the entitlement pool for the Red Hat OpenStack Platform 16:
						</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager list --available --all --matches="Red Hat OpenStack"</pre></li><li class="listitem"><p class="simpara">
							Use the pool ID located in the previous step to attach the Red Hat OpenStack Platform 16 entitlements:
						</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager attach --pool=pool_id</pre></li><li class="listitem"><p class="simpara">
							Disable all default repositories:
						</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager repos --disable=*</pre></li><li class="listitem"><p class="simpara">
							Enable the required Red Hat Enterprise Linux repositories.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									For x86_64 systems, run:
								</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager repos --enable=rhel-8-for-x86_64-baseos-eus-rpms --enable=rhel-8-for-x86_64-appstream-eus-rpms --enable=rhel-8-for-x86_64-highavailability-eus-rpms --enable=ansible-2.9-for-rhel-8-x86_64-rpms --enable=openstack-16.1-for-rhel-8-x86_64-rpms --enable=rhceph-4-osd-for-rhel-8-x86_64-rpms --enable=rhceph-4-mon-for-rhel-8-x86_64-rpms --enable=rhceph-4-tools-for-rhel-8-x86_64-rpms --enable=advanced-virt-for-rhel-8-x86_64-rpms --enable=fast-datapath-for-rhel-8-x86_64-rpms</pre></li><li class="listitem"><p class="simpara">
									For POWER systems, run:
								</p><pre class="screen">[root@controller-0 ~]# sudo subscription-manager repos --enable=rhel-8-for-ppc64le-baseos-rpms --enable=rhel-8-for-ppc64le-appstream-rpms --enable=rhel-8-for-ppc64le-highavailability-rpms --enable=ansible-2.8-for-rhel-8-ppc64le-rpms --enable=openstack-16-for-rhel-8-ppc64le-rpms --enable=advanced-virt-for-rhel-8-ppc64le-rpms</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Enable only the repositories listed. Additional repositories can cause package and software conflicts. Do not enable any additional repositories.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Update your system to ensure you have the latest base system packages:
						</p><pre class="screen">[root@controller-0 ~]# sudo dnf update -y
[root@controller-0 ~]# sudo reboot</pre></li></ol></div><p>
					The node is now ready to use for your overcloud.
				</p></section><section class="section" id="sect-Configuring_SSLTLS_on_Nodes"><div class="titlepage"><div><div><h2 class="title">9.4. Configuring SSL/TLS access to director</h2></div></div></div><p>
					If the director uses SSL/TLS, the pre-provisioned nodes require the certificate authority file used to sign the director’s SSL/TLS certificates. If you use your own certificate authority, perform the following actions on each overcloud node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Copy the certificate authority file to the <code class="literal">/etc/pki/ca-trust/source/anchors/</code> directory on each pre-provisioned node.
						</li><li class="listitem"><p class="simpara">
							Run the following command on each overcloud node:
						</p><pre class="screen">[root@controller-0 ~]#  sudo update-ca-trust extract</pre></li></ol></div><p>
					These steps ensure that the overcloud nodes can access the director’s Public API over SSL/TLS.
				</p></section><section class="section" id="configuring-networking-for-the-control-plane"><div class="titlepage"><div><div><h2 class="title">9.5. Configuring networking for the control plane</h2></div></div></div><p>
					The pre-provisioned overcloud nodes obtain metadata from director using standard HTTP requests. This means all overcloud nodes require L3 access to either:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The director Control Plane network, which is the subnet that you define with the <code class="literal">network_cidr</code> parameter in your <code class="literal">undercloud.conf</code> file. The overcloud nodes require either direct access to this subnet or routable access to the subnet.
						</li><li class="listitem">
							The director Public API endpoint, that you specify with the <code class="literal">undercloud_public_host</code> parameter in your <code class="literal">undercloud.conf</code> file. This option is available if you do not have an L3 route to the Control Plane or if you want to use SSL/TLS communication. For more information about configuring your overcloud nodes to use the Public API endpoint, see <a class="xref" href="index.html#using-a-separate-network-for-pre-provisioned-nodes" title="9.6. Using a separate network for pre-provisioned nodes">Section 9.6, “Using a separate network for pre-provisioned nodes”</a>.
						</li></ul></div><p>
					Director uses the Control Plane network to manage and configure a standard overcloud. For an overcloud with pre-provisioned nodes, your network configuration might require some modification to accommodate communication between the director and the pre-provisioned nodes.
				</p><div class="formalpara"><p class="title"><strong>Using network isolation</strong></p><p>
						You can use network isolation to group services to use specific networks, including the Control Plane. There are multiple network isolation strategies in the the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/">Advanced Overcloud Customization</a> guide. You can also define specific IP addresses for nodes on the Control Plane. For more information about isolating networks and creating predictable node placement strategies, see the following sections in the <span class="emphasis"><em>Advanced Overcloud Customizations</em></span> guide:
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/basic-network-isolation">"Basic network isolation"</a>
						</li><li class="listitem">
							<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-controlling_node_placement">"Controlling Node Placement"</a>
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you use network isolation, ensure that your NIC templates do not include the NIC used for undercloud access. These templates can reconfigure the NIC, which introduces connectivity and configuration problems during deployment.
					</p></div></div><div class="formalpara"><p class="title"><strong>Assigning IP addresses</strong></p><p>
						If you do not use network isolation, you can use a single Control Plane network to manage all services. This requires manual configuration of the Control Plane NIC on each node to use an IP address within the Control Plane network range. If you are using the director Provisioning network as the Control Plane, ensure that the overcloud IP addresses that you choose are outside of the DHCP ranges for both provisioning (<code class="literal">dhcp_start</code> and <code class="literal">dhcp_end</code>) and introspection (<code class="literal">inspection_iprange</code>).
					</p></div><p>
					During standard overcloud creation, director creates OpenStack Networking (neutron) ports and automatically assigns IP addresses to the overcloud nodes on the Provisioning / Control Plane network. However, this can cause director to assign different IP addresses to the ones that you configure manually for each node. In this situation, use a predictable IP address strategy to force director to use the pre-provisioned IP assignments on the Control Plane.
				</p><p>
					For example, you can use an environment file <code class="literal">ctlplane-assignments.yaml</code> with the following IP assignments to implement a predictable IP strategy:
				</p><pre class="screen">resource_registry:
  OS::TripleO::DeployedServer::ControlPlanePort: /usr/share/openstack-tripleo-heat-templates/deployed-server/deployed-neutron-port.yaml

parameter_defaults:
  DeployedServerPortMap:
    controller-0-ctlplane:
      fixed_ips:
        - ip_address: 192.168.24.2
      subnets:
        - cidr: 192.168.24.0/24
      network:
        tags:
          192.168.24.0/24
    compute-0-ctlplane:
      fixed_ips:
        - ip_address: 192.168.24.3
      subnets:
        - cidr: 192.168.24.0/24
      network:
        tags:
          - 192.168.24.0/24</pre><p>
					In this example, the <code class="literal">OS::TripleO::DeployedServer::ControlPlanePort</code> resource passes a set of parameters to director and defines the IP assignments of your pre-provisioned nodes. Use the <code class="literal">DeployedServerPortMap</code> parameter to define the IP addresses and subnet CIDRs that correspond to each overcloud node. The mapping defines the following attributes:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The name of the assignment, which follows the format <code class="literal">&lt;node_hostname&gt;-&lt;network&gt;</code> where the <code class="literal">&lt;node_hostname&gt;</code> value matches the short host name for the node, and <code class="literal">&lt;network&gt;</code> matches the lowercase name of the network. For example: <code class="literal">controller-0-ctlplane</code> for <code class="literal">controller-0.example.com</code> and <code class="literal">compute-0-ctlplane</code> for <code class="literal">compute-0.example.com</code>.
						</li><li class="listitem"><p class="simpara">
							The IP assignments, which use the following parameter patterns:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">fixed_ips/ip_address</code> - Defines the fixed IP addresses for the control plane. Use multiple <code class="literal">ip_address</code> parameters in a list to define multiple IP addresses.
								</li><li class="listitem">
									<code class="literal">subnets/cidr</code> - Defines the CIDR value for the subnet.
								</li></ul></div></li></ol></div><p>
					A later section in this chapter uses the resulting environment file (<code class="literal">ctlplane-assignments.yaml</code>) as part of the <code class="literal">openstack overcloud deploy</code> command.
				</p></section><section class="section" id="using-a-separate-network-for-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">9.6. Using a separate network for pre-provisioned nodes</h2></div></div></div><p>
					By default, director uses the Provisioning network as the overcloud Control Plane. However, if this network is isolated and non-routable, nodes cannot communicate with the director Internal API during configuration. In this situation, you might need to define a separate network for the nodes and configure them to communicate with the director over the Public API.
				</p><p>
					There are several requirements for this scenario:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The overcloud nodes must accommodate the basic network configuration from <a class="xref" href="index.html#configuring-networking-for-the-control-plane" title="9.5. Configuring networking for the control plane">Section 9.5, “Configuring networking for the control plane”</a>.
						</li><li class="listitem">
							You must enable SSL/TLS on the director for Public API endpoint usage. For more information, see <a class="xref" href="index.html#director-configuration-parameters" title="4.2. Director configuration parameters">Section 4.2, “Director configuration parameters”</a> and <a class="xref" href="index.html#configuring-custom-ssl-tls-certificates" title="Chapter 19. Configuring custom SSL/TLS certificates">Chapter 19, <em>Configuring custom SSL/TLS certificates</em></a>.
						</li><li class="listitem">
							You must define an accessible fully qualified domain name (FQDN) for director. This FQDN must resolve to a routable IP address for the director. Use the <code class="literal">undercloud_public_host</code> parameter in the <code class="literal">undercloud.conf</code> file to set this FQDN.
						</li></ul></div><p>
					The examples in this section use IP address assignments that differ from the main scenario:
				</p><div class="table" id="idm140301186845552"><p class="title"><strong>Table 9.2. Provisioning network IP assignments</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301164302416" scope="col">Node Name</th><th align="left" valign="top" id="idm140301164301328" scope="col">IP address or FQDN</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301164302416"> <p>
									Director (Internal API)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164301328"> <p>
									192.168.24.1 (Provisioning Network and Control Plane)
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164302416"> <p>
									Director (Public API)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164301328"> <p>
									10.1.1.1 / director.example.com
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164302416"> <p>
									Overcloud Virtual IP
								</p>
								 </td><td align="left" valign="top" headers="idm140301164301328"> <p>
									192.168.100.1
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164302416"> <p>
									Controller 0
								</p>
								 </td><td align="left" valign="top" headers="idm140301164301328"> <p>
									192.168.100.2
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164302416"> <p>
									Compute 0
								</p>
								 </td><td align="left" valign="top" headers="idm140301164301328"> <p>
									192.168.100.3
								</p>
								 </td></tr></tbody></table></div></div><p>
					The following sections provide additional configuration for situations that require a separate network for overcloud nodes.
				</p><div class="formalpara"><p class="title"><strong>IP address assignments</strong></p><p>
						The method for IP assignments is similar to <a class="xref" href="index.html#configuring-networking-for-the-control-plane" title="9.5. Configuring networking for the control plane">Section 9.5, “Configuring networking for the control plane”</a>. However, since the Control Plane is not routable from the deployed servers, you must use the <code class="literal">DeployedServerPortMap</code> parameter to assign IP addresses from your chosen overcloud node subnet, including the virtual IP address to access the Control Plane. The following example is a modified version of the <code class="literal">ctlplane-assignments.yaml</code> environment file from <a class="xref" href="index.html#configuring-networking-for-the-control-plane" title="9.5. Configuring networking for the control plane">Section 9.5, “Configuring networking for the control plane”</a> that accommodates this network architecture:
					</p></div><pre class="screen">resource_registry:
  OS::TripleO::DeployedServer::ControlPlanePort: /usr/share/openstack-tripleo-heat-templates/deployed-server/deployed-neutron-port.yaml
  OS::TripleO::Network::Ports::ControlPlaneVipPort: /usr/share/openstack-tripleo-heat-templates/deployed-server/deployed-neutron-port.yaml
  OS::TripleO::Network::Ports::RedisVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/noop.yaml <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>

parameter_defaults:
  NeutronPublicInterface: eth1
  EC2MetadataIp: 192.168.100.1 <span id="CO1-2"><!--Empty--></span><span class="callout">2</span>
  ControlPlaneDefaultRoute: 192.168.100.1
  DeployedServerPortMap:
    control_virtual_ip:
      fixed_ips:
        - ip_address: 192.168.100.1
      subnets:
        - cidr: 24
    controller-0-ctlplane:
      fixed_ips:
        - ip_address: 192.168.100.2
      subnets:
        - cidr: 24
    compute-0-ctlplane:
      fixed_ips:
        - ip_address: 192.168.100.3
      subnets:
        - cidr: 24</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
							The <code class="literal">RedisVipPort</code> resource is mapped to <code class="literal">network/ports/noop.yaml</code>. This mapping is necessary because the default Redis VIP address comes from the Control Plane. In this situation, use a <code class="literal">noop</code> to disable this Control Plane mapping.
						</div></dd><dt><a href="index.html#CO1-2"><span class="callout">2</span></a> </dt><dd><div class="para">
							The <code class="literal">EC2MetadataIp</code> and <code class="literal">ControlPlaneDefaultRoute</code> parameters are set to the value of the Control Plane virtual IP address. The default NIC configuration templates require these parameters and you must set them to use a pingable IP address to pass the validations performed during deployment. Alternatively, customize the NIC configuration so that they do not require these parameters.
						</div></dd></dl></div></section><section class="section" id="mapping-pre-provisioned-node-hostnames"><div class="titlepage"><div><div><h2 class="title">9.7. Mapping pre-provisioned node hostnames</h2></div></div></div><p>
					When you configure pre-provisioned nodes, you must map heat-based hostnames to their actual hostnames so that <code class="literal">ansible-playbook</code> can reach a resolvable host. Use the <code class="literal">HostnameMap</code> to map these values.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an environment file, for example <code class="literal">hostname-map.yaml</code>, and include the <code class="literal">HostnameMap</code> parameter and the hostname mappings. Use the following syntax:
						</p><pre class="screen">parameter_defaults:
  HostnameMap:
    [HEAT HOSTNAME]: [ACTUAL HOSTNAME]
    [HEAT HOSTNAME]: [ACTUAL HOSTNAME]</pre><p class="simpara">
							The <code class="literal">[HEAT HOSTNAME]</code> usually conforms to the following convention: <code class="literal">[STACK NAME]-[ROLE]-[INDEX]</code>:
						</p><pre class="screen">parameter_defaults:
  HostnameMap:
    overcloud-controller-0: controller-00-rack01
    overcloud-controller-1: controller-01-rack02
    overcloud-controller-2: controller-02-rack03
    overcloud-novacompute-0: compute-00-rack01
    overcloud-novacompute-1: compute-01-rack01
    overcloud-novacompute-2: compute-02-rack01</pre></li><li class="listitem">
							Save the <code class="literal">hostname-map.yaml</code> file.
						</li></ol></div></section><section class="section" id="configuring_ceph_storage_for_pre_provisioned_nodes"><div class="titlepage"><div><div><h2 class="title">9.8. Configuring Ceph Storage for pre-provisioned nodes</h2></div></div></div><p>
					Complete the following steps on the undercloud host to configure <code class="literal">ceph-ansible</code> for nodes that are already deployed.
				</p><p>
					<span class="strong strong"><strong>Procedure</strong></span>
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On the undercloud host, create an environment variable, <code class="literal">OVERCLOUD_HOSTS</code>, and set the variable to a space-separated list of IP addresses of the overcloud hosts that you want to use as Ceph clients:
						</p><pre class="screen">export OVERCLOUD_HOSTS="192.168.1.8 192.168.1.42"</pre></li><li class="listitem"><p class="simpara">
							The default overcloud plan name is <code class="literal">overcloud</code>. If you use a different name, create an environment variable <code class="literal">OVERCLOUD_PLAN</code> to store your custom name:
						</p><pre class="screen">export OVERCLOUD_PLAN="&lt;custom-stack-name&gt;"</pre><p class="simpara">
							Replace <code class="literal">&lt;custom-stack-name&gt;</code> with the name of your stack.
						</p></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">enable-ssh-admin.sh</code> script to configure a user on the overcloud nodes that Ansible can use to configure Ceph clients:
						</p><pre class="screen">bash /usr/share/openstack-tripleo-heat-templates/deployed-server/scripts/enable-ssh-admin.sh</pre></li></ol></div><p>
					When you run the <code class="literal">openstack overcloud deploy</code> command, Ansible configures the hosts that you define in the <code class="literal">OVERCLOUD_HOSTS</code> variable as Ceph clients.
				</p></section><section class="section" id="creating-the-overcloud-with-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">9.9. Creating the overcloud with pre-provisioned nodes</h2></div></div></div><p>
					The overcloud deployment uses the standard CLI methods from <a class="xref" href="index.html#deployment-command" title="7.13. Deployment command">Section 7.13, “Deployment command”</a>. For pre-provisioned nodes, the deployment command requires some additional options and environment files from the core heat template collection:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">--disable-validations</code> - Use this option to disable basic CLI validations for services not used with pre-provisioned infrastructure. If you do not disable these validations, the deployment fails.
						</li><li class="listitem">
							<code class="literal">environments/deployed-server-environment.yaml</code> - Include this environment file to create and configure the pre-provisioned infrastructure. This environment file substitutes the <code class="literal">OS::Nova::Server</code> resources with <code class="literal">OS::Heat::DeployedServer</code> resources.
						</li></ul></div><p>
					The following command is an example overcloud deployment command with the environment files specific to the pre-provisioned architecture:
				</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud deploy \
  [other arguments] \
  --disable-validations \
  -e /usr/share/openstack-tripleo-heat-templates/environments/deployed-server-environment.yaml \
  -e /home/stack/templates/hostname-map.yaml \
  --overcloud-ssh-user stack \
  --overcloud-ssh-key ~/.ssh/id_rsa \
  [OTHER OPTIONS]</pre><p>
					The <code class="literal">--overcloud-ssh-user</code> and <code class="literal">--overcloud-ssh-key</code> options are used to SSH into each overcloud node during the configuration stage, create an initial <code class="literal">tripleo-admin</code> user, and inject an SSH key into <code class="literal">/home/tripleo-admin/.ssh/authorized_keys</code>. To inject the SSH key, specify the credentials for the initial SSH connection with <code class="literal">--overcloud-ssh-user</code> and <code class="literal">--overcloud-ssh-key</code> (defaults to <code class="literal">~/.ssh/id_rsa</code>). To limit exposure to the private key that you specify with the <code class="literal">--overcloud-ssh-key</code> option, director never passes this key to any API service, such as heat or the Workflow service (mistral), and only the director <code class="literal">openstack overcloud deploy</code> command uses this key to enable access for the <code class="literal">tripleo-admin</code> user.
				</p></section><section class="section" id="overcloud-deployment-output-preprovisioned"><div class="titlepage"><div><div><h2 class="title">9.10. Overcloud deployment output</h2></div></div></div><p>
					When the overcloud creation completes, director provides a recap of the Ansible plays that were executed to configure the overcloud:
				</p><pre class="screen">PLAY RECAP *************************************************************
overcloud-compute-0     : ok=160  changed=67   unreachable=0    failed=0
overcloud-controller-0  : ok=210  changed=93   unreachable=0    failed=0
undercloud              : ok=10   changed=7    unreachable=0    failed=0

Tuesday 15 October 2018  18:30:57 +1000 (0:00:00.107) 1:06:37.514 ******
========================================================================</pre><p>
					Director also provides details to access your overcloud.
				</p><pre class="screen">Ansible passed.
Overcloud configuration completed.
Overcloud Endpoint: http://192.168.24.113:5000
Overcloud Horizon Dashboard URL: http://192.168.24.113:80/dashboard
Overcloud rc file: /home/stack/overcloudrc
Overcloud Deployed</pre></section><section class="section" id="accessing-the-overcloud-preprovisioned"><div class="titlepage"><div><div><h2 class="title">9.11. Accessing the overcloud</h2></div></div></div><p>
					The director generates a script to configure and help authenticate interactions with your overcloud from the undercloud. The director saves this file, <code class="literal">overcloudrc</code>, in the home directory of the <code class="literal">stack</code> user. Run the following command to use this file:
				</p><pre class="screen">(undercloud) $ source ~/overcloudrc</pre><p>
					This command loads the environment variables that are necessary to interact with your overcloud from the undercloud CLI. The command prompt changes to indicate this:
				</p><pre class="screen">(overcloud) $</pre><p>
					To return to interacting with the undercloud, run the following command:
				</p><pre class="screen">(overcloud) $ source ~/stackrc
(undercloud) $</pre><p>
					Each node in the overcloud also contains a <code class="literal">heat-admin</code> user. The <code class="literal">stack</code> user has SSH access to this user on each node. To access a node over SSH, find the IP address of the node that you want to access:
				</p><pre class="screen">(undercloud) $ openstack server list</pre><p>
					Then connect to the node using the <code class="literal">heat-admin</code> user and the IP address of the node:
				</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.24.23</pre></section><section class="section" id="scaling-pre-provisioned-nodes"><div class="titlepage"><div><div><h2 class="title">9.12. Scaling pre-provisioned nodes</h2></div></div></div><p>
					The process for scaling pre-provisioned nodes is similar to the standard scaling procedures in <a class="xref" href="index.html#scaling-overcloud-nodes" title="Chapter 16. Scaling overcloud nodes">Chapter 16, <em>Scaling overcloud nodes</em></a>. However, the process to add new pre-provisioned nodes differs because pre-provisioned nodes do not use the standard registration and management process from OpenStack Bare Metal (ironic) and OpenStack Compute (nova).
				</p><div class="formalpara"><p class="title"><strong>Scaling up pre-provisioned nodes</strong></p><p>
						When scaling up the overcloud with pre-provisioned nodes, you must configure the orchestration agent on each node to correspond to the director node count.
					</p></div><p>
					Perform the following actions to scale up overcloud nodes:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Prepare the new pre-provisioned nodes according to <a class="xref" href="index.html#pre-provisioned-node-requirements" title="9.1. Pre-provisioned node requirements">Section 9.1, “Pre-provisioned node requirements”</a>.
						</li><li class="listitem">
							Scale up the nodes. For more information, see <a class="xref" href="index.html#scaling-overcloud-nodes" title="Chapter 16. Scaling overcloud nodes">Chapter 16, <em>Scaling overcloud nodes</em></a>.
						</li><li class="listitem">
							After you execute the deployment command, wait until the director creates the new node resources and launches the configuration.
						</li></ol></div><div class="formalpara"><p class="title"><strong>Scaling down pre-provisioned nodes</strong></p><p>
						When scaling down the overcloud with pre-provisioned nodes, follow the scale down instructions in <a class="xref" href="index.html#scaling-overcloud-nodes" title="Chapter 16. Scaling overcloud nodes">Chapter 16, <em>Scaling overcloud nodes</em></a>.
					</p></div><p>
					In most scaling operations, you must obtain the UUID value of the node that you want to remove and pass this value to the <code class="literal">openstack overcloud node delete</code> command. To obtain this UUID, list the resources for the specific role:
				</p><pre class="screen">$ openstack stack resource list overcloud -c physical_resource_id -c stack_name -n5 --filter type=OS::TripleO::&lt;RoleName&gt;Server</pre><p>
					Replace <code class="literal">&lt;RoleName&gt;</code> with the name of the role that you want to scale down. For example, for the <code class="literal">ComputeDeployedServer</code> role, run the following command:
				</p><pre class="screen">$ openstack stack resource list overcloud -c physical_resource_id -c stack_name -n5 --filter type=OS::TripleO::ComputeDeployedServerServer</pre><p>
					Use the <code class="literal">stack_name</code> column in the command output to identify the UUID associated with each node. The <code class="literal">stack_name</code> includes the integer value of the index of the node in the heat resource group:
				</p><pre class="screen">+------------------------------------+----------------------------------+
| physical_resource_id               | stack_name                       |
+------------------------------------+----------------------------------+
| 294d4e4d-66a6-4e4e-9a8b-           | overcloud-ComputeDeployedServer- |
| 03ec80beda41                       | no7yfgnh3z7e-1-ytfqdeclwvcg      |
| d8de016d-                          | overcloud-ComputeDeployedServer- |
| 8ff9-4f29-bc63-21884619abe5        | no7yfgnh3z7e-0-p4vb3meacxwn      |
| 8c59f7b1-2675-42a9-ae2c-           | overcloud-ComputeDeployedServer- |
| 2de4a066f2a9                       | no7yfgnh3z7e-2-mmmaayxqnf3o      |
+------------------------------------+----------------------------------+</pre><p>
					The indices 0, 1, or 2 in the <code class="literal">stack_name</code> column correspond to the node order in the heat resource group. Pass the corresponding UUID value from the <code class="literal">physical_resource_id</code> column to <code class="literal">openstack overcloud node delete</code> command.
				</p><p>
					After you remove overcloud nodes from the stack, power off these nodes. In a standard deployment, the bare metal services on the director control this function. However, with pre-provisioned nodes, you must either manually shut down these nodes or use the power management control for each physical system. If you do not power off the nodes after removing them from the stack, they might remain operational and reconnect as part of the overcloud environment.
				</p><p>
					After you power off the removed nodes, reprovision them to a base operating system configuration so that they do not unintentionally join the overcloud in the future
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Do not attempt to reuse nodes previously removed from the overcloud without first reprovisioning them with a fresh base operating system. The scale down process only removes the node from the overcloud stack and does not uninstall any packages.
					</p></div></div></section><section class="section" id="sect-Deleting_a_Pre-Provisioned_Overcloud"><div class="titlepage"><div><div><h2 class="title">9.13. Removing a pre-provisioned overcloud</h2></div></div></div><p>
					To remove an entire overcloud that uses pre-provisioned nodes, see <a class="xref" href="index.html#removing-the-overcloud" title="12.5. Removing the overcloud">Section 12.5, “Removing the overcloud”</a> for the standard overcloud remove procedure. After you remove the overcloud, power off all nodes and reprovision them to a base operating system configuration.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Do not attempt to reuse nodes previously removed from the overcloud without first reprovisioning them with a fresh base operating system. The removal process only deletes the overcloud stack and does not uninstall any packages.
					</p></div></div></section><section class="section" id="sect-Completing_the_Overcloud_Creation_Pre_Provisioned"><div class="titlepage"><div><div><h2 class="title">9.14. Next steps</h2></div></div></div><p>
					This concludes the creation of the overcloud using pre-provisioned nodes. For post-creation functions, see <a class="xref" href="index.html#performing-overcloud-post-installation-tasks" title="Chapter 11. Performing overcloud post-installation tasks">Chapter 11, <em>Performing overcloud post-installation tasks</em></a>.
				</p></section></section><section class="chapter" id="deploying-multiple-overclouds"><div class="titlepage"><div><div><h2 class="title">Chapter 10. Deploying multiple overclouds</h2></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p></div></div><p>
				You can use a single undercloud node to deploy and manage multiple overclouds. Each overcloud is a unique heat stack that does not share stack resources. This can be useful for environments where a 1:1 ratio of underclouds to overclouds creates an unmanageable amount of overhead. For example, Edge, multi-site, and multi-product environments.
			</p><p>
				The overcloud environments in the multi-overcloud scenario are completely separate, and you can use the <code class="literal">source</code> command to switch between the environments. If you use Ironic for bare metal provisioning, all overclouds must be on the same provisioning network. If it is not possible to use the same provisioning network, you can use the deployed servers method to deploy multiple overclouds with routed networks. In this scenario, you must ensure that the value in the <code class="literal">HostnameMap</code> parameter matches the stack name for each overcloud.
			</p><p>
				Use the following workflow to understand the basic process:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Deploying the undercloud</span></dt><dd>
							Deploy the undercloud as normal. For more information, see <a class="xref" href="index.html#director_installation_and_configuration" title="Part I. Director installation and configuration">Part I, “Director installation and configuration”</a>.
						</dd><dt><span class="term">Deploying the first overcloud</span></dt><dd>
							Deploy the first overcloud as normal. For more information, see <a class="xref" href="index.html#basic_overcloud_deployment" title="Part II. Basic overcloud deployment">Part II, “Basic overcloud deployment”</a>.
						</dd><dt><span class="term">Deploying additional overclouds</span></dt><dd>
							Create a new set of environment files for the new overcloud. Run the deployment command, and specify the core heat templates together with the new configuration files and a new <code class="literal">stack</code> name.
						</dd></dl></div><section class="section" id="deploying-additional-overclouds"><div class="titlepage"><div><div><h2 class="title">10.1. Deploying additional overclouds</h2></div></div></div><p>
					In this example, <code class="literal">overcloud-one</code> is the existing overcloud. Complete the following steps to deploy a new overcloud <code class="literal">overcloud-two</code>.
				</p><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						Before you begin to deploy additional overclouds, ensure that your environment contains the following configurations:
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Successful undercloud and overcloud deployments.
						</li><li class="listitem">
							Nodes available for your additional overcloud.
						</li><li class="listitem">
							Custom networks for additional overclouds so that each overcloud has a unique network in the resulting stack.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new directory for the additional overcloud that you want to deploy:
						</p><pre class="screen">$ mkdir ~/overcloud-two</pre></li><li class="listitem"><p class="simpara">
							In the new directory, create new environment files specific to the requirements of the additional overcloud, and copy any relevant environment files from the existing overcloud:
						</p><pre class="screen">$ cp network-data.yaml ~/overcloud-two/network-data.yaml
$ cp network-environment.yaml ~/overcloud-two/network-environment.yaml</pre></li><li class="listitem"><p class="simpara">
							Modify the environment files according to the specification of the new overcloud. For example, the existing overcloud has the name <code class="literal">overcloud-one</code> and uses the VLANs that you define in the <code class="literal">network-data.yaml</code> environment file:
						</p><pre class="screen">- name: InternalApi
  name_lower: internal_api_cloud_1
  service_net_map_replace: internal_api
  vip: true
  vlan: 20
  ip_subnet: '172.17.0.0/24'
  allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]
  ipv6_subnet: 'fd00:fd00:fd00:2000::/64'
  ipv6_allocation_pools: [{'start': 'fd00:fd00:fd00:2000::10', 'end': 'fd00:fd00:fd00:2000:ffff:ffff:ffff:fffe'}]
  mtu: 1500
- name: Storage
  ...</pre><p class="simpara">
							The new overcloud has the name <code class="literal">overcloud-two</code> and uses different VLANs. Edit the <code class="literal">~/overcloud-two/network-data.yaml</code> environment file and include the new VLAN IDs for each subnet. You must also define a unique <code class="literal">name_lower</code> value, and set the <code class="literal">service_net_map_replace</code> attribute to the name of the network that you want to replace:
						</p><pre class="screen">- name: InternalApi
  name_lower: internal_api_cloud_2
  service_net_map_replace: internal_api
  vip: true
  vlan: 21
  ip_subnet: '172.21.0.0/24'
  allocation_pools: [{'start': '172.21.0.4', 'end': '172.21.0.250'}]
  ipv6_subnet: 'fd00:fd00:fd00:2001::/64'
  ipv6_allocation_pools: [{'start': 'fd00:fd00:fd00:2001::10', 'end': 'fd00:fd00:fd00:2001:ffff:ffff:ffff:fffe'}]
  mtu: 1500
- name: Storage
  ...</pre></li><li class="listitem"><p class="simpara">
							Modify the following parameters in the <code class="literal">~/overcloud-two/network-environment.yaml</code> file:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Enter a unique value in the <code class="literal">{'provider:physical_network'}</code> attribute of the <code class="literal">ExternalNetValueSpecs</code> parameter so that <code class="literal">overcloud-two</code> has a distinct external network, and define the network type with the <code class="literal">'provider:network_type'</code> attribute.
								</li><li class="listitem">
									Set the <code class="literal">ExternalInterfaceDefaultRoute</code> parameter to the IP address of the gateway for the external network so that the overcloud has external access.
								</li><li class="listitem"><p class="simpara">
									Set the <code class="literal">DnsServers</code> parameter to the IP address of your DNS server so that the overcloud can reach the DNS server.
								</p><pre class="screen">parameter_defaults:
  ...
  ExternalNetValueSpecs: {'provider:physical_network': 'external_2', 'provider:network_type': 'flat'}
  ExternalInterfaceDefaultRoute: 10.0.10.1
  DnsServers:
    - 10.0.10.2
  ...</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack overcloud deploy</code> command. Specify the core heat template collection with the <code class="literal">--templates</code> option, a new <code class="literal">stack</code> name with the <code class="literal">--stack</code> option, and any new environment files from the <code class="literal">~/overcloud-two</code> directory:
						</p><pre class="screen">$ openstack overcloud deploy --templates \
    --stack overcloud-two \
    ...
    -n ~/overcloud-two/network-data.yaml \
    -e ~/overcloud-two/network-environment.yaml \
    -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
    -e /usr/share/openstack-tripleo-heat-templates/environments/net-single-nic-with-vlans.yaml \
    ...</pre></li></ol></div><p>
					Each overcloud has a unique credential file. In this example, the deployment process creates <code class="literal">overcloud-onerc</code> for <code class="literal">overcloud-one</code>, and <code class="literal">overcloud-tworc</code> for <code class="literal">overcloud-two</code>. To interact with either overcloud, you must source the appropriate credential file. For example, to source the credential for the first overcloud, run the following command:
				</p><pre class="screen">$ source overcloud-onerc</pre></section><section class="section" id="managing-multiple-overclouds"><div class="titlepage"><div><div><h2 class="title">10.2. Managing multiple overclouds</h2></div></div></div><p>
					Each overcloud that you deploy uses the same set of core heat templates <code class="literal">/usr/share/openstack-tripleo-heat-templates</code>. Red Hat recommends that you do not modify or duplicate these templates, because using a non-standard set of core templates can introduce issues with updates and upgrades.
				</p><p>
					Instead, for ease of management when you deploy or maintain multiple overclouds, create separate directories of environment files specific to each cloud. When you run the deploy command for each cloud, include the core heat templates together with the cloud-specific environment files that you create separately. For example, create the following directories for the undercloud and two overclouds:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">~stack/undercloud</code></span></dt><dd>
								Contains the environment files specific to the undercloud.
							</dd><dt><span class="term"><code class="literal">~stack/overcloud-one</code></span></dt><dd>
								Contains the environment files specific to the first overcloud.
							</dd><dt><span class="term"><code class="literal">~stack/overcloud-two</code></span></dt><dd>
								Contains the environment files specific to the second overcloud.
							</dd></dl></div><p>
					When you deploy or redeploy <code class="literal">overcloud-one</code> or <code class="literal">overcloud-two</code>, include the core heat templates in the deploy command with the <code class="literal">--templates</code> option, and then specify any additional environment files from the cloud-specific environment file directories.
				</p><p>
					Alternatively, create a repository in a version control system and use branches for each deployment. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#sect-Using_Customized_Overcloud_Heat_Templates">Using Customized Core Heat Templates</a> section of the <span class="emphasis"><em>Advanced Overcloud Customization guide</em></span>.
				</p><p>
					Use the following command to view a list of overcloud plans that are available:
				</p><pre class="screen">$ openstack overcloud plan list</pre><p>
					Use the following command to view a list of overclouds that are currently deployed:
				</p><pre class="screen">$ openstack stack list</pre></section></section></div><div class="part" id="post_deployment_operations"><div class="titlepage"><div><div><h1 class="title">Part III. Post deployment operations</h1></div></div></div><section class="chapter" id="performing-overcloud-post-installation-tasks"><div class="titlepage"><div><div><h2 class="title">Chapter 11. Performing overcloud post-installation tasks</h2></div></div></div><p>
				This chapter contains information about tasks to perform immediately after you create your overcloud. These tasks ensure your overcloud is ready to use.
			</p><section class="section" id="checking-overcloud-deployment-status"><div class="titlepage"><div><div><h2 class="title">11.1. Checking overcloud deployment status</h2></div></div></div><p>
					To check the deployment status of the overcloud, use the <code class="literal">openstack overcloud status</code> command. This command returns the result of all deployment steps.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the deployment status command:
						</p><pre class="screen">$ openstack overcloud status</pre><p class="simpara">
							The output of this command displays the status of the overcloud:
						</p><pre class="screen">+-----------+---------------------+---------------------+-------------------+
| Plan Name |       Created       |       Updated       | Deployment Status |
+-----------+---------------------+---------------------+-------------------+
| overcloud | 2018-05-03 21:24:50 | 2018-05-03 21:27:59 |   DEPLOY_SUCCESS  |
+-----------+---------------------+---------------------+-------------------+</pre><p class="simpara">
							If your overcloud uses a different name, use the <code class="literal">--plan</code> argument to select an overcloud with a different name:
						</p><pre class="screen">$ openstack overcloud status --plan my-deployment</pre></li></ol></div></section><section class="section" id="sect-Creating-basic-overcloud-flavors"><div class="titlepage"><div><div><h2 class="title">11.2. Creating basic overcloud flavors</h2></div></div></div><p>
					Validation steps in this guide assume that your installation contains flavors. If you have not already created at least one flavor, complete the following steps to create a basic set of default flavors that have a range of storage and processing capabilities:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack flavor create</code> command to create a flavor. Use the following options to specify the hardware requirements for each flavor:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">--disk</span></dt><dd>
										Defines the hard disk space for a virtual machine volume.
									</dd><dt><span class="term">--ram</span></dt><dd>
										Defines the RAM required for a virtual machine.
									</dd><dt><span class="term">--vcpus</span></dt><dd>
										Defines the quantity of virtual CPUs for a virtual machine.
									</dd></dl></div></li><li class="listitem"><p class="simpara">
							The following example creates the default overcloud flavors:
						</p><pre class="screen">$ openstack flavor create m1.tiny --ram 512 --disk 0 --vcpus 1
$ openstack flavor create m1.smaller --ram 1024 --disk 0 --vcpus 1
$ openstack flavor create m1.small --ram 2048 --disk 10 --vcpus 1
$ openstack flavor create m1.medium --ram 3072 --disk 10 --vcpus 2
$ openstack flavor create m1.large --ram 8192 --disk 10 --vcpus 4
$ openstack flavor create m1.xlarge --ram 8192 --disk 10 --vcpus 8</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Use <code class="literal">$ openstack flavor create --help</code> to learn more about the <code class="literal">openstack flavor create</code> command.
					</p></div></div></section><section class="section" id="creating-a-default-tenant-network"><div class="titlepage"><div><div><h2 class="title">11.3. Creating a default tenant network</h2></div></div></div><p>
					The overcloud requires a default Tenant network so that virtual machines can communicate internally.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Create the default Tenant network:
						</p><pre class="screen">(overcloud) $ openstack network create default</pre></li><li class="listitem"><p class="simpara">
							Create a subnet on the network:
						</p><pre class="screen">(overcloud) $ openstack subnet create default --network default --gateway 172.20.1.1 --subnet-range 172.20.0.0/16</pre></li><li class="listitem"><p class="simpara">
							Confirm the created network:
						</p><pre class="screen">(overcloud) $ openstack network list
+-----------------------+-------------+--------------------------------------+
| id                    | name        | subnets                              |
+-----------------------+-------------+--------------------------------------+
| 95fadaa1-5dda-4777... | default     | 7e060813-35c5-462c-a56a-1c6f8f4f332f |
+-----------------------+-------------+--------------------------------------+</pre></li></ol></div><p>
					These commands create a basic Networking service (neutron) network named <code class="literal">default</code>. The overcloud automatically assigns IP addresses from this network to virtual machines using an internal DHCP mechanism.
				</p></section><section class="section" id="creating-a-default-floating-ip-network"><div class="titlepage"><div><div><h2 class="title">11.4. Creating a default floating IP network</h2></div></div></div><p>
					To access your virtual machines from outside of the overcloud, you must configure an external network that provides floating IP addresses to your virtual machines.
				</p><p>
					This procedure contains two examples. Use the example that best suits your environment:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Native VLAN (flat network)
						</li><li class="listitem">
							Non-Native VLAN (VLAN network)
						</li></ul></div><p>
					Both of these examples involve creating a network with the name <code class="literal">public</code>. The overcloud requires this specific name for the default floating IP pool. This name is also important for the validation tests in <a class="xref" href="index.html#validating-the-overcloud" title="11.7. Validating the overcloud">Section 11.7, “Validating the overcloud”</a>.
				</p><p>
					By default, Openstack Networking (neutron) maps a physical network name called <code class="literal">datacentre</code> to the the <code class="literal">br-ex</code> bridge on your host nodes. You connect the <code class="literal">public</code> overcloud network to the physical <code class="literal">datacentre</code> and this provides a gateway through the <code class="literal">br-ex</code> bridge.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A dedicated interface or native VLAN for the floating IP network.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">public</code> network:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Create a <code class="literal">flat</code> network for a native VLAN connection:
								</p><pre class="screen">(overcloud) $ openstack network create public --external --provider-network-type flat --provider-physical-network datacentre</pre></li><li class="listitem"><p class="simpara">
									Create a <code class="literal">vlan</code> network for non-native VLAN connections:
								</p><pre class="screen">(overcloud) $ openstack network create public --external --provider-network-type vlan --provider-physical-network datacentre --provider-segment 201</pre><p class="simpara">
									Use the <code class="literal">--provider-segment</code> option to define the VLAN that you want to use. In this example, the VLAN is <code class="literal">201</code>.
								</p></li></ul></div></li><li class="listitem"><p class="simpara">
							Create a subnet with an allocation pool for floating IP addresses. In this example, the IP range is <code class="literal">10.1.1.51</code> to <code class="literal">10.1.1.250</code>:
						</p><pre class="screen">(overcloud) $ openstack subnet create public --network public --dhcp --allocation-pool start=10.1.1.51,end=10.1.1.250 --gateway 10.1.1.1 --subnet-range 10.1.1.0/24</pre><p class="simpara">
							Ensure that this range does not conflict with other IP addresses in your external network.
						</p></li></ol></div></section><section class="section" id="creating-a-default-provider-network"><div class="titlepage"><div><div><h2 class="title">11.5. Creating a default provider network</h2></div></div></div><p>
					A provider network is another type of external network connection that routes traffic from private tenant networks to external infrastructure network. The provider network is similar to a floating IP network but the provider network uses a logical router to connect private networks to the provider network.
				</p><p>
					This procedure contains two examples. Use the example that best suits your environment:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Native VLAN (flat network)
						</li><li class="listitem">
							Non-Native VLAN (VLAN network)
						</li></ul></div><p>
					By default, Openstack Networking (neutron) maps a physical network name called <code class="literal">datacentre</code> to the the <code class="literal">br-ex</code> bridge on your host nodes. You connect the <code class="literal">public</code> overcloud network to the physical <code class="literal">datacentre</code> and this provides a gateway through the <code class="literal">br-ex</code> bridge.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Create the <code class="literal">provider</code> network:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Create a <code class="literal">flat</code> network for a native VLAN connection:
								</p><pre class="screen">(overcloud) $ openstack network create provider --external --provider-network-type flat --provider-physical-network datacentre --share</pre></li><li class="listitem"><p class="simpara">
									Create a <code class="literal">vlan</code> network for non-native VLAN connections:
								</p><pre class="screen">(overcloud) $ openstack network create provider --external --provider-network-type vlan --provider-physical-network datacentre --provider-segment 201 --share</pre><p class="simpara">
									Use the <code class="literal">--provider-segment</code> option to define the VLAN that you want to use. In this example, the VLAN is <code class="literal">201</code>.
								</p></li></ul></div><p class="simpara">
							These example commands create a shared network. It is also possible to specify a tenant instead of specifying <code class="literal">--share</code> so that only the tenant has access to the new network.
						</p><p class="simpara">
							+ If you mark a provider network as external, only the operator may create ports on that network.
						</p></li><li class="listitem"><p class="simpara">
							Add a subnet to the <code class="literal">provider</code> network to provide DHCP services:
						</p><pre class="screen">(overcloud) $ openstack subnet create provider-subnet --network  provider --dhcp --allocation-pool start=10.9.101.50,end=10.9.101.100 --gateway 10.9.101.254 --subnet-range 10.9.101.0/24</pre></li><li class="listitem"><p class="simpara">
							Create a router so that other networks can route traffic through the provider network:
						</p><pre class="screen">(overcloud) $ openstack router create external</pre></li><li class="listitem"><p class="simpara">
							Set the external gateway for the router to the <code class="literal">provider</code> network:
						</p><pre class="screen">(overcloud) $ openstack router set --external-gateway provider external</pre></li><li class="listitem"><p class="simpara">
							Attach other networks to this router. For example, run the following command to attach a subnet <code class="literal">subnet1</code> to the router:
						</p><pre class="screen">(overcloud) $ openstack router add subnet external subnet1</pre><p class="simpara">
							This command adds <code class="literal">subnet1</code> to the routing table and allows traffic from virtual machines using <code class="literal">subnet1</code> to route to the provider network.
						</p></li></ol></div></section><section class="section" id="creating-additional-bridge-mappings"><div class="titlepage"><div><div><h2 class="title">11.6. Creating additional bridge mappings</h2></div></div></div><p>
					Floating IP networks can use any bridge, not just <code class="literal">br-ex</code>, provided that you complete the following prerequisite actions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Set the <code class="literal">NeutronExternalNetworkBridge</code> parameter to <code class="literal">"''"</code> in your network environment file.
						</li><li class="listitem"><p class="simpara">
							Map the additional bridge during deployment. For example, to map a new bridge called <code class="literal">br-floating</code> to the <code class="literal">floating</code> physical network, include the <code class="literal">NeutronBridgeMappings</code> parameter in an environment file:
						</p><pre class="screen">parameter_defaults:
  NeutronBridgeMappings: "datacentre:br-ex,floating:br-floating"</pre></li></ul></div><p>
					With this method, you can create separate external networks after creating the overcloud. For example, to create a floating IP network that maps to the <code class="literal">floating</code> physical network, run the following commands:
				</p><pre class="screen">$ source ~/overcloudrc
(overcloud) $ openstack network create public --external --provider-physical-network floating --provider-network-type vlan --provider-segment 105
(overcloud) $ openstack subnet create public --network public --dhcp --allocation-pool start=10.1.2.51,end=10.1.2.250 --gateway 10.1.2.1 --subnet-range 10.1.2.0/24</pre></section><section class="section" id="validating-the-overcloud"><div class="titlepage"><div><div><h2 class="title">11.7. Validating the overcloud</h2></div></div></div><p>
					The overcloud uses the OpenStack Integration Test Suite (tempest) tool set to conduct a series of integration tests. This section contains information about preparations for running the integration tests. For full instructions about how to use the OpenStack Integration Test Suite, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/openstack_integration_test_suite_guide/">OpenStack Integration Test Suite Guide</a>.
				</p><p>
					The Integration Test Suite requires a few post-installation steps to ensure successful tests.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you run this test from the undercloud, ensure that the undercloud host has access to the Internal API network on the overcloud. For example, add a temporary VLAN on the undercloud host to access the Internal API network (ID: 201) using the 172.16.0.201/24 address:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ sudo ovs-vsctl add-port br-ctlplane vlan201 tag=201 -- set interface vlan201 type=internal
(undercloud) $ sudo ip l set dev vlan201 up; sudo ip addr add 172.16.0.201/24 dev vlan201</pre></li><li class="listitem"><p class="simpara">
							Before you run the OpenStack Integration Test Suite, ensure that the <code class="literal">heat_stack_owner</code> role exists in your overcloud:
						</p><pre class="screen">$ source ~/overcloudrc
(overcloud) $ openstack role list
+----------------------------------+------------------+
| ID                               | Name             |
+----------------------------------+------------------+
| 6226a517204846d1a26d15aae1af208f | swiftoperator    |
| 7c7eb03955e545dd86bbfeb73692738b | heat_stack_owner |
+----------------------------------+------------------+</pre></li><li class="listitem"><p class="simpara">
							If the role does not exist, create it:
						</p><pre class="screen">(overcloud) $ openstack role create heat_stack_owner</pre></li><li class="listitem">
							Run the integration tests as described in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/openstack_integration_test_suite_guide/">OpenStack Integration Test Suite Guide</a>.
						</li><li class="listitem"><p class="simpara">
							After completing the validation, remove any temporary connections to the overcloud Internal API. In this example, use the following commands to remove the previously created VLAN on the undercloud:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ sudo ovs-vsctl del-port vlan201</pre></li></ol></div></section><section class="section" id="protecting-the-overcloud-from-removal"><div class="titlepage"><div><div><h2 class="title">11.8. Protecting the overcloud from removal</h2></div></div></div><p>
					Heat contains a set of default policies in code that you can override by creating the <code class="literal">/var/lib/config-data/puppet-generated/heat/etc/heat/policy.json</code> file and adding customized rules. Add the following policy to deny all users the permissions necessary to delete the overcloud.
				</p><pre class="screen">{"stacks:delete": "rule:deny_everybody"}</pre><p>
					This prevents removal of the overcloud with the <code class="literal">heat</code> client. To allow removal of the overcloud, delete the custom policy and save <code class="literal">/var/lib/config-data/puppet-generated/heat/etc/heat/policy.json</code>.
				</p></section></section><section class="chapter" id="performing-basic-overcloud-administration-tasks"><div class="titlepage"><div><div><h2 class="title">Chapter 12. Performing basic overcloud administration tasks</h2></div></div></div><p>
				This chapter contains information about basic tasks you might need to perform during the lifecycle of your overcloud.
			</p><section class="section" id="managing-containerized-services-temp"><div class="titlepage"><div><div><h2 class="title">12.1. Managing containerized services</h2></div></div></div><p>
					Red Hat OpenStack Platform (RHOSP) runs services in containers on the undercloud and overcloud nodes. In certain situations, you might need to control the individual services on a host. This section contains information about some common commands you can run on a node to manage containerized services.
				</p><div class="formalpara"><p class="title"><strong>Listing containers and images</strong></p><p>
						To list running containers, run the following command:
					</p></div><pre class="screen">$ sudo podman ps</pre><p>
					To include stopped or failed containers in the command output, add the <code class="literal">--all</code> option to the command:
				</p><pre class="screen">$ sudo podman ps --all</pre><p>
					To list container images, run the following command:
				</p><pre class="screen">$ sudo podman images</pre><div class="formalpara"><p class="title"><strong>Inspecting container properties</strong></p><p>
						To view the properties of a container or container images, use the <code class="literal">podman inspect</code> command. For example, to inspect the <code class="literal">keystone</code> container, run the following command:
					</p></div><pre class="screen">$ sudo podman inspect keystone</pre><div class="formalpara"><p class="title"><strong>Managing containers with Systemd services</strong></p><p>
						Previous versions of OpenStack Platform managed containers with Docker and its daemon. In OpenStack Platform 16, the Systemd services interface manages the lifecycle of the containers. Each container is a service and you run Systemd commands to perform specific operations for each container.
					</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						It is not recommended to use the Podman CLI to stop, start, and restart containers because Systemd applies a restart policy. Use Systemd service commands instead.
					</p></div></div><p>
					To check a container status, run the <code class="literal">systemctl status</code> command:
				</p><pre class="screen">$ sudo systemctl status tripleo_keystone
● tripleo_keystone.service - keystone container
   Loaded: loaded (/etc/systemd/system/tripleo_keystone.service; enabled; vendor preset: disabled)
   Active: active (running) since Fri 2019-02-15 23:53:18 UTC; 2 days ago
 Main PID: 29012 (podman)
   CGroup: /system.slice/tripleo_keystone.service
           └─29012 /usr/bin/podman start -a keystone</pre><p>
					To stop a container, run the <code class="literal">systemctl stop</code> command:
				</p><pre class="screen">$ sudo systemctl stop tripleo_keystone</pre><p>
					To start a container, run the <code class="literal">systemctl start</code> command:
				</p><pre class="screen">$ sudo systemctl start tripleo_keystone</pre><p>
					To restart a container, run the <code class="literal">systemctl restart</code> command:
				</p><pre class="screen">$ sudo systemctl restart tripleo_keystone</pre><p>
					Because no daemon monitors the containers status, Systemd automatically restarts most containers in these situations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Clean exit code or signal, such as running <code class="literal">podman stop</code> command.
						</li><li class="listitem">
							Unclean exit code, such as the podman container crashing after a start.
						</li><li class="listitem">
							Unclean signals.
						</li><li class="listitem">
							Timeout if the container takes more than 1m 30s to start.
						</li></ul></div><p>
					For more information about Systemd services, see the <a class="link" href="https://www.freedesktop.org/software/systemd/man/systemd.service.html"><code class="literal">systemd.service</code> documentation</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Any changes to the service configuration files within the container revert after restarting the container. This is because the container regenerates the service configuration based on files on the local file system of the node in <code class="literal">/var/lib/config-data/puppet-generated/</code>. For example, if you edit <code class="literal">/etc/keystone/keystone.conf</code> within the <code class="literal">keystone</code> container and restart the container, the container regenerates the configuration using <code class="literal">/var/lib/config-data/puppet-generated/keystone/etc/keystone/keystone.conf</code> on the local file system of the node, which overwrites any the changes that were made within the container before the restart.
					</p></div></div><div class="formalpara"><p class="title"><strong>Monitoring podman containers with Systemd timers</strong></p><p>
						The Systemd timers interface manages container health checks. Each container has a timer that runs a service unit that executes health check scripts.
					</p></div><p>
					To list all OpenStack Platform containers timers, run the <code class="literal">systemctl list-timers</code> command and limit the output to lines containing <code class="literal">tripleo</code>:
				</p><pre class="screen">$ sudo systemctl list-timers | grep tripleo
Mon 2019-02-18 20:18:30 UTC  1s left       Mon 2019-02-18 20:17:26 UTC  1min 2s ago  tripleo_nova_metadata_healthcheck.timer            tripleo_nova_metadata_healthcheck.service
Mon 2019-02-18 20:18:33 UTC  4s left       Mon 2019-02-18 20:17:03 UTC  1min 25s ago tripleo_mistral_engine_healthcheck.timer           tripleo_mistral_engine_healthcheck.service
Mon 2019-02-18 20:18:34 UTC  5s left       Mon 2019-02-18 20:17:23 UTC  1min 5s ago  tripleo_keystone_healthcheck.timer                 tripleo_keystone_healthcheck.service
Mon 2019-02-18 20:18:35 UTC  6s left       Mon 2019-02-18 20:17:13 UTC  1min 15s ago tripleo_memcached_healthcheck.timer                tripleo_memcached_healthcheck.service
(...)</pre><p>
					To check the status of a specific container timer, run the <code class="literal">systemctl status</code> command for the healthcheck service:
				</p><pre class="screen">$ sudo systemctl status tripleo_keystone_healthcheck.service
● tripleo_keystone_healthcheck.service - keystone healthcheck
   Loaded: loaded (/etc/systemd/system/tripleo_keystone_healthcheck.service; disabled; vendor preset: disabled)
   Active: inactive (dead) since Mon 2019-02-18 20:22:46 UTC; 22s ago
  Process: 115581 ExecStart=/usr/bin/podman exec keystone /openstack/healthcheck (code=exited, status=0/SUCCESS)
 Main PID: 115581 (code=exited, status=0/SUCCESS)

Feb 18 20:22:46 undercloud.localdomain systemd[1]: Starting keystone healthcheck...
Feb 18 20:22:46 undercloud.localdomain podman[115581]: {"versions": {"values": [{"status": "stable", "updated": "2019-01-22T00:00:00Z", "..."}]}]}}
Feb 18 20:22:46 undercloud.localdomain podman[115581]: 300 192.168.24.1:35357 0.012 seconds
Feb 18 20:22:46 undercloud.localdomain systemd[1]: Started keystone healthcheck.</pre><p>
					To stop, start, restart, and show the status of a container timer, run the relevant <code class="literal">systemctl</code> command against the <code class="literal">.timer</code> Systemd resource. For example, to check the status of the <code class="literal">tripleo_keystone_healthcheck.timer</code> resource, run the following command:
				</p><pre class="screen">$ sudo systemctl status tripleo_keystone_healthcheck.timer
● tripleo_keystone_healthcheck.timer - keystone container healthcheck
   Loaded: loaded (/etc/systemd/system/tripleo_keystone_healthcheck.timer; enabled; vendor preset: disabled)
   Active: active (waiting) since Fri 2019-02-15 23:53:18 UTC; 2 days ago</pre><p>
					If the healthcheck service is disabled but the timer for that service is present and enabled, it means that the check is currently timed out, but will be run according to timer. You can also start the check manually.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">podman ps</code> command does not show the container health status.
					</p></div></div><div class="formalpara"><p class="title"><strong>Checking container logs</strong></p><p>
						OpenStack Platform 16 introduces a new logging directory <code class="literal">/var/log/containers/stdout</code> that contains the standard output (stdout) all of the containers, and standard errors (stderr) consolidated in one single file for each container.
					</p></div><p>
					Paunch and the <code class="literal">container-puppet.py</code> script configure podman containers to push their outputs to the <code class="literal">/var/log/containers/stdout</code> directory, which creates a collection of all logs, even for the deleted containers, such as <code class="literal">container-puppet-*</code> containers.
				</p><p>
					The host also applies log rotation to this directory, which prevents huge files and disk space issues.
				</p><p>
					In case a container is replaced, the new container outputs to the same log file, because <code class="literal">podman</code> uses the container name instead of container ID.
				</p><p>
					You can also check the logs for a containerized service with the <code class="literal">podman logs</code> command. For example, to view the logs for the <code class="literal">keystone</code> container, run the following command:
				</p><pre class="screen">$ sudo podman logs keystone</pre><div class="formalpara"><p class="title"><strong>Accessing containers</strong></p><p>
						To enter the shell for a containerized service, use the <code class="literal">podman exec</code> command to launch <code class="literal">/bin/bash</code>. For example, to enter the shell for the <code class="literal">keystone</code> container, run the following command:
					</p></div><pre class="screen">$ sudo podman exec -it keystone /bin/bash</pre><p>
					To enter the shell for the <code class="literal">keystone</code> container as the root user, run the following command:
				</p><pre class="screen">$ sudo podman exec --user 0 -it &lt;NAME OR ID&gt; /bin/bash</pre><p>
					To exit the container, run the following command:
				</p><pre class="screen"># exit</pre></section><section class="section" id="modifying-the-overcloud-environment"><div class="titlepage"><div><div><h2 class="title">12.2. Modifying the overcloud environment</h2></div></div></div><p>
					You can modify the overcloud to add additional features or alter existing operations. To modify the overcloud, make modifications to your custom environment files and heat templates, then rerun the <code class="literal">openstack overcloud deploy</code> command from your initial overcloud creation. For example, if you created an overcloud using <a class="xref" href="index.html#deployment-command" title="7.13. Deployment command">Section 7.13, “Deployment command”</a>, rerun the following command:
				</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud deploy --templates \
  -e ~/templates/node-info.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
  -e ~/templates/network-environment.yaml \
  -e ~/templates/storage-environment.yaml \
  --ntp-server pool.ntp.org</pre><p>
					Director checks the <code class="literal">overcloud</code> stack in heat, and then updates each item in the stack with the environment files and heat templates. Director does not recreate the overcloud, but rather changes the existing overcloud.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Removing parameters from custom environment files does not revert the parameter value to the default configuration. You must identify the default value from the core heat template collection in <code class="literal">/usr/share/openstack-tripleo-heat-templates</code> and set the value in your custom environment file manually.
					</p></div></div><p>
					If you want to include a new environment file, add it to the <code class="literal">openstack overcloud deploy</code> command with the`-e` option. For example:
				</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud deploy --templates \
  -e ~/templates/new-environment.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
  -e ~/templates/network-environment.yaml \
  -e ~/templates/storage-environment.yaml \
  -e ~/templates/node-info.yaml \
  --ntp-server pool.ntp.org</pre><p>
					This command includes the new parameters and resources from the environment file into the stack.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						It is not advisable to make manual modifications to the overcloud configuration because director might overwrite these modifications later.
					</p></div></div></section><section class="section" id="importing-virtual-machines-into-the-overcloud"><div class="titlepage"><div><div><h2 class="title">12.3. Importing virtual machines into the overcloud</h2></div></div></div><p>
					You can migrate virtual machines from an existing OpenStack environment to your Red Hat OpenStack Platform (RHOSP) environment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							On the existing OpenStack environment, create a new image by taking a snapshot of a running server and download the image:
						</p><pre class="screen">$ openstack server image create instance_name --name image_name
$ openstack image save image_name --file exported_vm.qcow2</pre></li><li class="listitem"><p class="simpara">
							Copy the exported image to the undercloud node:
						</p><pre class="screen">$ scp exported_vm.qcow2 stack@192.168.0.2:~/.</pre></li><li class="listitem">
							Log in to the undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Upload the exported image into the overcloud:
						</p><pre class="screen">(overcloud) $ openstack image create imported_image --file exported_vm.qcow2 --disk-format qcow2 --container-format bare</pre></li><li class="listitem"><p class="simpara">
							Launch a new instance:
						</p><pre class="screen">(overcloud) $ openstack server create  imported_instance --key-name default --flavor m1.demo --image imported_image --nic net-id=net_id</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						These commands copy each virtual machine disk from the existing OpenStack environment to the new Red Hat OpenStack Platform. QCOW snapshots lose their original layering system.
					</p></div></div><p>
					This process migrates all instances from a Compute node. You can now perform maintenance on the node without any instance downtime. To return the Compute node to an enabled state, run the following command:
				</p><pre class="screen">$ source ~/overcloudrc
(overcloud) $ openstack compute service set [hostname] nova-compute --enable</pre></section><section class="section" id="running-the-dynamic-inventory-script"><div class="titlepage"><div><div><h2 class="title">12.4. Running the dynamic inventory script</h2></div></div></div><p>
					Director can run Ansible-based automation in your Red Hat OpenStack Platform (RHOSP) environment. Director uses the <code class="literal">tripleo-ansible-inventory</code> command to generate a dynamic inventory of nodes in your environment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view a dynamic inventory of nodes, run the <code class="literal">tripleo-ansible-inventory</code> command after sourcing <code class="literal">stackrc</code>:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ tripleo-ansible-inventory --list</pre><p class="simpara">
							Use the <code class="literal">--list</code> option to return details about all hosts. This command outputs the dynamic inventory in a JSON format:
						</p><pre class="screen">{"overcloud": {"children": ["controller", "compute"], "vars": {"ansible_ssh_user": "heat-admin"}}, "controller": ["192.168.24.2"], "undercloud": {"hosts": ["localhost"], "vars": {"overcloud_horizon_url": "http://192.168.24.4:80/dashboard", "overcloud_admin_password": "abcdefghijklm12345678", "ansible_connection": "local"}}, "compute": ["192.168.24.3"]}</pre></li><li class="listitem"><p class="simpara">
							To execute Ansible playbooks on your environment, run the <code class="literal">ansible</code> command and include the full path of the dynamic inventory tool using the <code class="literal">-i</code> option. For example:
						</p><pre class="screen">(undercloud) $ ansible [HOSTS] -i /bin/tripleo-ansible-inventory [OTHER OPTIONS]</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Replace <code class="literal">[HOSTS]</code> with the type of hosts that you want to use to use:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											<code class="literal">controller</code> for all Controller nodes
										</li><li class="listitem">
											<code class="literal">compute</code> for all Compute nodes
										</li><li class="listitem">
											<code class="literal">overcloud</code> for all overcloud child nodes. For example, <code class="literal">controller</code> and <code class="literal">compute</code> nodes
										</li><li class="listitem">
											<code class="literal">undercloud</code> for the undercloud
										</li><li class="listitem">
											<code class="literal">"*"</code> for all nodes
										</li></ul></div></li><li class="listitem"><p class="simpara">
									Replace <code class="literal">[OTHER OPTIONS]</code> with additional Ansible options.
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Use the <code class="literal">--ssh-extra-args='-o StrictHostKeyChecking=no'</code> option to bypass confirmation on host key checking.
										</li><li class="listitem">
											Use the <code class="literal">-u [USER]</code> option to change the SSH user that executes the Ansible automation. The default SSH user for the overcloud is automatically defined using the <code class="literal">ansible_ssh_user</code> parameter in the dynamic inventory. The <code class="literal">-u</code> option overrides this parameter.
										</li><li class="listitem">
											Use the <code class="literal">-m [MODULE]</code> option to use a specific Ansible module. The default is <code class="literal">command</code>, which executes Linux commands.
										</li><li class="listitem">
											Use the <code class="literal">-a [MODULE_ARGS]</code> option to define arguments for the chosen module.
										</li></ul></div></li></ul></div></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Custom Ansible automation on the overcloud is not part of the standard overcloud stack. Subsequent execution of the <code class="literal">openstack overcloud deploy</code> command might override Ansible-based configuration for OpenStack Platform services on overcloud nodes.
					</p></div></div></section><section class="section" id="removing-the-overcloud"><div class="titlepage"><div><div><h2 class="title">12.5. Removing the overcloud</h2></div></div></div><p>
					To remove the overcloud, complete the following steps:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Delete an existing overcloud:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud delete overcloud</pre></li><li class="listitem"><p class="simpara">
							Confirm that the overcloud is no longer present in the output of the <code class="literal">openstack stack list</code> command:
						</p><pre class="screen">(undercloud) $ openstack stack list</pre><p class="simpara">
							Deletion takes a few minutes.
						</p></li><li class="listitem">
							When the deletion completes, follow the standard steps in the deployment scenarios to recreate your overcloud.
						</li></ol></div></section></section><section class="chapter" id="configuring-the-overcloud-with-ansible"><div class="titlepage"><div><div><h2 class="title">Chapter 13. Configuring the overcloud with Ansible</h2></div></div></div><p>
				Ansible is the main method to apply the overcloud configuration. This chapter provides information about how to interact with the overcloud Ansible configuration.
			</p><p>
				Although director generates the Ansible playbooks automatically, it is a good idea to familiarize yourself with Ansible syntax. For more information about using Ansible, see <a class="link" href="https://docs.ansible.com/">https://docs.ansible.com/</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Ansible also uses the concept of roles, which are different to OpenStack Platform director roles. <span class="strong strong"><strong>Ansible roles</strong></span> form reusable components of playbooks, whereas director roles contain mappings of OpenStack services to node types.
				</p></div></div><section class="section" id="ansible-based-overcloud-configuration-config-download"><div class="titlepage"><div><div><h2 class="title">13.1. Ansible-based overcloud configuration (config-download)</h2></div></div></div><p>
					The <code class="literal">config-download</code> feature is the method that director uses to configure the overcloud. Director uses <code class="literal">config-download</code> in conjunction with OpenStack Orchestration (heat) and OpenStack Workflow Service (mistral) to generate the software configuration and apply the configuration to each overcloud node. Although heat creates all deployment data from <code class="literal">SoftwareDeployment</code> resources to perform the overcloud installation and configuration, heat does not apply any of the configuration. Heat only provides the configuration data through the heat API. When director creates the stack, a mistral workflow queries the heat API to obtain the configuration data, generate a set of Ansible playbooks, and applies the Ansible playbooks to the overcloud.
				</p><p>
					As a result, when you run the <code class="literal">openstack overcloud deploy</code> command, the following process occurs:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Director creates a new deployment plan based on <code class="literal">openstack-tripleo-heat-templates</code> and includes any environment files and parameters to customize the plan.
						</li><li class="listitem">
							Director uses heat to interpret the deployment plan and create the overcloud stack and all descendant resources. This includes provisioning nodes with the OpenStack Bare Metal service (ironic).
						</li><li class="listitem">
							Heat also creates the software configuration from the deployment plan. Director compiles the Ansible playbooks from this software configuration.
						</li><li class="listitem">
							Director generates a temporary user (<code class="literal">tripleo-admin</code>) on the overcloud nodes specifically for Ansible SSH access.
						</li><li class="listitem">
							Director downloads the heat software configuration and generates a set of Ansible playbooks using heat outputs.
						</li><li class="listitem">
							Director applies the Ansible playbooks to the overcloud nodes using <code class="literal">ansible-playbook</code>.
						</li></ul></div></section><section class="section" id="config-download-working-directory"><div class="titlepage"><div><div><h2 class="title">13.2. config-download working directory</h2></div></div></div><p>
					Director generates a set of Ansible playbooks for the <code class="literal">config-download</code> process. These playbooks are stored in a working directory in the <code class="literal">/var/lib/mistral/</code>. This directory is named after the name of the overcloud, which is <code class="literal">overcloud</code> by default.
				</p><p>
					The working directory contains a set of sub-directories named after each overcloud role. These sub-directories contain all tasks relevant to the configuration of the nodes in the overcloud role. These sub-directories also contain additional sub-directories named after each specific node. These sub-directories contain node-specific variables to apply to the overcloud role tasks. As a result, the overcloud roles within the working directory use the following structure:
				</p><pre class="screen">─ /var/lib/mistral/overcloud
  |
  ├── Controller
  │   ├── overcloud-controller-0
  |   ├── overcloud-controller-1
  │   └── overcloud-controller-2
  ├── Compute
  │   ├── overcloud-compute-0
  |   ├── overcloud-compute-1
  │   └── overcloud-compute-2
  ...</pre><p>
					Each working directory is a local Git repository that records changes after each deployment operation. Use the local Git repositories to track configuration changes between each deployment.
				</p></section><section class="section" id="enabling-access-to-config-download-working-directories"><div class="titlepage"><div><div><h2 class="title">13.3. Enabling access to config-download working directories</h2></div></div></div><p>
					The <code class="literal">mistral</code> user in the OpenStack Workflow service (mistral) containers own all files in the <code class="literal">/var/lib/mistral/</code> working directories. You can grant the <code class="literal">stack</code> user on the undercloud access to all files in this directory. This helps with performing certain operations within the directory.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Use the <code class="literal">setfacl</code> command to grant the <code class="literal">stack</code> user on the undercloud access to the files in the <code class="literal">/var/lib/mistral</code> directory:
						</p><pre class="screen">$ sudo setfacl -R -m u:stack:rwx /var/lib/mistral</pre><p class="simpara">
							This command retains <code class="literal">mistral</code> user access to the directory.
						</p></li></ol></div></section><section class="section" id="checking-config-download-logs"><div class="titlepage"><div><div><h2 class="title">13.4. Checking config-download log</h2></div></div></div><p>
					During the <code class="literal">config-download</code> process, Ansible creates a log file on the undercloud in the <code class="literal">config-download</code> working directory.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View the log with the <code class="literal">less</code> command within the <code class="literal">config-download</code> working directory. The following example uses the <code class="literal">overcloud</code> working directory:
						</p><pre class="screen">$ less /var/lib/mistral/overcloud/ansible.log</pre></li></ol></div></section><section class="section" id="separating-the-provisioning-and-configuration-processes"><div class="titlepage"><div><div><h2 class="title">13.5. Separating the provisioning and configuration processes</h2></div></div></div><p>
					The <code class="literal">openstack overcloud deploy</code> command runs the heat-based provisioning process and then the <code class="literal">config-download</code> configuration process. You can also run the command to execute each process individually.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the deployment command with the <code class="literal">--stack-only</code> option. Include any environment files required for your overcloud:
						</p><pre class="screen">$ openstack overcloud deploy \
  --templates \
  -e environment-file1.yaml \
  -e environment-file2.yaml \
  ...
  --stack-only</pre></li><li class="listitem">
							Wait until the provisioning process completes.
						</li><li class="listitem"><p class="simpara">
							Enable SSH access from the undercloud to the overcloud for the <code class="literal">tripleo-admin</code> user. The <code class="literal">config-download</code> process uses the <code class="literal">tripleo-admin</code> user to perform the Ansible-based configuration:
						</p><pre class="screen">$ openstack overcloud admin authorize</pre></li><li class="listitem"><p class="simpara">
							Run the deployment command with the <code class="literal">--config-download-only</code> option. Include any environment files required for your overcloud:
						</p><pre class="screen">$ openstack overcloud deploy \
  --templates \
  -e environment-file1.yaml \
  -e environment-file2.yaml \
  ...
  --config-download-only</pre></li><li class="listitem">
							Wait until the configuration process completes.
						</li></ol></div></section><section class="section" id="running-config-download-manually"><div class="titlepage"><div><div><h2 class="title">13.6. Running config-download manually</h2></div></div></div><p>
					The working directory in <code class="literal">/var/lib/mistral/overcloud</code> contains the playbooks and scripts necessary to interact with <code class="literal">ansible-playbook</code> directly. This procedure shows how to interact with these files.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change to the directory of the Ansible playbook::
						</p><pre class="screen">$ cd /var/lib/mistral/overcloud/</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">ansible-playbook-command.sh</code> command to reproduce the deployment:
						</p><pre class="screen">$ ./ansible-playbook-command.sh</pre><p class="simpara">
							You can pass additional Ansible arguments to this script, which are then passed unchanged to the <code class="literal">ansible-playbook</code> command. This means that you can use other Ansible features, such as check mode (<code class="literal">--check</code>), limiting hosts (<code class="literal">--limit</code>), or overriding variables (<code class="literal">-e</code>). For example:
						</p><pre class="screen">$ ./ansible-playbook-command.sh --limit Controller</pre></li><li class="listitem"><p class="simpara">
							The working directory contains a playbook called <code class="literal">deploy_steps_playbook.yaml</code>, which runs the overcloud configuration. To view this playbook, run the following command:
						</p><pre class="screen">$ less deploy_steps_playbook.yaml</pre><p class="simpara">
							The playbook uses various task files contained in the working directory. Some task files are common to all OpenStack Platform roles and some are specific to certain OpenStack Platform roles and servers.
						</p></li><li class="listitem"><p class="simpara">
							The working directory also contains sub-directories that correspond to each role that you define in your overcloud <code class="literal">roles_data</code> file. For example:
						</p><pre class="screen">$ ls Controller/</pre><p class="simpara">
							Each OpenStack Platform role directory also contains sub-directories for individual servers of that role type. The directories use the composable role hostname format:
						</p><pre class="screen">$ ls Controller/overcloud-controller-0</pre></li><li class="listitem"><p class="simpara">
							The Ansible tasks are tagged. To see the full list of tags, use the CLI argument <code class="literal">--list-tags</code> for <code class="literal">ansible-playbook</code>:
						</p><pre class="screen">$ ansible-playbook -i tripleo-ansible-inventory.yaml --list-tags deploy_steps_playbook.yaml</pre><p class="simpara">
							Then apply tagged configuration using the <code class="literal">--tags</code>, <code class="literal">--skip-tags</code>, or <code class="literal">--start-at-task</code> with the <code class="literal">ansible-playbook-command.sh</code> script:
						</p><pre class="screen">$ ./ansible-playbook-command.sh --tags overcloud</pre></li><li class="listitem"><p class="simpara">
							When <code class="literal">config-download</code> configures Ceph, Ansible executes <code class="literal">ceph-ansible</code> from within the <code class="literal">config-download external_deploy_steps_tasks</code> playbook. When you run <code class="literal">config-download</code> manually, the second Ansible execution does not inherit the <code class="literal">ssh_args</code> argument. To pass Ansible environment variables to this execution, use a heat environment file. For example:
						</p><pre class="screen">parameter_defaults:
  CephAnsibleEnvironmentVariables:
    ANSIBLE_HOST_KEY_CHECKING: 'False'
    ANSIBLE_PRIVATE_KEY_FILE: '/home/stack/.ssh/id_rsa'</pre></li></ol></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						When you use ansible-playbook CLI arguments such as <code class="literal">--tags</code>, <code class="literal">--skip-tags</code>, or <code class="literal">--start-at-task</code>, do not run or apply deployment configuration out of order. These CLI arguments are a convenient way to rerun previously failed tasks or to iterate over an initial deployment. However, to guarantee a consistent deployment, you must run all tasks from <code class="literal">deploy_steps_playbook.yaml</code> in order.
					</p></div></div></section><section class="section" id="performing-git-operations-on-the-working-directory"><div class="titlepage"><div><div><h2 class="title">13.7. Performing Git operations on the working directory</h2></div></div></div><p>
					The <code class="literal">config-download</code> working directory is a local Git repository. Every time a deployment operation runs, director adds a Git commit to the working directory with the relevant changes. You can perform Git operations to view configuration for the deployment at different stages and compare the configuration with different deployments.
				</p><p>
					Be aware of the limitations of the working directory. For example, if you use Git to revert to a previous version of the <code class="literal">config-download</code> working directory, this action affects only the configuration in the working directory. It does not affect the following configurations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<span class="strong strong"><strong>The overcloud data schema:</strong></span> Applying a previous version of the working directory software configuration does not undo data migration and schema changes.
						</li><li class="listitem">
							<span class="strong strong"><strong>The hardware layout of the overcloud:</strong></span> Reverting to previous software configuration does not undo changes related to overcloud hardware, such as scaling up or down.
						</li><li class="listitem">
							<span class="strong strong"><strong>The heat stack:</strong></span> Reverting to earlier revisions of the working directory has no effect on the configuration stored in the heat stack. The heat stack creates a new version of the software configuration that applies to the overcloud. To make permanent changes to the overcloud, modify the environment files applied to the overcloud stack before you rerun the <code class="literal">openstack overcloud deploy</code> command.
						</li></ul></div><p>
					Complete the following steps to compare different commits of the <code class="literal">config-download</code> working directory.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change to the <code class="literal">config-download</code> working directory for your overcloud. In this example, the working directory is for the overcloud named <code class="literal">overcloud</code>:
						</p><pre class="screen">$ cd /var/lib/mistral/overcloud</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">git log</code> command to list the commits in your working directory. You can also format the log output to show the date:
						</p><pre class="screen">$ git log --format=format:"%h%x09%cd%x09"
a7e9063 Mon Oct 8 21:17:52 2018 +1000
dfb9d12 Fri Oct 5 20:23:44 2018 +1000
d0a910b Wed Oct 3 19:30:16 2018 +1000
...</pre><p class="simpara">
							By default, the most recent commit appears first.
						</p></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">git diff</code> command against two commit hashes to see all changes between the deployments:
						</p><pre class="screen">$ git diff a7e9063 dfb9d12</pre></li></ol></div></section><section class="section" id="creating-config-download-files-manually"><div class="titlepage"><div><div><h2 class="title">13.8. Creating config-download files manually</h2></div></div></div><p>
					You can generate your own <code class="literal">config-download</code> files outside of the standard workflow. For example, you can generate the overcloud heat stack using the <code class="literal">--stack-only</code> option with the <code class="literal">openstack overcloud deploy</code> command so that you can apply the configuration separately. Complete the following steps to create your own <code class="literal">config-download</code> files manually.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Generate the <code class="literal">config-download</code> files:
						</p><pre class="screen">$ openstack overcloud config download \
  --name overcloud \
  --config-dir ~/config-download</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">--name</code> is the name of the overcloud that you want to use for the Ansible file export.
								</li><li class="listitem">
									<code class="literal">--config-dir</code> is the location where you want to save the <code class="literal">config-download</code> files.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Change to the directory that contains your <code class="literal">config-download</code> files:
						</p><pre class="screen">$ cd ~/config-download</pre></li><li class="listitem"><p class="simpara">
							Generate a static inventory file:
						</p><pre class="screen">$ tripleo-ansible-inventory \
  --ansible_ssh_user heat-admin \
  --static-yaml-inventory inventory.yaml</pre></li></ol></div><p>
					Use the <code class="literal">config-download</code> files and the static inventory file to perform a configuration. To execute the deployment playbook, run the <code class="literal">ansible-playbook</code> command:
				</p><pre class="screen">$ ansible-playbook \
  -i inventory.yaml \
  --private-key ~/.ssh/id_rsa \
  --become \
  ~/config-download/deploy_steps_playbook.yaml</pre><p>
					To generate an <code class="literal">overcloudrc</code> file manually from this configuration, run the following command:
				</p><pre class="screen">$ openstack action execution run \
  --save-result \
  --run-sync \
  tripleo.deployment.overcloudrc \
  '{"container":"overcloud"}' \
  | jq -r '.["result"]["overcloudrc.v3"]' &gt; overcloudrc.v3</pre></section><section class="section" id="config-download-top-level-files"><div class="titlepage"><div><div><h2 class="title">13.9. config-download top level files</h2></div></div></div><p>
					The following file are important top level files within a <code class="literal">config-download</code> working directory.
				</p><div class="formalpara"><p class="title"><strong>Ansible configuration and execution</strong></p><p>
						The following files are specific to configuring and executing Ansible within the <code class="literal">config-download</code> working directory.
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">ansible.cfg</span></dt><dd>
								Configuration file used when running <code class="literal">ansible-playbook</code>.
							</dd><dt><span class="term">ansible.log</span></dt><dd>
								Log file from the last run of <code class="literal">ansible-playbook</code>.
							</dd><dt><span class="term">ansible-errors.json</span></dt><dd>
								JSON structured file that contains any deployment errors.
							</dd><dt><span class="term">ansible-playbook-command.sh</span></dt><dd>
								Executable script to rerun the <code class="literal">ansible-playbook</code> command from the last deployment operation.
							</dd><dt><span class="term">ssh_private_key</span></dt><dd>
								Private SSH key that Ansible uses to access the overcloud nodes.
							</dd><dt><span class="term">tripleo-ansible-inventory.yaml</span></dt><dd>
								Ansible inventory file that contains hosts and variables for all the overcloud nodes.
							</dd><dt><span class="term">overcloud-config.tar.gz</span></dt><dd>
								Archive of the working directory.
							</dd></dl></div><div class="formalpara"><p class="title"><strong>Playbooks</strong></p><p>
						The following files are playbooks within the <code class="literal">config-download</code> working directory.
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">deploy_steps_playbook.yaml</span></dt><dd>
								Main deployment steps. This playbook performs the main configuration operations for your overcloud.
							</dd><dt><span class="term">pre_upgrade_rolling_steps_playbook.yaml</span></dt><dd>
								Pre upgrade steps for major upgrade
							</dd><dt><span class="term">upgrade_steps_playbook.yaml</span></dt><dd>
								Major upgrade steps.
							</dd><dt><span class="term">post_upgrade_steps_playbook.yaml</span></dt><dd>
								Post upgrade steps for major upgrade.
							</dd><dt><span class="term">update_steps_playbook.yaml</span></dt><dd>
								Minor update steps.
							</dd><dt><span class="term">fast_forward_upgrade_playbook.yaml</span></dt><dd>
								Fast forward upgrade tasks. Use this playbook only when you want to upgrade from one long-life version of Red Hat OpenStack Platform to the next.
							</dd></dl></div></section><section class="section" id="config-download-tags"><div class="titlepage"><div><div><h2 class="title">13.10. config-download tags</h2></div></div></div><p>
					The playbooks use tagged tasks to control the tasks that they apply to the overcloud. Use tags with the <code class="literal">ansible-playbook</code> CLI arguments <code class="literal">--tags</code> or <code class="literal">--skip-tags</code> to control which tasks to execute. The following list contains information about the tags that are enabled by default:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">facts</span></dt><dd>
								Fact gathering operations.
							</dd><dt><span class="term">common_roles</span></dt><dd>
								Ansible roles common to all nodes.
							</dd><dt><span class="term">overcloud</span></dt><dd>
								All plays for overcloud deployment.
							</dd><dt><span class="term">pre_deploy_steps</span></dt><dd>
								Deployments that happen before the <code class="literal">deploy_steps</code> operations.
							</dd><dt><span class="term">host_prep_steps</span></dt><dd>
								Host preparation steps.
							</dd><dt><span class="term">deploy_steps</span></dt><dd>
								Deployment steps.
							</dd><dt><span class="term">post_deploy_steps</span></dt><dd>
								Steps that happen after the <code class="literal">deploy_steps</code> operations.
							</dd><dt><span class="term">external</span></dt><dd>
								All external deployment tasks.
							</dd><dt><span class="term">external_deploy_steps</span></dt><dd>
								External deployment tasks that run on the undercloud only.
							</dd></dl></div></section><section class="section" id="config-download-deployment-steps"><div class="titlepage"><div><div><h2 class="title">13.11. config-download deployment steps</h2></div></div></div><p>
					The <code class="literal">deploy_steps_playbook.yaml</code> playbook configures the overcloud. This playbook applies all software configuration that is necessary to deploy a full overcloud based on the overcloud deployment plan.
				</p><p>
					This section contains a summary of the different Ansible plays used within this playbook. The play names in this section are the same names that are used within the playbook and that are displayed in the <code class="literal">ansible-playbook</code> output. This section also contains information about the Ansible tags that are set on each play.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Gather facts from undercloud</span></dt><dd><p class="simpara">
								Fact gathering for the undercloud node.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">facts</code>
							</p></dd><dt><span class="term">Gather facts from overcloud</span></dt><dd><p class="simpara">
								Fact gathering for the overcloud nodes.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">facts</code>
							</p></dd><dt><span class="term">Load global variables</span></dt><dd><p class="simpara">
								Loads all variables from <code class="literal">global_vars.yaml</code>.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">always</code>
							</p></dd><dt><span class="term">Common roles for TripleO servers</span></dt><dd><p class="simpara">
								Applies common Ansible roles to all overcloud nodes, including tripleo-bootstrap for installing bootstrap packages, and tripleo-ssh-known-hosts for configuring ssh known hosts.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">common_roles</code>
							</p></dd><dt><span class="term">Overcloud deploy step tasks for step 0</span></dt><dd><p class="simpara">
								Applies tasks from the deploy_steps_tasks template interface.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">deploy_steps</code>
							</p></dd><dt><span class="term">Server deployments</span></dt><dd><p class="simpara">
								Applies server-specific heat deployments for configuration such as networking and hieradata. Includes NetworkDeployment, &lt;Role&gt;Deployment, &lt;Role&gt;AllNodesDeployment, etc.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">pre_deploy_steps</code>
							</p></dd><dt><span class="term">Host prep steps</span></dt><dd><p class="simpara">
								Applies tasks from the host_prep_steps template interface.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">host_prep_steps</code>
							</p></dd><dt><span class="term">External deployment step [1,2,3,4,5]</span></dt><dd><p class="simpara">
								Applies tasks from the external_deploy_steps_tasks template interface. Ansible runs these tasks only against the undercloud node.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">external</code>, <code class="literal">external_deploy_steps</code>
							</p></dd><dt><span class="term">Overcloud deploy step tasks for [1,2,3,4,5]</span></dt><dd><p class="simpara">
								Applies tasks from the deploy_steps_tasks template interface.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">deploy_steps</code>
							</p></dd><dt><span class="term">Overcloud common deploy step tasks [1,2,3,4,5]</span></dt><dd><p class="simpara">
								Applies the common tasks performed at each step, including puppet host configuration, <code class="literal">container-puppet.py</code>, and paunch (container configuration).
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">deploy_steps</code>
							</p></dd><dt><span class="term">Server Post Deployments</span></dt><dd><p class="simpara">
								Applies server specific heat deployments for configuration performed after the 5-step deployment process.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">overcloud</code>, <code class="literal">post_deploy_steps</code>
							</p></dd><dt><span class="term">External deployment Post Deploy tasks</span></dt><dd><p class="simpara">
								Applies tasks from the external_post_deploy_steps_tasks template interface. Ansible runs these tasks only against the undercloud node.
							</p><p class="simpara">
								<span class="strong strong"><strong>Tags:</strong></span> <code class="literal">external</code>, <code class="literal">external_deploy_steps</code>
							</p></dd></dl></div></section><section class="section" id="next_steps_4"><div class="titlepage"><div><div><h2 class="title">13.12. Next Steps</h2></div></div></div><p>
					You can now continue your regular overcloud operations.
				</p></section></section><section class="chapter" id="managing-containers-with-ansible_preprovisioned"><div class="titlepage"><div><div><h2 class="title">Chapter 14. Managing containers with Ansible</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p></div></div><p>
				Red Hat OpenStack Platform 16.1 uses Paunch to manage containers. However, you can also use the Ansible role <code class="literal">tripleo-container-manage</code> to perform management operations on your containers. If you want to use the <code class="literal">tripleo-container-manage</code> role, you must first disable Paunch. With Paunch disabled, director uses the Ansible role automatically, and you can also write custom playbooks to perform specific container management operations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Collect the container configuration data that heat generates. The <code class="literal">tripleo-container-manage</code> role uses this data to orchestrate container deployment.
					</li><li class="listitem">
						Start containers.
					</li><li class="listitem">
						Stop containers.
					</li><li class="listitem">
						Update containers.
					</li><li class="listitem">
						Delete containers.
					</li><li class="listitem">
						Run a container with a specific configuration.
					</li></ul></div><p>
				Although director performs container management automatically, you might want to customize a container configuration, or apply a hotfix to a container without redeploying the overcloud.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This role supports only Podman container management.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
					</li></ul></div><section class="section" id="enabling-the-tripleo-container-manage-ansible-role-on-the-undercloud_managing-containers-with-ansible"><div class="titlepage"><div><div><h2 class="title">14.1. Enabling the tripleo-container-manage Ansible role on the undercloud</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					Paunch is the default container management mechanism in Red Hat OpenStack Platform 16.1. However, you can also use the <code class="literal">tripleo-container-manage</code> Ansible role. If you want to use this role, you must disable Paunch.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A host machine with a base operating system and the <code class="literal">python3-tripleoclient</code> package installed. For more information, see <a class="xref" href="index.html#preparing-for-director-installation" title="Chapter 3. Preparing for director installation">Chapter 3, <em>Preparing for director installation</em></a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud host as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Set the <code class="literal">undercloud_enable_paunch</code> parameter to <code class="literal">false</code> in the <code class="literal">undercloud.conf</code> file:
						</p><pre class="screen">undercloud_enable_paunch: false</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack undercloud install</code> command:
						</p><pre class="screen">$ openstack undercloud install</pre></li></ol></div></section><section class="section" id="enabling-the-tripleo-container-manage-ansible-role-on-the-overcloud_managing-containers-with-ansible"><div class="titlepage"><div><div><h2 class="title">14.2. Enabling the tripleo-container-manage Ansible role on the overcloud</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					Paunch is the default container management mechanism in Red Hat OpenStack Platform 16.1. However, you can also use the <code class="literal">tripleo-container-manage</code> Ansible role. If you want to use this role, you must disable Paunch.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud host as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> credentials file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Include the <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/disable-paunch.yaml</code> file in the overcloud deployment command, along with any other environment files that are relevant for your deployment:
						</p><pre class="screen">(undercloud) [stack@director ~]$ openstack overcloud deploy --templates \
  -e /usr/share/openstack-tripleo-heat-templates/environments/disable-paunch.yaml
  -e &lt;OTHER_ENVIRONMENT_FILES
  ...</pre></li></ol></div></section><section class="section" id="performing-operations-on-a-single-container_managing-containers-with-ansible"><div class="titlepage"><div><div><h2 class="title">14.3. Performing operations on a single container</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					You can use the <code class="literal">tripleo-container-manage</code> role to manage all containers, or a specific container. If you want to manage a specific container, you must identify the container deployment step and the name of the container configuration JSON file so that you can target the specific container with a custom Ansible playbook.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A successful undercloud installation. For more information, see <a class="xref" href="index.html#installing-director" title="4.7. Installing director">Section 4.7, “Installing director”</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> credential file:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem">
							Identify the container deployment step. You can find the container configuration for each step in the <code class="literal">/var/lib/tripleo-config/container-startup-config/step_{1,2,3,4,5,6}</code> directory.
						</li><li class="listitem">
							Identify the JSON configuration file for the container. You can find the container configuration file in the relevant <code class="literal">step_*</code> directory. For example, the configuration file for the HAProxy container in step 1 is <code class="literal">/var/lib/tripleo-config/container-startup-config/step_1/haproxy.json</code>.
						</li><li class="listitem"><p class="simpara">
							Write a suitable Ansible playbook. For example, to replace the HAProxy container image, use the following sample playbook:
						</p><pre class="screen">- hosts: localhost
  become: true
  tasks:
    - name: Manage step_1 containers using tripleo-ansible
      block:
        - name: "Manage HAproxy container at step 1 with tripleo-ansible"
          include_role:
            name: tripleo-container-manage
          vars:
            tripleo_container_manage_systemd_order: true
            tripleo_container_manage_config_patterns: 'haproxy.json'
            tripleo_container_manage_config: "/var/lib/tripleo-config/container-startup-config/step_1"
            tripleo_container_manage_config_id: "tripleo_step1"
            tripleo_container_manage_config_overrides:
              haproxy:
                image: registry.redhat.io/tripleomaster/&lt;HAProxy-container&gt;:hotfix</pre><p class="simpara">
							For more information about the variables that you can use with the <code class="literal">tripleo-container-manage</code> role, see <a class="xref" href="index.html#tripleo-container-manage-role-variables_managing-containers-with-ansible" title="14.4. tripleo-container-manage role variables">Section 14.4, “tripleo-container-manage role variables”</a>.
						</p></li><li class="listitem"><p class="simpara">
							Run the playbook:
						</p><pre class="screen">(overcloud) [stack@director]$ ansible-playbook &lt;custom_playbook&gt;.yaml</pre><p class="simpara">
							If you want to execute the playbook without applying any changes, include the <code class="literal">--check</code> option in the <code class="literal">ansible-playbook</code> command:
						</p><pre class="screen">(overcloud) [stack@director]$ ansible-playbook &lt;custom_playbook&gt;.yaml --check</pre><p class="simpara">
							If you want to identify the changes that your playbook makes to your containers without applying the changes, include the <code class="literal">--check</code> and <code class="literal">--diff</code> options in the <code class="literal">ansible-playbook</code> command:
						</p><pre class="screen">(overcloud) [stack@director]$ ansible-playbook &lt;custom_playbook&gt;.yaml --check --diff</pre></li></ol></div></section><section class="section" id="tripleo-container-manage-role-variables_managing-containers-with-ansible"><div class="titlepage"><div><div><h2 class="title">14.4. tripleo-container-manage role variables</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
					</p></div></div><p>
					The <code class="literal">tripleo-container-manage</code> Ansible role contains the following variables:
				</p><div class="table" id="idm140301165349248"><p class="title"><strong>Table 14.1. Role variables</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301168260528" scope="col">Name</th><th align="left" valign="top" id="idm140301168259440" scope="col">Default value</th><th align="left" valign="top" id="idm140301168258352" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_check_puppet_config
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									false
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable if you want Ansible to check Puppet container configurations. Ansible can identify updated container configuration using the configuration hash. If a container has a new configuration from Puppet, set this variable to <code class="literal">true</code> so that Ansible can detect the new configuration and add the container to the list of containers that Ansible must restart.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_cli
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									podman
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the command line interface that you want to use to manage containers. The <code class="literal">tripleo-container-manage</code> role supports only Podman.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_concurrency
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									1
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the number of containers that you want to manage concurrently.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_config
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									/var/lib/tripleo-config/
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the path to the container configuration directory.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_config_id
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									tripleo
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the ID of a specific configuration step. For example, set this value to <code class="literal">tripleo_step2</code> to manage containers for step two of the deployment.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_config_patterns
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									*.json
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the bash regular expression that identifies configuration files in the container configuration directory.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_debug
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									false
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to enable or disable debug mode. Run the <code class="literal">tripleo-container-manage</code> role in debug mode if you want to run a container with a specific one-time configuration, to output the container commands that manage the lifecycle of containers, or to run no-op container management operations for testing and verification purposes.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_healthcheck_disable
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									false
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to enable or disable healthchecks.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_log_path
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									/var/log/containers/stdouts
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to set the stdout log path for containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_systemd_order
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									false
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to enable or disable systemd shutdown ordering with Ansible.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_systemd_teardown
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									true
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to trigger the cleanup of obsolete containers.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_config_overrides
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									{}
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to override any container configuration. This variable takes a dictionary of values where each key is the container name and the parameters that you want to override, for example, the container image or user. This variable does not write custom overrides to the JSON container configuration files and any new container deployments, updates, or upgrades revert to the content of the JSON configuration file.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301168260528"> <p>
									tripleo_container_manage_valid_exit_code
								</p>
								 </td><td align="left" valign="top" headers="idm140301168259440"> <p>
									[]
								</p>
								 </td><td align="left" valign="top" headers="idm140301168258352"> <p>
									Use this variable to check if a container returns an exit code. This value must be a list, for example, <code class="literal">[0,3]</code>.
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="chapter" id="using-the-validation-framework"><div class="titlepage"><div><div><h2 class="title">Chapter 15. Using the validation framework</h2></div></div></div><p>
				Red Hat OpenStack Platform includes a validation framework that you can use to verify the requirements and functionality of the undercloud and overcloud. The framework includes two types of validations:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Manual Ansible-based validations, which you execute through the <code class="literal">openstack tripleo validator</code> command set.
					</li><li class="listitem">
						Automatic in-flight validations, which execute during the deployment process.
					</li></ul></div><section class="section" id="ansible-based-validations"><div class="titlepage"><div><div><h2 class="title">15.1. Ansible-based validations</h2></div></div></div><p>
					During the installation of Red Hat OpenStack Platform director, director also installs a set of playbooks from the <code class="literal">openstack-tripleo-validations</code> package. Each playbook contains tests for certain system requirements and a set of groups that define when to run the test:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">no-op</code></span></dt><dd>
								Validations that run a <span class="emphasis"><em>no-op</em></span> (no operation) task to verify to workflow functions correctly. These validations run on both the undercloud and overcloud.
							</dd><dt><span class="term"><code class="literal">prep</code></span></dt><dd>
								Validations that check the hardware configuration of the undercloud node. Run these validation before you run the <code class="literal">openstack undercloud install</code> command.
							</dd><dt><span class="term"><code class="literal">openshift-on-openstack</code></span></dt><dd>
								Validations that check that the environment meets the requirements to be able to deploy OpenShift on OpenStack.
							</dd><dt><span class="term"><code class="literal">pre-introspection</code></span></dt><dd>
								Validations to run before the nodes introspection using Ironic Inspector.
							</dd><dt><span class="term"><code class="literal">pre-deployment</code></span></dt><dd>
								Validations to run before the <code class="literal">openstack overcloud deploy</code> command.
							</dd><dt><span class="term"><code class="literal">post-deployment</code></span></dt><dd>
								Validations to run after the overcloud deployment has finished.
							</dd><dt><span class="term"><code class="literal">pre-upgrade</code></span></dt><dd>
								Validations to validate your OpenStack deployment before an upgrade.
							</dd><dt><span class="term"><code class="literal">post-upgrade</code></span></dt><dd>
								Validations to validate your OpenStack deployment after an upgrade.
							</dd></dl></div></section><section class="section" id="listing-validations"><div class="titlepage"><div><div><h2 class="title">15.2. Listing validations</h2></div></div></div><p>
					Run the <code class="literal">openstack tripleo validator list</code> command to list the different types of validations available.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file.
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack tripleo validator list</code> command:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To list all validations, run the command without any options:
								</p><pre class="screen">$ openstack tripleo validator list</pre></li><li class="listitem"><p class="simpara">
									To list validations in a group, run the command with the <code class="literal">--group</code> option:
								</p><pre class="screen">$ openstack tripleo validator list --group prep</pre></li></ul></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For a full list of options, run <code class="literal">openstack tripleo validator list --help</code>.
					</p></div></div></section><section class="section" id="running-validations"><div class="titlepage"><div><div><h2 class="title">15.3. Running validations</h2></div></div></div><p>
					To run a validation or validation group, use the <code class="literal">openstack tripleo validator run</code> command. To see a full list of options, use the <code class="literal">openstack tripleo validator run --help</code> command.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Enter the <code class="literal">openstack tripleo validator run</code> command:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									To run a single validation, enter the command with the <code class="literal">--validation</code> option and the name of the validation. For example, to check the undercloud memory requirements, enter <code class="literal">--validation undercloud-ram</code>:
								</p><pre class="screen">$ openstack tripleo validator run --validation undercloud-ram</pre></li><li class="listitem"><p class="simpara">
									To run all validations in a group, enter the command with the <code class="literal">--group</code> option:
								</p><pre class="screen">$ openstack tripleo validator run --group prep</pre><p class="simpara">
									To view detailed output from a specific validation, run the <code class="literal">openstack tripleo validator show run</code> command against the UUID of the specific validation from the report:
								</p><pre class="screen">$ openstack tripleo validator show run &lt;UUID&gt;</pre></li></ul></div></li></ol></div></section><section class="section" id="in-flight-validations"><div class="titlepage"><div><div><h2 class="title">15.4. In-flight validations</h2></div></div></div><p>
					Red Hat OpenStack Platform includes in-flight validations in the templates of composable services. In-flight validations verify the operational status of services at key steps of the overcloud deployment process.
				</p><p>
					In-flight validations run automatically as part of the deployment process. Some in-flight validations also use the roles from the <code class="literal">openstack-tripleo-validations</code> package.
				</p></section></section><section class="chapter" id="scaling-overcloud-nodes"><div class="titlepage"><div><div><h2 class="title">Chapter 16. Scaling overcloud nodes</h2></div></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					Do not use <code class="literal">openstack server delete</code> to remove nodes from the overcloud. Follow the procedures in this section to remove and replace nodes correctly.
				</p></div></div><p>
				If you want to add or remove nodes after the creation of the overcloud, you must update the overcloud.
			</p><p>
				Use the following table to determine support for scaling each node type:
			</p><div class="table" id="idm140301168576384"><p class="title"><strong>Table 16.1. Scale support for each node type</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top"> <p>
								Node type
							</p>
							 </td><td align="left" valign="top"> <p>
								Scale up?
							</p>
							 </td><td align="left" valign="top"> <p>
								Scale down?
							</p>
							 </td><td align="left" valign="top"> <p>
								Notes
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								Controller
							</p>
							 </td><td align="left" valign="top"> <p>
								N
							</p>
							 </td><td align="left" valign="top"> <p>
								N
							</p>
							 </td><td align="left" valign="top"> <p>
								You can replace Controller nodes using the procedures in <a class="xref" href="index.html#replacing-controller-nodes" title="Chapter 17. Replacing Controller nodes">Chapter 17, <em>Replacing Controller nodes</em></a>.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								Compute
							</p>
							 </td><td align="left" valign="top"> <p>
								Y
							</p>
							 </td><td align="left" valign="top"> <p>
								Y
							</p>
							 </td><td align="left" valign="top"> </td></tr><tr><td align="left" valign="top"> <p>
								Ceph Storage nodes
							</p>
							 </td><td align="left" valign="top"> <p>
								Y
							</p>
							 </td><td align="left" valign="top"> <p>
								N
							</p>
							 </td><td align="left" valign="top"> <p>
								You must have at least 1 Ceph Storage node from the initial overcloud creation.
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								Object Storage nodes
							</p>
							 </td><td align="left" valign="top"> <p>
								Y
							</p>
							 </td><td align="left" valign="top"> <p>
								Y
							</p>
							 </td><td align="left" valign="top"> </td></tr></tbody></table></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Ensure that you have at least 10 GB free space before you scale the overcloud. This free space accommodates image conversion and caching during the node provisioning process.
				</p></div></div><section class="section" id="adding-nodes-to-the-overcloud"><div class="titlepage"><div><div><h2 class="title">16.1. Adding nodes to the overcloud</h2></div></div></div><p>
					Complete the following steps to add more nodes to the director node pool.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new JSON file (<code class="literal">newnodes.json</code>) that contains details of the new node that you want to register:
						</p><pre class="screen">{
  "nodes":[
    {
        "mac":[
            "dd:dd:dd:dd:dd:dd"
        ],
        "cpu":"4",
        "memory":"6144",
        "disk":"40",
        "arch":"x86_64",
        "pm_type":"ipmi",
        "pm_user":"admin",
        "pm_password":"p@55w0rd!",
        "pm_addr":"192.168.24.207"
    },
    {
        "mac":[
            "ee:ee:ee:ee:ee:ee"
        ],
        "cpu":"4",
        "memory":"6144",
        "disk":"40",
        "arch":"x86_64",
        "pm_type":"ipmi",
        "pm_user":"admin",
        "pm_password":"p@55w0rd!",
        "pm_addr":"192.168.24.208"
    }
  ]
}</pre></li><li class="listitem"><p class="simpara">
							Run the following command to register the new nodes:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud node import newnodes.json</pre></li><li class="listitem"><p class="simpara">
							After you register the new nodes, run the following commands to launch the introspection process for each new node:
						</p><pre class="screen">(undercloud) $ openstack baremetal node manage [NODE UUID]
(undercloud) $ openstack overcloud node introspect [NODE UUID] --provide</pre><p class="simpara">
							This process detects and benchmarks the hardware properties of the nodes.
						</p></li><li class="listitem"><p class="simpara">
							Configure the image properties for the node:
						</p><pre class="screen">(undercloud) $ openstack overcloud node configure [NODE UUID]</pre></li></ol></div></section><section class="section" id="increasing-node-counts-for-roles"><div class="titlepage"><div><div><h2 class="title">16.2. Increasing node counts for roles</h2></div></div></div><p>
					Complete the following steps to scale overcloud nodes for a specific role, such as a Compute node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Tag each new node with the role you want. For example, to tag a node with the Compute role, run the following command:
						</p><pre class="screen">(undercloud) $ openstack baremetal node set --property capabilities='profile:compute,boot_option:local' [NODE UUID]</pre></li><li class="listitem"><p class="simpara">
							To scale the overcloud, you must edit the environment file that contains your node counts and re-deploy the overcloud. For example, to scale your overcloud to 5 Compute nodes, edit the <code class="literal">ComputeCount</code> parameter:
						</p><pre class="screen">parameter_defaults:
  ...
  ComputeCount: 5
  ...</pre></li><li class="listitem"><p class="simpara">
							Rerun the deployment command with the updated file, which in this example is called <code class="literal">node-info.yaml</code>:
						</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates -e /home/stack/templates/node-info.yaml [OTHER_OPTIONS]</pre><p class="simpara">
							Ensure that you include all environment files and options from your initial overcloud creation. This includes the same scale parameters for non-Compute nodes.
						</p></li><li class="listitem">
							Wait until the deployment operation completes.
						</li></ol></div></section><section class="section" id="removing-compute-nodes"><div class="titlepage"><div><div><h2 class="title">16.3. Removing Compute nodes</h2></div></div></div><p>
					There might be situations where you need to remove Compute nodes from the overcloud. For example, you might need to replace a problematic Compute node.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Before you remove a Compute node from the overcloud, migrate the workload from the node to other Compute nodes. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/migrating-virtual-machines-between-compute-nodes">Migrating virtual machine instances between Compute nodes</a>.
					</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Placement service package <code class="literal">python3-osc-placement</code> installed on the undercloud.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the overcloud configuration:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Disable the Compute service on the outgoing node on the overcloud to prevent the node from scheduling new instances:
						</p><pre class="screen">(overcloud) $ openstack compute service list
(overcloud) $ openstack compute service set &lt;hostname&gt; nova-compute --disable</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							Use the <code class="literal">--disable-reason</code> option to add a short explanation on why the service is being disabled. This is useful if you intend to redeploy the Compute service at a later point.
						</p></div></div></li><li class="listitem"><p class="simpara">
							Source the undercloud configuration:
						</p><pre class="screen">(overcloud) $ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Identify the UUID of the overcloud stack:
						</p><pre class="screen">(undercloud) $ openstack stack list</pre></li><li class="listitem"><p class="simpara">
							Identify the UUIDs or hostnames of the nodes that you want to delete:
						</p><pre class="screen">(undercloud) $ openstack server list</pre></li><li class="listitem"><p class="simpara">
							Redeploy the overcloud with the <code class="literal">--update-plan-only</code> option, including all of the environment files that are relevant to your deployment:
						</p><pre class="screen">$ openstack overcloud deploy --update-plan-only \
  --templates  \
  -e /usr/share/openstack-tripleo-heat-templates/environments/network-isolation.yaml \
  -e /home/stack/templates/network-environment.yaml \
  -e /home/stack/templates/storage-environment.yaml \
  -e /home/stack/templates/rhel-registration/environment-rhel-registration.yaml \
  [-e |...]</pre></li><li class="listitem"><p class="simpara">
							Delete the nodes from the stack:
						</p><pre class="screen">$ openstack overcloud node delete --stack [STACK_UUID] --templates -e [ENVIRONMENT_FILE] [NODE1_UUID] [NODE2_UUID] [NODE3_UUID]</pre><p class="simpara">
							Replace [node] with a UUID or hostname of a node.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Do not use a mix of UUIDs and hostnames. Use either only UUIDs or only hostnames.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal">openstack overcloud node delete</code> command runs to completion:
						</p><pre class="screen">(undercloud) $ openstack stack list</pre><p class="simpara">
							The status of the <code class="literal">overcloud</code> stack shows <code class="literal">UPDATE_COMPLETE</code> when the delete operation is complete.
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If the IPMI interface of the node that you want to remove is not reachable, the <code class="literal">openstack overcloud node delete</code> command fails and the stack is in an <code class="literal">UPDATE_FAILED</code> status. Move the node to maintenance mode and re-run the <code class="literal">openstack overcloud node delete</code> command:
								</p><pre class="screen">$ openstack baremetal node maintenance set &lt;NODE_ID&gt;</pre></li><li class="listitem"><p class="simpara">
									Adjust the Compute count and rerun the <code class="literal">openstack overcloud deploy</code> command that you used to deploy the existing overcloud.
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										If you intend to redeploy the Compute service with the same host name, you must use the existing service records for the redeployed node. If this is the case, skip the remaining steps in this procedure, and proceed with the instructions detailed in <a class="xref" href="index.html#redeploy-compute-service" title="Redeploying the Compute service using the same host name">Redeploying the Compute service using the same host name</a>.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Remove the Compute service from the node:
						</p><pre class="screen">(undercloud) $ source ~/overcloudrc
(overcloud) $ openstack compute service list
(overcloud) $ openstack compute service delete &lt;service-id&gt;</pre></li><li class="listitem"><p class="simpara">
							Check the network agents in your overcloud environment:
						</p><pre class="screen">(overcloud) $ openstack network agent list</pre></li><li class="listitem"><p class="simpara">
							If any agents appear for the old node, remove them:
						</p><pre class="screen">(overcloud) $ for AGENT in $(openstack network agent list --host &lt;scaled-down-node&gt; -c ID -f value) ; do openstack network agent delete $AGENT ; done</pre><p class="simpara">
							where &lt;scaled-down-node&gt; is the node being removed.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In an ML2/OVN deployment, known issues prevent removal of the OVN controller and metadata agents. To track progress on these issues, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1828889">BZ1828889</a> and <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1738554">BZ1738554</a>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Remove the deleted Compute service as a resource provider from the Placement service:
						</p><pre class="screen">(overcloud) $ openstack resource provider list
(overcloud) $ openstack resource provider delete &lt;uuid&gt;</pre></li><li class="listitem"><p class="simpara">
							Decrease the <code class="literal">ComputeCount</code> parameter in the environment file that contains your node counts. This file is usually named <code class="literal">node-info.yaml</code>. For example, decrease the node count from five nodes to three nodes if you removed two nodes:
						</p><pre class="screen">parameter_defaults:
  ...
  ComputeCount: 3
  ...</pre><p class="simpara">
							Decreasing the node count ensures director does not provision any new nodes when you run <code class="literal">openstack overcloud deploy</code>.
						</p></li></ol></div><p>
					You can remove the node from the overcloud and re-provision it for other purposes.
				</p><div id="redeploy-compute-service" class="formalpara"><p class="title"><strong>Redeploying the Compute service using the same host name</strong></p><p>
						To redeploy a disabled Compute service, re-enable it after you redeploy a Compute node with the same host name.
					</p></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Remove the deleted Compute service as a resource provider from the Placement service:
						</p><pre class="screen">(undercloud) $ source ~/overcloudrc
(overcloud) $ openstack resource provider list
(overcloud) $ openstack resource provider delete &lt;uuid&gt;</pre></li><li class="listitem"><p class="simpara">
							Check the status of the Compute service:
						</p><pre class="screen">(overcloud) $ openstack compute service list --long
...
| ID | Binary       | Host                  | Zone  | Status   | State | Updated At                 | Disabled Reason      |
| 80 | nova-compute | compute-1.localdomain | nova  | disabled | up    | 2018-07-13T14:35:04.000000 | gets re-provisioned |
...</pre></li><li class="listitem"><p class="simpara">
							When the service state of the redeployed Compute node changes to <code class="literal">up</code>, re-enable the service:
						</p><pre class="screen">(overcloud) $ openstack compute service set compute-1.localdomain nova-compute --enable</pre></li></ol></div></section><section class="section" id="replacing-ceph-storage-nodes"><div class="titlepage"><div><div><h2 class="title">16.4. Replacing Ceph Storage nodes</h2></div></div></div><p>
					You can use director to replace Ceph Storage nodes in a director-created cluster. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/deploying_an_overcloud_with_containerized_red_hat_ceph/">Deploying an Overcloud with Containerized Red Hat Ceph</a> guide.
				</p></section><section class="section" id="replacing-object-storage-nodes"><div class="titlepage"><div><div><h2 class="title">16.5. Replacing Object Storage nodes</h2></div></div></div><p>
					Follow the instructions in this section to understand how to replace Object Storage nodes without impact to the integrity of the cluster. This example involves a three-node Object Storage cluster in which you want to replace the node <code class="literal">overcloud-objectstorage-1</code> node. The goal of the procedure is to add one more node and then remove the <code class="literal">overcloud-objectstorage-1</code> node. The new node replaces the <code class="literal">overcloud-objectstorage-1</code> node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Increase the Object Storage count using the <code class="literal">ObjectStorageCount</code> parameter. This parameter is usually located in <code class="literal">node-info.yaml</code>, which is the environment file that contains your node counts:
						</p><pre class="screen">parameter_defaults:
  ObjectStorageCount: 4</pre><p class="simpara">
							The <code class="literal">ObjectStorageCount</code> parameter defines the quantity of Object Storage nodes in your environment. In this example, scale the quantity of Object Storage nodes from <code class="literal">3</code> to <code class="literal">4</code>.
						</p></li><li class="listitem"><p class="simpara">
							Run the deployment command with the updated <code class="literal">ObjectStorageCount</code> parameter:
						</p><pre class="literallayout">$ source ~/stackrc
(undercloud) $ openstack overcloud deploy --templates -e node-info.yaml <span class="emphasis"><em>ENVIRONMENT_FILES</em></span></pre></li><li class="listitem">
							After the deployment command completes, the overcloud contains an additional Object Storage node.
						</li><li class="listitem"><p class="simpara">
							Replicate data to the new node. Before you remove a node, in this case, <code class="literal">overcloud-objectstorage-1</code>, wait for a replication pass to finish on the new node. Check the replication pass progress in the <code class="literal">/var/log/swift/swift.log</code> file. When the pass finishes, the Object Storage service should log entries similar to the following example:
						</p><pre class="literallayout">Mar 29 08:49:05 localhost <span class="strong strong"><strong>object-server: Object replication complete.</strong></span>
Mar 29 08:49:11 localhost <span class="strong strong"><strong>container-server: Replication run OVER</strong></span>
Mar 29 08:49:13 localhost <span class="strong strong"><strong>account-server: Replication run OVER</strong></span></pre></li><li class="listitem"><p class="simpara">
							To remove the old node from the ring, reduce the <code class="literal">ObjectStorageCount</code> parameter to omit the old node. In this example, reduce the <code class="literal">ObjectStorageCount</code> parameter to <code class="literal">3</code>:
						</p><pre class="screen">parameter_defaults:
  ObjectStorageCount: 3</pre></li><li class="listitem"><p class="simpara">
							Create a new environment file named <code class="literal">remove-object-node.yaml</code>. This file identifies and removes the specified Object Storage node. The following content specifies the removal of <code class="literal">overcloud-objectstorage-1</code>:
						</p><pre class="screen">parameter_defaults:
  ObjectStorageRemovalPolicies:
    [{'resource_list': ['1']}]</pre></li><li class="listitem"><p class="simpara">
							Include both the <code class="literal">node-info.yaml</code> and <code class="literal">remove-object-node.yaml</code> files in the deployment command:
						</p><pre class="literallayout">(undercloud) $ openstack overcloud deploy --templates -e node-info.yaml <span class="emphasis"><em>ENVIRONMENT_FILES</em></span> -e remove-object-node.yaml</pre></li></ol></div><p>
					Director deletes the Object Storage node from the overcloud and updates the rest of the nodes on the overcloud to accommodate the node removal.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Include all environment files and options from your initial overcloud creation. This includes the same scale parameters for non-Compute nodes.
					</p></div></div></section><section class="section" id="blacklisting-nodes"><div class="titlepage"><div><div><h2 class="title">16.6. Blacklisting nodes</h2></div></div></div><p>
					You can exclude overcloud nodes from receiving an updated deployment. This is useful in scenarios where you want to scale new nodes and exclude existing nodes from receiving an updated set of parameters and resources from the core heat template collection. This means that the blacklisted nodes are isolated from the effects of the stack operation.
				</p><p>
					Use the <code class="literal">DeploymentServerBlacklist</code> parameter in an environment file to create a blacklist.
				</p><div class="formalpara"><p class="title"><strong>Setting the blacklist</strong></p><p>
						The <code class="literal">DeploymentServerBlacklist</code> parameter is a list of server names. Write a new environment file, or add the parameter value to an existing custom environment file and pass the file to the deployment command:
					</p></div><pre class="screen">parameter_defaults:
  DeploymentServerBlacklist:
    - overcloud-compute-0
    - overcloud-compute-1
    - overcloud-compute-2</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The server names in the parameter value are the names according to OpenStack Orchestration (heat), not the actual server hostnames.
					</p></div></div><p>
					Include this environment file with your <code class="literal">openstack overcloud deploy</code> command:
				</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack overcloud deploy --templates \
  -e server-blacklist.yaml \
  [OTHER OPTIONS]</pre><p>
					Heat blacklists any servers in the list from receiving updated heat deployments. After the stack operation completes, any blacklisted servers remain unchanged. You can also power off or stop the <code class="literal">os-collect-config</code> agents during the operation.
				</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Exercise caution when you blacklist nodes. Only use a blacklist if you fully understand how to apply the requested change with a blacklist in effect. It is possible to create a hung stack or configure the overcloud incorrectly when you use the blacklist feature. For example, if cluster configuration changes apply to all members of a Pacemaker cluster, blacklisting a Pacemaker cluster member during this change can cause the cluster to fail.
							</li><li class="listitem">
								Do not use the blacklist during update or upgrade procedures. Those procedures have their own methods for isolating changes to particular servers. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/upgrading_red_hat_openstack_platform/index">Upgrading Red Hat OpenStack Platform</a> guide.
							</li><li class="listitem">
								When you add servers to the blacklist, further changes to those nodes are not supported until you remove the server from the blacklist. This includes updates, upgrades, scale up, scale down, and node replacement. For example, when you blacklist existing Compute nodes while scaling out the overcloud with new Compute nodes, the blacklisted nodes miss the information added to <code class="literal">/etc/hosts</code> and <code class="literal">/etc/ssh/ssh_known_hosts</code>. This can cause live migration to fail, depending on the destination host. The Compute nodes are updated with the information added to <code class="literal">/etc/hosts</code> and <code class="literal">/etc/ssh/ssh_known_hosts</code> during the next overcloud deployment where they are no longer blacklisted.
							</li></ul></div></div></div><div class="formalpara"><p class="title"><strong>Clearing the blacklist</strong></p><p>
						To clear the blacklist for subsequent stack operations, edit the <code class="literal">DeploymentServerBlacklist</code> to use an empty array:
					</p></div><pre class="screen">parameter_defaults:
  DeploymentServerBlacklist: []</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
						Do not omit the <code class="literal">DeploymentServerBlacklist</code> parameter. If you omit the parameter, the overcloud deployment uses the previously saved value.
					</p></div></div></section></section><section class="chapter" id="replacing-controller-nodes"><div class="titlepage"><div><div><h2 class="title">Chapter 17. Replacing Controller nodes</h2></div></div></div><p>
				In certain circumstances a Controller node in a high availability cluster might fail. In these situations, you must remove the node from the cluster and replace it with a new Controller node.
			</p><p>
				Complete the steps in this section to replace a Controller node. The Controller node replacement process involves running the <code class="literal">openstack overcloud deploy</code> command to update the overcloud with a request to replace a Controller node.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The following procedure applies only to high availability environments. Do not use this procedure if you are using only one Controller node.
				</p></div></div><section class="section" id="preparing-for-controller-replacement"><div class="titlepage"><div><div><h2 class="title">17.1. Preparing for Controller replacement</h2></div></div></div><p>
					Before you replace an overcloud Controller node, it is important to check the current state of your Red Hat OpenStack Platform environment. Checking the current state can help avoid complications during the Controller replacement process. Use the following list of preliminary checks to determine if it is safe to perform a Controller node replacement. Run all commands for these checks on the undercloud.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check the current status of the <code class="literal">overcloud</code> stack on the undercloud:
						</p><pre class="screen">$ source stackrc
(undercloud) $ openstack stack list --nested</pre><p class="simpara">
							The <code class="literal">overcloud</code> stack and its subsequent child stacks should have either a <code class="literal">CREATE_COMPLETE</code> or <code class="literal">UPDATE_COMPLETE</code>.
						</p></li><li class="listitem"><p class="simpara">
							Install the database client tools:
						</p><pre class="screen">(undercloud) $ sudo dnf -y install mariadb</pre></li><li class="listitem"><p class="simpara">
							Configure root user access to the database:
						</p><pre class="screen">(undercloud) $ sudo cp /var/lib/config-data/puppet-generated/mysql/root/.my.cnf /root/.</pre></li><li class="listitem"><p class="simpara">
							Perform a backup of the undercloud databases:
						</p><pre class="screen">(undercloud) $ mkdir /home/stack/backup
(undercloud) $ sudo mysqldump --all-databases --quick --single-transaction | gzip &gt; /home/stack/backup/dump_db_undercloud.sql.gz</pre></li><li class="listitem"><p class="simpara">
							Check that your undercloud contains 10 GB free storage to accommodate for image caching and conversion when you provision the new node:
						</p><pre class="screen">(undercloud) $ df -h</pre></li><li class="listitem"><p class="simpara">
							Check the status of Pacemaker on the running Controller nodes. For example, if 192.168.0.47 is the IP address of a running Controller node, use the following command to view the Pacemaker status:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 'sudo pcs status'</pre><p class="simpara">
							The output shows all services that are running on the existing nodes and that are stopped on the failed node.
						</p></li><li class="listitem"><p class="simpara">
							Check the following parameters on each node of the overcloud MariaDB cluster:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">wsrep_local_state_comment: Synced</code>
								</li><li class="listitem"><p class="simpara">
									<code class="literal">wsrep_cluster_size: 2</code>
								</p><p class="simpara">
									Use the following command to check these parameters on each running Controller node. In this example, the Controller node IP addresses are 192.168.0.47 and 192.168.0.46:
								</p><pre class="screen">(undercloud) $ for i in 192.168.24.6 192.168.24.7 ; do echo "*** $i ***" ; ssh heat-admin@$i "sudo podman exec \$(sudo podman ps --filter name=galera-bundle -q) mysql -e \"SHOW STATUS LIKE 'wsrep_local_state_comment'; SHOW STATUS LIKE 'wsrep_cluster_size';\""; done</pre></li></ul></div></li><li class="listitem"><p class="simpara">
							Check the RabbitMQ status. For example, if 192.168.0.47 is the IP address of a running Controller node, use the following command to view the RabbitMQ status:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo podman exec \$(sudo podman ps -f name=rabbitmq-bundle -q) rabbitmqctl cluster_status"</pre><p class="simpara">
							The <code class="literal">running_nodes</code> key should show only the two available nodes and not the failed node.
						</p></li><li class="listitem"><p class="simpara">
							If you are using Open Virtual Switch (OVS) and replaced Controller nodes in the past without restarting the OVS agents, then restart the agents on the compute nodes before replacing this Controller. Restarting the OVS agents ensures that they have a full complement of RabbitMQ connections.
						</p><p class="simpara">
							Run the following command to restart the OVS agent:
						</p><pre class="screen">[heat-admin@overcloud-compute-0 ~]$ sudo systemctl restart tripleo_neutron_ovs_agent</pre></li><li class="listitem"><p class="simpara">
							If fencing is enabled, disable it. For example, if 192.168.0.47 is the IP address of a running Controller node, use the following command to check the status of fencing:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs property show stonith-enabled"</pre><p class="simpara">
							Run the following command to disable fencing:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs property set stonith-enabled=false"</pre></li><li class="listitem"><p class="simpara">
							Check the Compute services are active on the director node:
						</p><pre class="screen">(undercloud) $ openstack hypervisor list</pre><p class="simpara">
							The output should show all non-maintenance mode nodes as <code class="literal">up</code>.
						</p></li><li class="listitem"><p class="simpara">
							Ensure all undercloud containers are running:
						</p><pre class="screen">(undercloud) $ sudo podman ps</pre></li></ol></div></section><section class="section" id="removing-a-ceph-monitor-daemon"><div class="titlepage"><div><div><h2 class="title">17.2. Removing a Ceph Monitor daemon</h2></div></div></div><p>
					If your Controller node is running a Ceph monitor service, complete the following steps to remove the <code class="literal">ceph-mon</code> daemon..
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Adding a new Controller node to the cluster also adds a new Ceph monitor daemon automatically.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Connect to the Controller node that you want to replace and become the root user:
						</p><pre class="screen"># ssh heat-admin@192.168.0.47
# sudo su -</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If the Controller node is unreachable, skip steps 1 and 2 and continue the procedure at step 3 on any working Controller node.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Stop the monitor:
						</p><pre class="screen"># systemctl stop ceph-mon@&lt;monitor_hostname&gt;</pre><p class="simpara">
							For example:
						</p><pre class="screen"># systemctl stop ceph-mon@overcloud-controller-1</pre></li><li class="listitem">
							Disconnect from the Controller node that you want to replace.
						</li><li class="listitem"><p class="simpara">
							Connect to one of the existing Controller nodes.
						</p><pre class="screen"># ssh heat-admin@192.168.0.46
# sudo su -</pre></li><li class="listitem"><p class="simpara">
							Remove the monitor from the cluster:
						</p><pre class="screen"># sudo podman exec -it ceph-mon-controller-0 ceph mon remove overcloud-controller-1</pre></li><li class="listitem"><p class="simpara">
							On all Controller nodes, remove the v1 and v2 monitor entries from <code class="literal">/etc/ceph/ceph.conf</code>. For example, if you remove controller-1, then remove the IPs and hostname for controller-1.
						</p><p class="simpara">
							Before:
						</p><pre class="screen">mon host = [v2:172.18.0.21:3300,v1:172.18.0.21:6789],[v2:172.18.0.22:3300,v1:172.18.0.22:6789],[v2:172.18.0.24:3300,v1:172.18.0.24:6789]
mon initial members = overcloud-controller-2,overcloud-controller-1,overcloud-controller-0</pre><p class="simpara">
							After:
						</p><pre class="screen">mon host = [v2:172.18.0.21:3300,v1:172.18.0.21:6789],[v2:172.18.0.24:3300,v1:172.18.0.24:6789]
mon initial members = overcloud-controller-2,overcloud-controller-0</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Director updates the <code class="literal">ceph.conf</code> file on the relevant overcloud nodes when you add the replacement Controller node. Normally, director manages this configuration file exclusively and you should not edit the file manually. However, you can edit the file manually if you want to ensure consistency in case the other nodes restart before you add the new node.
							</p></div></div></li><li class="listitem"><p class="simpara">
							(Optional) Archive the monitor data and save the archive on another server:
						</p><pre class="screen"># mv /var/lib/ceph/mon/&lt;cluster&gt;-&lt;daemon_id&gt; /var/lib/ceph/mon/removed-&lt;cluster&gt;-&lt;daemon_id&gt;</pre></li></ol></div></section><section class="section" id="preparing-the-cluster-for-controller-replacement"><div class="titlepage"><div><div><h2 class="title">17.3. Preparing the cluster for Controller node replacement</h2></div></div></div><p>
					Before you replace the old node, you must ensure that Pacemaker is not running on the node and then remove that node from the Pacemaker cluster.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the list of IP addresses for the Controller nodes, run the following command:
						</p><pre class="screen">(undercloud) $ openstack server list -c Name -c Networks
+------------------------+-----------------------+
| Name                   | Networks              |
+------------------------+-----------------------+
| overcloud-compute-0    | ctlplane=192.168.0.44 |
| overcloud-controller-0 | ctlplane=192.168.0.47 |
| overcloud-controller-1 | ctlplane=192.168.0.45 |
| overcloud-controller-2 | ctlplane=192.168.0.46 |
+------------------------+-----------------------+</pre></li><li class="listitem"><p class="simpara">
							If the old node is still reachable, log in to one of the remaining nodes and stop pacemaker on the old node. For this example, stop pacemaker on overcloud-controller-1:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs status | grep -w Online | grep -w overcloud-controller-1"
(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs cluster stop overcloud-controller-1"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In case the old node is physically unavailable or stopped, it is not necessary to perform the previous operation, as pacemaker is already stopped on that node.
							</p></div></div></li><li class="listitem"><p class="simpara">
							After you stop Pacemaker on the old node, delete the old node from the pacemaker cluster. The following example command logs in to <code class="literal">overcloud-controller-0</code> to remove <code class="literal">overcloud-controller-1</code>:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs cluster node remove overcloud-controller-1"</pre><p class="simpara">
							If the node that that you want to replace is unreachable (for example, due to a hardware failure), run the <code class="literal">pcs</code> command with additional <code class="literal">--skip-offline</code> and <code class="literal">--force</code> options to forcibly remove the node from the cluster:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs cluster node remove overcloud-controller-1 --skip-offline --force"</pre></li><li class="listitem"><p class="simpara">
							After you remove the old node from the pacemaker cluster, remove the node from the list of known hosts in pacemaker:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs host deauth overcloud-controller-1"</pre><p class="simpara">
							You can run this command whether the node is reachable or not.
						</p></li><li class="listitem"><p class="simpara">
							The overcloud database must continue to run during the replacement procedure. To ensure that Pacemaker does not stop Galera during this procedure, select a running Controller node and run the following command on the undercloud with the IP address of the Controller node:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.0.47 "sudo pcs resource unmanage galera-bundle"</pre></li></ol></div></section><section class="section" id="replacing-a-controller-node"><div class="titlepage"><div><div><h2 class="title">17.4. Replacing a Controller node</h2></div></div></div><p>
					To replace a Controller node, identify the index of the node that you want to replace.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If the node is a virtual node, identify the node that contains the failed disk and restore the disk from a backup. Ensure that the MAC address of the NIC used for PXE boot on the failed server remains the same after disk replacement.
						</li><li class="listitem">
							If the node is a bare metal node, replace the disk, prepare the new disk with your overcloud configuration, and perform a node introspection on the new hardware.
						</li><li class="listitem">
							If the node is a part of a high availability cluster with fencing, you might need recover the Galera nodes separately. For more information, see the article <a class="link" href="https://access.redhat.com/solutions/3215501">How Galera works and how to rescue Galera clusters in the context of Red Hat OpenStack Platform</a>.
						</li></ul></div><p>
					Complete the following example steps to replace the the <code class="literal">overcloud-controller-1</code> node with the <code class="literal">overcloud-controller-3</code> node. The <code class="literal">overcloud-controller-3</code> node has the ID <code class="literal">75b25e9a-948d-424a-9b3b-f0ef70a6eacf</code>.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						To replace the node with an existing bare metal node, enable maintenance mode on the outgoing node so that the director does not automatically reprovision the node.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Replacement of an overcloud Controller might cause swift rings to become inconsistent across nodes. This can result in decreased availability of Object Storage service. This is a known issue. If this happens, log in to the previously existing Controller node using SSH, deploy the updated rings, and restart the Object Storage containers:
					</p></div></div><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ source stackrc
(undercloud) [stack@undercloud-0 ~]$ nova list
...
| 3fab687e-99c2-4e66-805f-3106fb41d868 | controller-1 | ACTIVE | -          | Running     | ctlplane=192.168.24.17 |
| a87276ea-8682-4f27-9426-6b272955b486 | controller-2 | ACTIVE | -          | Running     | ctlplane=192.168.24.38 |
| a000b156-9adc-4d37-8169-c1af7800788b | controller-3 | ACTIVE | -          | Running     | ctlplane=192.168.24.35 |
...

(undercloud) [stack@undercloud-0 ~]$ for ip in 192.168.24.17 192.168.24.38 192.168.24.35; do ssh $ip 'sudo podman restart swift_copy_rings ; sudo podman restart $(sudo podman ps -a --format="{{.Names}}" --filter="name=swift_*")'; done</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Identify the index of the <code class="literal">overcloud-controller-1</code> node:
						</p><pre class="screen">$ INSTANCE=$(openstack server list --name overcloud-controller-1 -f value -c ID)</pre></li><li class="listitem"><p class="simpara">
							Identify the bare metal node associated with the instance:
						</p><pre class="screen">$ NODE=$(openstack baremetal node list -f csv --quote minimal | grep $INSTANCE | cut -f1 -d,)</pre></li><li class="listitem"><p class="simpara">
							Set the node to maintenance mode:
						</p><pre class="screen">$ openstack baremetal node maintenance set $NODE</pre></li><li class="listitem"><p class="simpara">
							If the Controller node is a virtual node, run the following command on the Controller host to replace the virtual disk from a backup:
						</p><pre class="screen">$ cp &lt;VIRTUAL_DISK_BACKUP&gt; /var/lib/libvirt/images/&lt;VIRTUAL_DISK&gt;</pre><p class="simpara">
							Replace <code class="literal">&lt;VIRTUAL_DISK_BACKUP&gt;</code> with the path to the backup of the failed virtual disk, and replace <code class="literal">&lt;VIRTUAL_DISK&gt;</code> with the name of the virtual disk that you want to replace.
						</p><p class="simpara">
							If you do not have a backup of the outgoing node, you must use a new virtualized node.
						</p><p class="simpara">
							If the Controller node is a bare metal node, complete the following steps to replace the disk with a new bare metal disk:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Replace the physical hard drive or solid state drive.
								</li><li class="listitem">
									Prepare the node with the same configuration as the failed node.
								</li></ol></div></li><li class="listitem"><p class="simpara">
							List unassociated nodes and identify the ID of the new node:
						</p><pre class="screen">$ openstack baremetal node list --unassociated</pre></li><li class="listitem"><p class="simpara">
							Tag the new node with the <code class="literal">control</code> profile:
						</p><pre class="screen">(undercloud) $ openstack baremetal node set --property capabilities='profile:control,boot_option:local' 75b25e9a-948d-424a-9b3b-f0ef70a6eacf</pre></li></ol></div></section><section class="section" id="triggering-the-rode-replacement"><div class="titlepage"><div><div><h2 class="title">17.5. Triggering the Controller node replacement</h2></div></div></div><p>
					Complete the following steps to remove the old Controller node and replace it with a new Controller node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Determine the UUID of the node that you want to remove and store it in the <code class="literal">NODEID</code> variable. Ensure that you replace <span class="emphasis"><em>NODE_NAME</em></span> with the name of the node that you want to remove:
						</p><pre class="screen">$ NODEID=$(openstack server list -f value -c ID --name <span class="emphasis"><em>NODE_NAME</em></span>)</pre></li><li class="listitem"><p class="simpara">
							To identify the Heat resource ID, enter the following command:
						</p><pre class="screen">$ openstack stack resource show overcloud ControllerServers -f json -c attributes | jq --arg NODEID "$NODEID" -c <span class="emphasis"><em>.attributes.value | keys[] as $k | if .[$k] == $NODEID then "Node index \($k) for \(.[$k])" else empty end</em></span></pre></li><li class="listitem"><p class="simpara">
							Create the following environment file <code class="literal">~/templates/remove-controller.yaml</code> and include the node index of the Controller node that you want to remove:
						</p><pre class="screen">parameters:
  ControllerRemovalPolicies:
    [{<span class="emphasis"><em>resource_list</em></span>: [<span class="emphasis"><em><span class="emphasis"><em>NODE_INDEX</em></span></em></span>]}]</pre></li><li class="listitem"><p class="simpara">
							Enter the overcloud deployment command, and include the <code class="literal">remove-controller.yaml</code> environment file with any other environment files relevant to your environment:
						</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
    -e /home/stack/templates/remove-controller.yaml \
    [OTHER OPTIONS]</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Include <code class="literal">-e ~/templates/remove-controller.yaml</code> only for this instance of the deployment command. Remove this environment file from subsequent deployment operations.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Director removes the old node, creates a new node, and updates the overcloud stack. You can check the status of the overcloud stack with the following command:
						</p><pre class="screen">(undercloud) $ openstack stack list --nested</pre></li><li class="listitem"><p class="simpara">
							When the deployment command completes, director shows that the old node is replaced with the new node:
						</p><pre class="screen">(undercloud) $ openstack server list -c Name -c Networks
+------------------------+-----------------------+
| Name                   | Networks              |
+------------------------+-----------------------+
| overcloud-compute-0    | ctlplane=192.168.0.44 |
| overcloud-controller-0 | ctlplane=192.168.0.47 |
| overcloud-controller-2 | ctlplane=192.168.0.46 |
| overcloud-controller-3 | ctlplane=192.168.0.48 |
+------------------------+-----------------------+</pre><p class="simpara">
							The new node now hosts running control plane services.
						</p></li></ol></div></section><section class="section" id="cleaning-up-after-the-controller-node-replacement"><div class="titlepage"><div><div><h2 class="title">17.6. Cleaning up after Controller node replacement</h2></div></div></div><p>
					After you complete the node replacement, complete the following steps to finalize the Controller cluster.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log into a Controller node.
						</li><li class="listitem"><p class="simpara">
							Enable Pacemaker management of the Galera cluster and start Galera on the new node:
						</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo pcs resource refresh galera-bundle
[heat-admin@overcloud-controller-0 ~]$ sudo pcs resource manage galera-bundle</pre></li><li class="listitem"><p class="simpara">
							Perform a final status check to ensure that the services are running correctly:
						</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo pcs status</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If any services have failed, use the <code class="literal">pcs resource refresh</code> command to resolve and restart the failed services.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Exit to director:
						</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ exit</pre></li><li class="listitem"><p class="simpara">
							Source the <code class="literal">overcloudrc</code> file so that you can interact with the overcloud:
						</p><pre class="screen">$ source ~/overcloudrc</pre></li><li class="listitem"><p class="simpara">
							Check the network agents in your overcloud environment:
						</p><pre class="screen">(overcloud) $ openstack network agent list</pre></li><li class="listitem"><p class="simpara">
							If any agents appear for the old node, remove them:
						</p><pre class="screen">(overcloud) $ for AGENT in $(openstack network agent list --host overcloud-controller-1.localdomain -c ID -f value) ; do openstack network agent delete $AGENT ; done</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								In an ML2/OVN deployment, bugs prevent removal of the OVN controller and metadata agents. To track progress on these bugs, see <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1828889">BZ1828889</a> and <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1738554">BZ1738554</a>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							If necessary, add your router to the L3 agent host on the new node. Use the following example command to add a router named <code class="literal">r1</code> to the L3 agent using the UUID 2d1c1dc1-d9d4-4fa9-b2c8-f29cd1a649d4:
						</p><pre class="screen">(overcloud) $ openstack network agent add router --l3 2d1c1dc1-d9d4-4fa9-b2c8-f29cd1a649d4 r1</pre></li><li class="listitem"><p class="simpara">
							Because compute services for the removed node still exist in the overcloud, you must remove them. First, check the compute services for the removed node:
						</p><pre class="screen">[stack@director ~]$ source ~/overcloudrc
(overcloud) $ openstack compute service list --host overcloud-controller-1.localdomain</pre></li><li class="listitem"><p class="simpara">
							Remove the compute services for the removed node:
						</p><pre class="screen">(overcloud) $ for SERVICE in $(openstack compute service list --host overcloud-controller-1.localdomain -c ID -f value ) ; do openstack compute service delete $SERVICE ; done</pre></li><li class="listitem"><p class="simpara">
							If you are using Open Virtual Switch (OVS), and the IP address for the Controller node has changed, then you must restart the OVS agent on all compute nodes:
						</p><pre class="screen">[heat-admin@overcloud-compute-0 ~]$ sudo systemctl restart tripleo_neutron_ovs_agent</pre></li></ol></div></section></section><section class="chapter" id="rebooting-nodes"><div class="titlepage"><div><div><h2 class="title">Chapter 18. Rebooting nodes</h2></div></div></div><p>
				You might need to reboot the nodes in the undercloud and overcloud. Use the following procedures to understand how to reboot different node types.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you reboot all nodes in one role, it is advisable to reboot each node individually. If you reboot all nodes in a role simultaneously, service downtime can occurduring the reboot operation.
					</li><li class="listitem">
						If you reboot all nodes in your OpenStack Platform environment, reboot the nodes in the following sequential order:
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Recommended node reboot order</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Reboot the undercloud node.
					</li><li class="listitem">
						Reboot Controller and other composable nodes.
					</li><li class="listitem">
						Reboot standalone Ceph MON nodes.
					</li><li class="listitem">
						Reboot Ceph Storage nodes.
					</li><li class="listitem">
						Reboot Compute nodes.
					</li></ol></div><section class="section" id="rebooting_the_undercloud"><div class="titlepage"><div><div><h2 class="title">18.1. Rebooting the undercloud node</h2></div></div></div><p>
					Complete the following steps to reboot the undercloud node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							Reboot the undercloud:
						</p><pre class="screen">$ sudo reboot</pre></li><li class="listitem">
							Wait until the node boots.
						</li></ol></div></section><section class="section" id="rebooting_controller_nodes"><div class="titlepage"><div><div><h2 class="title">18.2. Rebooting Controller and composable nodes</h2></div></div></div><p>
					Complete the following steps to reboot Controller nodes and standalone nodes based on composable roles, excluding Compute nodes and Ceph Storage nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the node that you want to reboot.
						</li><li class="listitem"><p class="simpara">
							Optional: If the node uses Pacemaker resources, stop the cluster:
						</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo pcs cluster stop</pre></li><li class="listitem"><p class="simpara">
							Reboot the node:
						</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo reboot</pre></li><li class="listitem">
							Wait until the node boots.
						</li><li class="listitem"><p class="simpara">
							Check the services. For example:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									If the node uses Pacemaker services, check that the node has rejoined the cluster:
								</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo pcs status</pre></li><li class="listitem"><p class="simpara">
									If the node uses Systemd services, check that all services are enabled:
								</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo systemctl status</pre></li><li class="listitem"><p class="simpara">
									If the node uses containerized services, check that all containers on the node are active:
								</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman ps</pre></li></ol></div></li></ol></div></section><section class="section" id="rebooting_standalone_ceph_mon_nodes"><div class="titlepage"><div><div><h2 class="title">18.3. Rebooting standalone Ceph MON nodes</h2></div></div></div><p>
					Complete the following steps to reboot standalone Ceph MON nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to a Ceph MON node.
						</li><li class="listitem"><p class="simpara">
							Reboot the node:
						</p><pre class="screen">$ sudo reboot</pre></li><li class="listitem">
							Wait until the node boots and rejoins the MON cluster.
						</li></ol></div><p>
					Repeat these steps for each MON node in the cluster.
				</p></section><section class="section" id="rebooting_a_ceph_storage_cluster"><div class="titlepage"><div><div><h2 class="title">18.4. Rebooting a Ceph Storage (OSD) cluster</h2></div></div></div><p>
					Complete the following steps to reboot a cluster of Ceph Storage (OSD) nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Log into a Ceph MON or Controller node and disable Ceph Storage cluster rebalancing temporarily:
						</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph osd set noout
$ sudo podman exec -it ceph-mon-controller-0 ceph osd set norebalance</pre></li><li class="listitem">
							Select the first Ceph Storage node that you want to reboot and log in to the node.
						</li><li class="listitem"><p class="simpara">
							Reboot the node:
						</p><pre class="screen">$ sudo reboot</pre></li><li class="listitem">
							Wait until the node boots.
						</li><li class="listitem"><p class="simpara">
							Log into the node and check the cluster status:
						</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph status</pre><p class="simpara">
							Check that the <code class="literal">pgmap</code> reports all <code class="literal">pgs</code> as normal (<code class="literal">active+clean</code>).
						</p></li><li class="listitem">
							Log out of the node, reboot the next node, and check its status. Repeat this process until you have rebooted all Ceph storage nodes.
						</li><li class="listitem"><p class="simpara">
							When complete, log into a Ceph MON or Controller node and re-enable cluster rebalancing:
						</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph osd unset noout
$ sudo podman exec -it ceph-mon-controller-0 ceph osd unset norebalance</pre></li><li class="listitem"><p class="simpara">
							Perform a final status check to verify that the cluster reports <code class="literal">HEALTH_OK</code>:
						</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph status</pre></li></ol></div></section><section class="section" id="rebooting_compute_nodes"><div class="titlepage"><div><div><h2 class="title">18.5. Rebooting Compute nodes</h2></div></div></div><p>
					Complete the following steps to reboot Compute nodes. To ensure minimal downtime of instances in your Red Hat OpenStack Platform environment, this procedure also includes instructions about migrating instances from the Compute node that you want to reboot. This involves the following workflow:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Decide whether to migrate instances to another Compute node before rebooting the node.
						</li><li class="listitem">
							Select and disable the Compute node you want to reboot so that it does not provision new instances.
						</li><li class="listitem">
							Migrate the instances to another Compute node.
						</li><li class="listitem">
							Reboot the empty Compute node.
						</li><li class="listitem">
							Enable the empty Compute node.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Prerequisites</strong></p><p>
						Before you reboot the Compute node, you must decide whether to migrate instances to another Compute node while the node is rebooting.
					</p></div><p>
					If for some reason you cannot or do not want to migrate the instances, you can set the following core template parameters to control the state of the instances after the Compute node reboots:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">NovaResumeGuestsStateOnHostBoot</code></span></dt><dd>
								Determines whether to return instances to the same state on the Compute node after reboot. When set to <code class="literal">False</code>, the instances remain down and you must start them manually. Default value is: <code class="literal">False</code>
							</dd><dt><span class="term"><code class="literal">NovaResumeGuestsShutdownTimeout</code></span></dt><dd>
								Number of seconds to wait for an instance to shut down before rebooting. It is not recommended to set this value to <code class="literal">0</code>. Default value is: 300
							</dd></dl></div><p>
					For more information about overcloud parameters and their usage, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/overcloud_parameters/index#compute-nova-parameters">Overcloud Parameters</a>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log in to the undercloud as the <code class="literal">stack</code> user.
						</li><li class="listitem"><p class="simpara">
							List all Compute nodes and their UUIDs:
						</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack server list --name compute</pre><p class="simpara">
							Identify the UUID of the Compute node that you want to reboot.
						</p></li><li class="listitem"><p class="simpara">
							From the undercloud, select a Compute node. Disable the node:
						</p><pre class="screen">$ source ~/overcloudrc
(overcloud) $ openstack compute service list
(overcloud) $ openstack compute service set [hostname] nova-compute --disable</pre></li><li class="listitem"><p class="simpara">
							List all instances on the Compute node:
						</p><pre class="screen">(overcloud) $ openstack server list --host [hostname] --all-projects</pre></li><li class="listitem">
							If you decide not to migrate instances, skip to <a class="link" href="index.html#nomigration">this step</a>.
						</li><li class="listitem"><p class="simpara">
							If you decide to migrate the instances to another Compute node, use one of the following commands:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Migrate the instance to a different host:
								</p><pre class="screen">(overcloud) $ openstack server migrate [instance-id] --live [target-host]--wait</pre></li><li class="listitem"><p class="simpara">
									Let <code class="literal">nova-scheduler</code> automatically select the target host:
								</p><pre class="screen">(overcloud) $ nova live-migration [instance-id]</pre></li><li class="listitem"><p class="simpara">
									Live migrate all instances at once:
								</p><pre class="screen">$ nova host-evacuate-live [hostname]</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										The <code class="literal">nova</code> command might cause some deprecation warnings, which are safe to ignore.
									</p></div></div></li></ul></div></li><li class="listitem">
							Wait until migration completes.
						</li><li class="listitem"><p class="simpara">
							Confirm that the migration was successful:
						</p><pre class="screen">(overcloud) $ openstack server list --host [hostname] --all-projects</pre></li><li class="listitem">
							Continue to migrate instances until none remain on the chosen Compute node.
						</li><li class="listitem"><p class="simpara">
							<span id="nomigration"><!--Empty--></span>Log in to the Compute node and reboot the node:
						</p><pre class="screen">[heat-admin@overcloud-compute-0 ~]$ sudo reboot</pre></li><li class="listitem">
							Wait until the node boots.
						</li><li class="listitem"><p class="simpara">
							Re-enable the Compute node:
						</p><pre class="screen">$ source ~/overcloudrc
(overcloud) $ openstack compute service set [hostname] nova-compute --enable</pre></li><li class="listitem"><p class="simpara">
							Check that the Compute node is enabled:
						</p><pre class="screen">(overcloud) $ openstack compute service list</pre></li></ol></div></section></section></div><div class="part" id="additional_director_operations_and_configuration"><div class="titlepage"><div><div><h1 class="title">Part IV. Additional director operations and configuration</h1></div></div></div><section class="chapter" id="configuring-custom-ssl-tls-certificates"><div class="titlepage"><div><div><h2 class="title">Chapter 19. Configuring custom SSL/TLS certificates</h2></div></div></div><p>
				You can configure the undercloud to use SSL/TLS for communication over public endpoints. However, if want to you use a SSL certificate with your own certificate authority, you must complete the following configuration steps.
			</p><section class="section" id="initializing-the-signing-host"><div class="titlepage"><div><div><h2 class="title">19.1. Initializing the signing host</h2></div></div></div><p>
					The signing host is the host that generates and signs new certificates with a certificate authority. If you have never created SSL certificates on the chosen signing host, you might need to initialize the host so that it can sign new certificates.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							The <code class="literal">/etc/pki/CA/index.txt</code> file contains records of all signed certificates. Check if this file exists. If it does not exist, create an empty file:
						</p><pre class="screen">$ sudo touch /etc/pki/CA/index.txt</pre></li><li class="listitem"><p class="simpara">
							The <code class="literal">/etc/pki/CA/serial</code> file identifies the next serial number to use for the next certificate to sign. Check if this file exists. If the file does not exist, create a new file with a new starting value:
						</p><pre class="screen">$ echo '1000' | sudo tee /etc/pki/CA/serial</pre></li></ol></div></section><section class="section" id="creating-a-certificate-authority"><div class="titlepage"><div><div><h2 class="title">19.2. Creating a certificate authority</h2></div></div></div><p>
					Normally you sign your SSL/TLS certificates with an external certificate authority. In some situations, you might want to use your own certificate authority. For example, you might want to have an internal-only certificate authority.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Generate a key and certificate pair to act as the certificate authority:
						</li></ol></div><pre class="screen">$ openssl genrsa -out ca.key.pem 4096
$ openssl req  -key ca.key.pem -new -x509 -days 7300 -extensions v3_ca -out ca.crt.pem</pre><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							The <code class="literal">openssl req</code> command requests certain details about your authority. Enter these details at the prompt.
						</li></ol></div><p>
					These commands create a certificate authority file called <code class="literal">ca.crt.pem</code>.
				</p></section><section class="section" id="adding-the-certificate-authority-to-clients"><div class="titlepage"><div><div><h2 class="title">19.3. Adding the certificate authority to clients</h2></div></div></div><p>
					For any external clients aiming to communicate using SSL/TLS, copy the certificate authority file to each client that requires access to your Red Hat OpenStack Platform environment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the certificate authority to the client system:
						</p><pre class="screen">$ sudo cp ca.crt.pem /etc/pki/ca-trust/source/anchors/</pre></li><li class="listitem"><p class="simpara">
							After you copy the certificate authority file to each client, run the following command on each client to add the certificate to the certificate authority trust bundle:
						</p><pre class="screen">$ sudo update-ca-trust extract</pre></li></ol></div></section><section class="section" id="creating-an-ssl-tls-key"><div class="titlepage"><div><div><h2 class="title">19.4. Creating an SSL/TLS key</h2></div></div></div><p>
					Enabling SSL/TLS on an OpenStack environment requires an SSL/TLS key to generate your certificates.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to generate the SSL/TLS key (<code class="literal">server.key.pem</code>):
						</p><pre class="screen">$ openssl genrsa -out server.key.pem 2048</pre></li></ol></div></section><section class="section" id="creating-an-ssl-tls-certificate-signing-request"><div class="titlepage"><div><div><h2 class="title">19.5. Creating an SSL/TLS certificate signing request</h2></div></div></div><p>
					Complete the following steps to create a certificate signing request.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the default OpenSSL configuration file:
						</p><pre class="screen">$ cp /etc/pki/tls/openssl.cnf .</pre></li><li class="listitem"><p class="simpara">
							Edit the new <code class="literal">openssl.cnf</code> file and configure the SSL parameters that you want to use for director. An example of the types of parameters to modify include:
						</p><pre class="screen">[req]
distinguished_name = req_distinguished_name
req_extensions = v3_req

[req_distinguished_name]
countryName = Country Name (2 letter code)
countryName_default = AU
stateOrProvinceName = State or Province Name (full name)
stateOrProvinceName_default = Queensland
localityName = Locality Name (eg, city)
localityName_default = Brisbane
organizationalUnitName = Organizational Unit Name (eg, section)
organizationalUnitName_default = Red Hat
commonName = Common Name
commonName_default = 192.168.0.1
commonName_max = 64

[ v3_req ]
# Extensions to add to a certificate request
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
IP.1 = 192.168.0.1
DNS.1 = instack.localdomain
DNS.2 = vip.localdomain
DNS.3 = 192.168.0.1</pre><p class="simpara">
							Set the <code class="literal">commonName_default</code> to one of the following entries:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you are using an IP address to access director over SSL/TLS, use the <code class="literal">undercloud_public_host</code> parameter in the <code class="literal">undercloud.conf</code> file.
								</li><li class="listitem"><p class="simpara">
									If you are using a fully qualified domain name to access director over SSL/TLS, use the domain name.
								</p><p class="simpara">
									Edit the <code class="literal">alt_names</code> section to include the following entries:
								</p></li><li class="listitem">
									<code class="literal">IP</code> - A list of IP addresses that clients use to access director over SSL.
								</li><li class="listitem">
									<code class="literal">DNS</code> - A list of domain names that clients use to access director over SSL. Also include the Public API IP address as a DNS entry at the end of the <code class="literal">alt_names</code> section.
								</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For more information about <code class="literal">openssl.cnf</code>, run the <code class="literal">man openssl.cnf</code> command.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Run the following command to generate a certificate signing request (<code class="literal">server.csr.pem</code>):
						</p><pre class="screen">$ openssl req -config openssl.cnf -key server.key.pem -new -out server.csr.pem</pre><p class="simpara">
							Ensure that you include your OpenStack SSL/TLS key with the <code class="literal">-key</code> option.
						</p></li></ol></div><p>
					This command generates a <code class="literal">server.csr.pem</code> file, which is the certificate signing request. Use this file to create your OpenStack SSL/TLS certificate.
				</p></section><section class="section" id="creating-the-ssl-tls-certificate"><div class="titlepage"><div><div><h2 class="title">19.6. Creating the SSL/TLS certificate</h2></div></div></div><p>
					To generate the SSL/TLS certificate for your OpenStack environment, the following files must be present:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">openssl.cnf</code></span></dt><dd>
								The customized configuration file that specifies the v3 extensions.
							</dd><dt><span class="term"><code class="literal">server.csr.pem</code></span></dt><dd>
								The certificate signing request to generate and sign the certificate with a certificate authority.
							</dd><dt><span class="term"><code class="literal">ca.crt.pem</code></span></dt><dd>
								The certificate authority, which signs the certificate.
							</dd><dt><span class="term"><code class="literal">ca.key.pem</code></span></dt><dd>
								The certificate authority private key.
							</dd></dl></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to create a certificate for your undercloud or overcloud:
						</p><pre class="screen">$ sudo openssl ca -config openssl.cnf -extensions v3_req -days 3650 -in server.csr.pem -out server.crt.pem -cert ca.crt.pem -keyfile ca.key.pem</pre><p class="simpara">
							This command uses the following options:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">-config</code></span></dt><dd>
										Use a custom configuration file, which is the <code class="literal">openssl.cnf</code> file with v3 extensions.
									</dd><dt><span class="term"><code class="literal">-extensions v3_req</code></span></dt><dd>
										Enabled v3 extensions.
									</dd><dt><span class="term"><code class="literal">-days</code></span></dt><dd>
										Defines how long in days until the certificate expires.
									</dd><dt><span class="term"><code class="literal">-in</code>'</span></dt><dd>
										The certificate signing request.
									</dd><dt><span class="term"><code class="literal">-out</code></span></dt><dd>
										The resulting signed certificate.
									</dd><dt><span class="term"><code class="literal">-cert</code></span></dt><dd>
										The certificate authority file.
									</dd><dt><span class="term"><code class="literal">-keyfile</code></span></dt><dd>
										The certificate authority private key.
									</dd></dl></div></li></ol></div><p>
					This command creates a new certificate named <code class="literal">server.crt.pem</code>. Use this certificate in conjunction with your OpenStack SSL/TLS key
				</p></section><section class="section" id="adding-the-certificate-to-the-undercloud"><div class="titlepage"><div><div><h2 class="title">19.7. Adding the certificate to the undercloud</h2></div></div></div><p>
					Complete the following steps to add your OpenStack SSL/TLS certificate to the undercloud trust bundle.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the following command to combine the certificate and key:
						</p><pre class="screen">$ cat server.crt.pem server.key.pem &gt; undercloud.pem</pre><p class="simpara">
							This command creates a <code class="literal">undercloud.pem</code> file.
						</p></li><li class="listitem"><p class="simpara">
							Copy the <code class="literal">undercloud.pem</code> file to a location within your <code class="literal">/etc/pki</code> directory and set the necessary SELinux context so that HAProxy can read it:
						</p><pre class="screen">$ sudo mkdir /etc/pki/undercloud-certs
$ sudo cp ~/undercloud.pem /etc/pki/undercloud-certs/.
$ sudo semanage fcontext -a -t etc_t "/etc/pki/undercloud-certs(/.*)?"
$ sudo restorecon -R /etc/pki/undercloud-certs</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">undercloud.pem</code> file location to the <code class="literal">undercloud_service_certificate</code> option in the <code class="literal">undercloud.conf</code> file:
						</p><pre class="screen">undercloud_service_certificate = /etc/pki/undercloud-certs/undercloud.pem</pre></li><li class="listitem"><p class="simpara">
							Add the certificate authority that signed the certificate to the list of trusted Certificate Authorities on the undercloud so that different services within the undercloud have access to the certificate authority:
						</p><pre class="screen">$ sudo cp ca.crt.pem /etc/pki/ca-trust/source/anchors/
$ sudo update-ca-trust extract</pre></li></ol></div></section></section><section class="chapter" id="additional-introspection-operations"><div class="titlepage"><div><div><h2 class="title">Chapter 20. Additional introspection operations</h2></div></div></div><section class="section" id="performing_individual_node_introspection"><div class="titlepage"><div><div><h2 class="title">20.1. Performing individual node introspection</h2></div></div></div><p>
					To perform a single introspection on an available node, run the following commands to set the node to management mode and perform the introspection:
				</p><pre class="screen">(undercloud) $ openstack baremetal node manage [NODE UUID]
(undercloud) $ openstack overcloud node introspect [NODE UUID] --provide</pre><p>
					After the introspection completes, the node changes to an <code class="literal">available</code> state.
				</p></section><section class="section" id="performing_node_introspection_after_initial_introspection"><div class="titlepage"><div><div><h2 class="title">20.2. Performing node introspection after initial introspection</h2></div></div></div><p>
					After an initial introspection, all nodes enter an <code class="literal">available</code> state due to the <code class="literal">--provide</code> option. To perform introspection on all nodes after the initial introspection, set all nodes to a <code class="literal">manageable</code> state and run the bulk introspection command:
				</p><pre class="screen">(undercloud) $ for node in $(openstack baremetal node list --fields uuid -f value) ; do openstack baremetal node manage $node ; done
(undercloud) $ openstack overcloud node introspect --all-manageable --provide</pre><p>
					After the introspection completes, all nodes change to an <code class="literal">available</code> state.
				</p></section><section class="section" id="performing_network_introspection_for_interface_information"><div class="titlepage"><div><div><h2 class="title">20.3. Performing network introspection for interface information</h2></div></div></div><p>
					Network introspection retrieves link layer discovery protocol (LLDP) data from network switches. The following commands show a subset of LLDP information for all interfaces on a node, or full information for a particular node and interface. This can be useful for troubleshooting. Director enables LLDP data collection by default.
				</p><p>
					To get a list of interfaces on a node, run the following command:
				</p><pre class="screen">(undercloud) $ openstack baremetal introspection interface list [NODE UUID]</pre><p>
					For example:
				</p><pre class="screen">(undercloud) $ openstack baremetal introspection interface list c89397b7-a326-41a0-907d-79f8b86c7cd9
+-----------+-------------------+------------------------+-------------------+----------------+
| Interface | MAC Address       | Switch Port VLAN IDs   | Switch Chassis ID | Switch Port ID |
+-----------+-------------------+------------------------+-------------------+----------------+
| p2p2      | 00:0a:f7:79:93:19 | [103, 102, 18, 20, 42] | 64:64:9b:31:12:00 | 510            |
| p2p1      | 00:0a:f7:79:93:18 | [101]                  | 64:64:9b:31:12:00 | 507            |
| em1       | c8:1f:66:c7:e8:2f | [162]                  | 08:81:f4:a6:b3:80 | 515            |
| em2       | c8:1f:66:c7:e8:30 | [182, 183]             | 08:81:f4:a6:b3:80 | 559            |
+-----------+-------------------+------------------------+-------------------+----------------+</pre><p>
					To view interface data and switch port information, run the following command:
				</p><pre class="screen">(undercloud) $ openstack baremetal introspection interface show [NODE UUID] [INTERFACE]</pre><p>
					For example:
				</p><pre class="screen">(undercloud) $ openstack baremetal introspection interface show c89397b7-a326-41a0-907d-79f8b86c7cd9 p2p1
+--------------------------------------+------------------------------------------------------------------------------------------------------------------------+
| Field                                | Value                                                                                                                  |
+--------------------------------------+------------------------------------------------------------------------------------------------------------------------+
| interface                            | p2p1                                                                                                                   |
| mac                                  | 00:0a:f7:79:93:18                                                                                                      |
| node_ident                           | c89397b7-a326-41a0-907d-79f8b86c7cd9                                                                                   |
| switch_capabilities_enabled          | [u'Bridge', u'Router']                                                                                                 |
| switch_capabilities_support          | [u'Bridge', u'Router']                                                                                                 |
| switch_chassis_id                    | 64:64:9b:31:12:00                                                                                                      |
| switch_port_autonegotiation_enabled  | True                                                                                                                   |
| switch_port_autonegotiation_support  | True                                                                                                                   |
| switch_port_description              | ge-0/0/2.0                                                                                                             |
| switch_port_id                       | 507                                                                                                                    |
| switch_port_link_aggregation_enabled | False                                                                                                                  |
| switch_port_link_aggregation_id      | 0                                                                                                                      |
| switch_port_link_aggregation_support | True                                                                                                                   |
| switch_port_management_vlan_id       | None                                                                                                                   |
| switch_port_mau_type                 | Unknown                                                                                                                |
| switch_port_mtu                      | 1514                                                                                                                   |
| switch_port_physical_capabilities    | [u'1000BASE-T fdx', u'100BASE-TX fdx', u'100BASE-TX hdx', u'10BASE-T fdx', u'10BASE-T hdx', u'Asym and Sym PAUSE fdx'] |
| switch_port_protocol_vlan_enabled    | None                                                                                                                   |
| switch_port_protocol_vlan_ids        | None                                                                                                                   |
| switch_port_protocol_vlan_support    | None                                                                                                                   |
| switch_port_untagged_vlan_id         | 101                                                                                                                    |
| switch_port_vlan_ids                 | [101]                                                                                                                  |
| switch_port_vlans                    | [{u'name': u'RHOS13-PXE', u'id': 101}]                                                                                 |
| switch_protocol_identities           | None                                                                                                                   |
| switch_system_name                   | rhos-compute-node-sw1                                                                                                  |
+--------------------------------------+------------------------------------------------------------------------------------------------------------------------+</pre><div id="proc_introspection-numa-topology" class="formalpara"><p class="title"><strong>Retrieving hardware introspection details</strong></p><p>
						The Bare Metal service hardware-inspection-extras feature is enabled by default, and you can use it to retrieve hardware details for overcloud configuration. For more information about the <code class="literal">inspection_extras</code> parameter in the <code class="literal">undercloud.conf</code> file, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/installing-the-undercloud#configuring-the-director">Configuring the Director</a>.
					</p></div><p>
					For example, the <code class="literal">numa_topology</code> collector is part of the hardware-inspection extras and includes the following information for each NUMA node:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							RAM (in kilobytes)
						</li><li class="listitem">
							Physical CPU cores and their sibling threads
						</li><li class="listitem">
							NICs associated with the NUMA node
						</li></ul></div><p>
					To retrieve the information listed above, substitute &lt;UUID&gt; with the UUID of the bare-metal node to complete the following command:
				</p><pre class="screen"># openstack baremetal introspection data save &lt;UUID&gt; | jq .numa_topology</pre><p>
					The following example shows the retrieved NUMA information for a bare-metal node:
				</p><pre class="screen">{
  "cpus": [
    {
      "cpu": 1,
      "thread_siblings": [
        1,
        17
      ],
      "numa_node": 0
    },
    {
      "cpu": 2,
      "thread_siblings": [
        10,
        26
      ],
      "numa_node": 1
    },
    {
      "cpu": 0,
      "thread_siblings": [
        0,
        16
      ],
      "numa_node": 0
    },
    {
      "cpu": 5,
      "thread_siblings": [
        13,
        29
      ],
      "numa_node": 1
    },
    {
      "cpu": 7,
      "thread_siblings": [
        15,
        31
      ],
      "numa_node": 1
    },
    {
      "cpu": 7,
      "thread_siblings": [
        7,
        23
      ],
      "numa_node": 0
    },
    {
      "cpu": 1,
      "thread_siblings": [
        9,
        25
      ],
      "numa_node": 1
    },
    {
      "cpu": 6,
      "thread_siblings": [
        6,
        22
      ],
      "numa_node": 0
    },
    {
      "cpu": 3,
      "thread_siblings": [
        11,
        27
      ],
      "numa_node": 1
    },
    {
      "cpu": 5,
      "thread_siblings": [
        5,
        21
      ],
      "numa_node": 0
    },
    {
      "cpu": 4,
      "thread_siblings": [
        12,
        28
      ],
      "numa_node": 1
    },
    {
      "cpu": 4,
      "thread_siblings": [
        4,
        20
      ],
      "numa_node": 0
    },
    {
      "cpu": 0,
      "thread_siblings": [
        8,
        24
      ],
      "numa_node": 1
    },
    {
      "cpu": 6,
      "thread_siblings": [
        14,
        30
      ],
      "numa_node": 1
    },
    {
      "cpu": 3,
      "thread_siblings": [
        3,
        19
      ],
      "numa_node": 0
    },
    {
      "cpu": 2,
      "thread_siblings": [
        2,
        18
      ],
      "numa_node": 0
    }
  ],
  "ram": [
    {
      "size_kb": 66980172,
      "numa_node": 0
    },
    {
      "size_kb": 67108864,
      "numa_node": 1
    }
  ],
  "nics": [
    {
      "name": "ens3f1",
      "numa_node": 1
    },
    {
      "name": "ens3f0",
      "numa_node": 1
    },
    {
      "name": "ens2f0",
      "numa_node": 0
    },
    {
      "name": "ens2f1",
      "numa_node": 0
    },
    {
      "name": "ens1f1",
      "numa_node": 0
    },
    {
      "name": "ens1f0",
      "numa_node": 0
    },
    {
      "name": "eno4",
      "numa_node": 0
    },
    {
      "name": "eno1",
      "numa_node": 0
    },
    {
      "name": "eno3",
      "numa_node": 0
    },
    {
      "name": "eno2",
      "numa_node": 0
    }
  ]
}</pre></section></section><section class="chapter" id="automatically-discover-bare-metal-nodes"><div class="titlepage"><div><div><h2 class="title">Chapter 21. Automatically discovering bare metal nodes</h2></div></div></div><p>
				You can use auto-discovery to register overcloud nodes and generate their metadata, without the need to create an <code class="literal">instackenv.json</code> file. This improvement can help to reduce the time it takes to collect information about a node. For example, if you use auto-discovery, you do not to collate the IPMI IP addresses and subsequently create the <code class="literal">instackenv.json</code>.
			</p><section class="section" id="prerequisites"><div class="titlepage"><div><div><h2 class="title">21.1. Prerequisites</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You have configured all overcloud nodes BMCs to be accessible to director through the IPMI.
						</li><li class="listitem">
							You have configured all overcloud nodes to PXE boot from the NIC that is connected to the undercloud control plane network.
						</li></ul></div></section><section class="section" id="enabling_auto_discovery"><div class="titlepage"><div><div><h2 class="title">21.2. Enabling auto-discovery</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Enable Bare Metal auto-discovery in the <code class="literal">undercloud.conf</code> file:
						</p><pre class="screen">enable_node_discovery = True
discovery_default_driver = ipmi</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">enable_node_discovery</code> - When enabled, any node that boots the introspection ramdisk using PXE is enrolled in the Bare Metal service (ironic) automatically.
								</li><li class="listitem">
									<code class="literal">discovery_default_driver</code> - Sets the driver to use for discovered nodes. For example, <code class="literal">ipmi</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Add your IPMI credentials to ironic:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Add your IPMI credentials to a file named <code class="literal">ipmi-credentials.json</code>. Replace the <code class="literal">SampleUsername</code>, <code class="literal">RedactedSecurePassword</code>, and <code class="literal">bmc_address</code> values in this example to suit your environment:
								</p><pre class="screen">[
    {
        "description": "Set default IPMI credentials",
        "conditions": [
            {"op": "eq", "field": "data://auto_discovered", "value": true}
        ],
        "actions": [
            {"action": "set-attribute", "path": "driver_info/ipmi_username",
             "value": "SampleUsername"},
            {"action": "set-attribute", "path": "driver_info/ipmi_password",
             "value": "RedactedSecurePassword"},
            {"action": "set-attribute", "path": "driver_info/ipmi_address",
             "value": "{data[inventory][bmc_address]}"}
        ]
    }
]</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Import the IPMI credentials file into ironic:
						</p><pre class="screen">$ openstack baremetal introspection rule import ipmi-credentials.json</pre></li></ol></div></section><section class="section" id="testing_auto_discovery"><div class="titlepage"><div><div><h2 class="title">21.3. Testing auto-discovery</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Power on the required nodes.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack baremetal node list</code> command. You should see the new nodes listed in an <code class="literal">enrolled</code> state:
						</p><pre class="screen">$ openstack baremetal node list
+--------------------------------------+------+---------------+-------------+--------------------+-------------+
| UUID                                 | Name | Instance UUID | Power State | Provisioning State | Maintenance |
+--------------------------------------+------+---------------+-------------+--------------------+-------------+
| c6e63aec-e5ba-4d63-8d37-bd57628258e8 | None | None          | power off   | enroll             | False       |
| 0362b7b2-5b9c-4113-92e1-0b34a2535d9b | None | None          | power off   | enroll             | False       |
+--------------------------------------+------+---------------+-------------+--------------------+-------------+</pre></li><li class="listitem"><p class="simpara">
							Set the resource class for each node:
						</p><pre class="screen">$ for NODE in `openstack baremetal node list -c UUID -f value` ; do openstack baremetal node set $NODE --resource-class baremetal ; done</pre></li><li class="listitem"><p class="simpara">
							Configure the kernel and ramdisk for each node:
						</p><pre class="screen">$ for NODE in `openstack baremetal node list -c UUID -f value` ; do openstack baremetal node manage $NODE ; done
$ openstack overcloud node configure --all-manageable</pre></li><li class="listitem"><p class="simpara">
							Set all nodes to available:
						</p><pre class="screen">$ for NODE in `openstack baremetal node list -c UUID -f value` ; do openstack baremetal node provide $NODE ; done</pre></li></ol></div></section><section class="section" id="using_rules_to_discover_different_vendor_hardware"><div class="titlepage"><div><div><h2 class="title">21.4. Using rules to discover different vendor hardware</h2></div></div></div><p>
					If you have a heterogeneous hardware environment, you can use introspection rules to assign credentials and remote management credentials. For example, you might want a separate discovery rule to handle your Dell nodes that use DRAC:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file named <code class="literal">dell-drac-rules.json</code> with the following contents:
						</p><pre class="screen">[
    {
        "description": "Set default IPMI credentials",
        "conditions": [
            {"op": "eq", "field": "data://auto_discovered", "value": true},
            {"op": "ne", "field": "data://inventory.system_vendor.manufacturer",
             "value": "Dell Inc."}
        ],
        "actions": [
            {"action": "set-attribute", "path": "driver_info/ipmi_username",
             "value": "SampleUsername"},
            {"action": "set-attribute", "path": "driver_info/ipmi_password",
             "value": "RedactedSecurePassword"},
            {"action": "set-attribute", "path": "driver_info/ipmi_address",
             "value": "{data[inventory][bmc_address]}"}
        ]
    },
    {
        "description": "Set the vendor driver for Dell hardware",
        "conditions": [
            {"op": "eq", "field": "data://auto_discovered", "value": true},
            {"op": "eq", "field": "data://inventory.system_vendor.manufacturer",
             "value": "Dell Inc."}
        ],
        "actions": [
            {"action": "set-attribute", "path": "driver", "value": "idrac"},
            {"action": "set-attribute", "path": "driver_info/drac_username",
             "value": "SampleUsername"},
            {"action": "set-attribute", "path": "driver_info/drac_password",
             "value": "RedactedSecurePassword"},
            {"action": "set-attribute", "path": "driver_info/drac_address",
             "value": "{data[inventory][bmc_address]}"}
        ]
    }
]</pre><p class="simpara">
							Replace the user name and password values in this example to suit your environment:
						</p></li><li class="listitem"><p class="simpara">
							Import the rule into ironic:
						</p><pre class="screen">$ openstack baremetal introspection rule import dell-drac-rules.json</pre></li></ol></div></section></section><section class="chapter" id="configuring-automatic-profile-tagging"><div class="titlepage"><div><div><h2 class="title">Chapter 22. Configuring automatic profile tagging</h2></div></div></div><p>
				The introspection process performs a series of benchmark tests. The director saves the data from these tests. You can create a set of policies that use this data in various ways:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The policies can identify underperforming or unstable nodes and isolate these nodes from use in the overcloud.
					</li><li class="listitem">
						The policies can define whether to tag nodes into specific profiles automatically.
					</li></ul></div><section class="section" id="policy-file-syntax"><div class="titlepage"><div><div><h2 class="title">22.1. Policy file syntax</h2></div></div></div><p>
					Policy files use a JSON format that contains a set of rules. Each rule defines a description, a condition, and an action. A <span class="strong strong"><strong>description</strong></span> is a plain text description of the rule, a <span class="strong strong"><strong>condition</strong></span> defines an evaluation using a key-value pattern, and an <span class="strong strong"><strong>action</strong></span> is the performance of the condition.
				</p><div class="formalpara"><p class="title"><strong>Description</strong></p><p>
						A description is a plain text description of the rule.
					</p></div><p>
					<span class="strong strong"><strong>Example:</strong></span>
				</p><pre class="screen">"description": "A new rule for my node tagging policy"</pre><div class="formalpara"><p class="title"><strong>Conditions</strong></p><p>
						A condition defines an evaluation using the following key-value pattern:
					</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">field</span></dt><dd><p class="simpara">
								Defines the field to evaluate:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">memory_mb</code> - The amount of memory for the node in MB.
									</li><li class="listitem">
										<code class="literal">cpus</code> - The total number of threads for the node CPU.
									</li><li class="listitem">
										<code class="literal">cpu_arch</code> - The architecture of the node CPU.
									</li><li class="listitem">
										<code class="literal">local_gb</code> - The total storage space of the node root disk.
									</li></ul></div></dd><dt><span class="term">op</span></dt><dd><p class="simpara">
								Defines the operation to use for the evaluation. This includes the following attributes:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">eq</code> - Equal to
									</li><li class="listitem">
										<code class="literal">ne</code> - Not equal to
									</li><li class="listitem">
										<code class="literal">lt</code> - Less than
									</li><li class="listitem">
										<code class="literal">gt</code> - Greater than
									</li><li class="listitem">
										<code class="literal">le</code> - Less than or equal to
									</li><li class="listitem">
										<code class="literal">ge</code> - Greater than or equal to
									</li><li class="listitem">
										<code class="literal">in-net</code> - Checks that an IP address is in a given network
									</li><li class="listitem">
										<code class="literal">matches</code> - Requires a full match against a given regular expression
									</li><li class="listitem">
										<code class="literal">contains</code> - Requires a value to contain a given regular expression
									</li><li class="listitem">
										<code class="literal">is-empty</code> - Checks that <code class="literal">field</code> is empty
									</li></ul></div></dd><dt><span class="term">invert</span></dt><dd>
								Boolean value to define whether to invert the result of the evaluation.
							</dd><dt><span class="term">multiple</span></dt><dd><p class="simpara">
								Defines the evaluation to use if multiple results exist. This parameter includes the following attributes:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">any</code> - Requires any result to match
									</li><li class="listitem">
										<code class="literal">all</code> - Requires all results to match
									</li><li class="listitem">
										<code class="literal">first</code> - Requires the first result to match
									</li></ul></div></dd><dt><span class="term">value</span></dt><dd>
								Defines the value in the evaluation. If the field and operation result in the value, the condition return a true result. Otherwise, the condition returns a false result.
							</dd></dl></div><p>
					<span class="strong strong"><strong>Example:</strong></span>
				</p><pre class="screen">"conditions": [
  {
    "field": "local_gb",
    "op": "ge",
    "value": 1024
  }
],</pre><div class="formalpara"><p class="title"><strong>Actions</strong></p><p>
						If a condition is <code class="literal">true</code>, the policy performs an action. The action uses the <code class="literal">action</code> key and additional keys depending on the value of <code class="literal">action</code>:
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">fail</code> - Fails the introspection. Requires a <code class="literal">message</code> parameter for the failure message.
						</li><li class="listitem">
							<code class="literal">set-attribute</code> - Sets an attribute on an ironic node. Requires a <code class="literal">path</code> field, which is the path to an ironic attribute (for example, <code class="literal">/driver_info/ipmi_address</code>), and a <code class="literal">value</code> to set.
						</li><li class="listitem">
							<code class="literal">set-capability</code> - Sets a capability on an ironic node. Requires <code class="literal">name</code> and <code class="literal">value</code> fields, which are the name and the value for a new capability. This replaces the existing value for this capability. For example, use this to define node profiles.
						</li><li class="listitem">
							<code class="literal">extend-attribute</code> - The same as <code class="literal">set-attribute</code> but treats the existing value as a list and appends value to it. If the optional <code class="literal">unique</code> parameter is set to True, nothing is added if the given value is already in a list.
						</li></ul></div><p>
					<span class="strong strong"><strong>Example:</strong></span>
				</p><pre class="screen">"actions": [
  {
    "action": "set-capability",
    "name": "profile",
    "value": "swift-storage"
  }
]</pre></section><section class="section" id="policy-file-example"><div class="titlepage"><div><div><h2 class="title">22.2. Policy file example</h2></div></div></div><p>
					The following is an example JSON file (<code class="literal">rules.json</code>) that contains introspection rules:
				</p><pre class="screen">[
  {
    "description": "Fail introspection for unexpected nodes",
    "conditions": [
      {
        "op": "lt",
        "field": "memory_mb",
        "value": 4096
      }
    ],
    "actions": [
      {
        "action": "fail",
        "message": "Memory too low, expected at least 4 GiB"
      }
    ]
  },
  {
    "description": "Assign profile for object storage",
    "conditions": [
      {
        "op": "ge",
        "field": "local_gb",
        "value": 1024
      }
    ],
    "actions": [
      {
        "action": "set-capability",
        "name": "profile",
        "value": "swift-storage"
      }
    ]
  },
  {
    "description": "Assign possible profiles for compute and controller",
    "conditions": [
      {
        "op": "lt",
        "field": "local_gb",
        "value": 1024
      },
      {
        "op": "ge",
        "field": "local_gb",
        "value": 40
      }
    ],
    "actions": [
      {
        "action": "set-capability",
        "name": "compute_profile",
        "value": "1"
      },
      {
        "action": "set-capability",
        "name": "control_profile",
        "value": "1"
      },
      {
        "action": "set-capability",
        "name": "profile",
        "value": null
      }
    ]
  }
]</pre><p>
					This example consists of three rules:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Fail introspection if memory is lower than 4096 MiB. You can apply these types of rules if you want to exclude certain nodes from your cloud.
						</li><li class="listitem">
							Nodes with a hard drive size 1 TiB and bigger are assigned the swift-storage profile unconditionally.
						</li><li class="listitem">
							Nodes with a hard drive less than 1 TiB but more than 40 GiB can be either Compute or Controller nodes. You can assign two capabilities (<code class="literal">compute_profile</code> and <code class="literal">control_profile</code>) so that the <code class="literal">openstack overcloud profiles match</code> command can later make the final choice. For this process to succeed, you must remove the existing profile capability, otherwise the existing profile capability has priority.
						</li></ul></div><p>
					The profile matching rules do not change any other nodes.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Using introspection rules to assign the <code class="literal">profile</code> capability always overrides the existing value. However, <code class="literal">[PROFILE]_profile</code> capabilities are ignored for nodes that already have a profile capability.
					</p></div></div></section><section class="section" id="importing-policy-files"><div class="titlepage"><div><div><h2 class="title">22.3. Importing policy files</h2></div></div></div><p>
					To import policy files to director, complete the following steps.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Import the policy file into director:
						</p><pre class="screen">$ openstack baremetal introspection rule import rules.json</pre></li><li class="listitem"><p class="simpara">
							Run the introspection process:
						</p><pre class="screen">$ openstack overcloud node introspect --all-manageable</pre></li><li class="listitem"><p class="simpara">
							After introspection completes, check the nodes and their assigned profiles:
						</p><pre class="screen">$ openstack overcloud profiles list</pre></li><li class="listitem"><p class="simpara">
							If you made a mistake in introspection rules, run the following command to delete all rules:
						</p><pre class="screen">$ openstack baremetal introspection rule purge</pre></li></ol></div></section></section><section class="chapter" id="creating-whole-disk-images"><div class="titlepage"><div><div><h2 class="title">Chapter 23. Creating whole disk images</h2></div></div></div><p>
				The main overcloud image is a flat partition image that contains no partitioning information or bootloader. Director uses a separate kernel and ramdisk when it boots nodes and creates a basic partitioning layout when it writes the overcloud image to disk. However, you can create a whole disk image, which includes a partitioning layout, bootloader, and hardened security.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The following process uses the director image building feature. Red Hat only supports images that use the guidelines contained in this section. Custom images built outside of these specifications are not supported.
				</p></div></div><section class="section" id="security-hardening-measures"><div class="titlepage"><div><div><h2 class="title">23.1. Security hardening measures</h2></div></div></div><p>
					The whole disk image includes extra security hardening measures necessary for Red Hat OpenStack Platform deployments where security is an important feature.
				</p><div class="itemizedlist"><p class="title"><strong>Security recommendations for image creation</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">/tmp</code> directory is mounted on a separate volume or partition and has the <code class="literal">rw</code>, <code class="literal">nosuid</code>, <code class="literal">nodev</code>, <code class="literal">noexec</code>, and <code class="literal">relatime</code> flags.
						</li><li class="listitem">
							The <code class="literal">/var</code>, <code class="literal">/var/log</code> and the <code class="literal">/var/log/audit</code> directories are mounted on separate volumes or partitions, with the <code class="literal">rw</code> and <code class="literal">relatime</code> flags.
						</li><li class="listitem">
							The <code class="literal">/home</code> directory is mounted on a separate partition or volume and has the <code class="literal">rw</code>, <code class="literal">nodev</code>, and <code class="literal">relatime</code> flags.
						</li><li class="listitem"><p class="simpara">
							Include the following changes to the <code class="literal">GRUB_CMDLINE_LINUX</code> setting:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									To enable auditing, add the <code class="literal">audit=1</code> kernel boot flag.
								</li><li class="listitem">
									To disable the kernel support for USB using boot loader configuration, add <code class="literal">nousb</code>.
								</li><li class="listitem">
									To remove the insecure boot flags, set <code class="literal">crashkernel=auto</code>.
								</li></ul></div></li><li class="listitem">
							Blacklist insecure modules (<code class="literal">usb-storage</code>, <code class="literal">cramfs</code>, <code class="literal">freevxfs</code>, <code class="literal">jffs2</code>, <code class="literal">hfs</code>, <code class="literal">hfsplus</code>, <code class="literal">squashfs</code>, <code class="literal">udf</code>, <code class="literal">vfat</code>) and prevent these modules from loading.
						</li><li class="listitem">
							Remove any insecure packages (<code class="literal">kdump</code> installed by <code class="literal">kexec-tools</code> and <code class="literal">telnet</code>) from the image because they are installed by default.
						</li></ul></div></section><section class="section" id="whole-disk-image-workflow"><div class="titlepage"><div><div><h2 class="title">23.2. Whole disk image workflow</h2></div></div></div><p>
					To build a whole disk image, complete the following workflow:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Download a base Red Hat Enterprise Linux 8 image.
						</li><li class="listitem">
							Set the environment variables specific to registration.
						</li><li class="listitem">
							Customize the image by modifying the partition schema and the size.
						</li><li class="listitem">
							Create the image.
						</li><li class="listitem">
							Upload the image to director.
						</li></ol></div></section><section class="section" id="downloading-the-base-cloud-image"><div class="titlepage"><div><div><h2 class="title">23.3. Downloading the base cloud image</h2></div></div></div><p>
					Before you build a whole disk image, you must download an existing cloud image of Red Hat Enterprise Linux to use as a basis.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Navigate to the Red Hat Customer Portal:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/">https://access.redhat.com/</a>
								</li></ul></div></li><li class="listitem">
							Click <span class="strong strong"><strong>DOWNLOADS</strong></span> on the top menu.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Red Hat Enterprise Linux 8</strong></span>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Enter your customer Customer Portal login details if a prompt appears.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Select the KVM Guest Image that you want to download. For example, the KVM Guest Image for the latest Red Hat Enterprise Linux is available on the following page:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<a class="link" href="https://access.redhat.com/downloads/content/479">"Installers and Images for Red Hat Enterprise Linux Server"</a>
								</li></ul></div></li></ol></div></section><section class="section" id="disk-image-environment-variables"><div class="titlepage"><div><div><h2 class="title">23.4. Disk image environment variables</h2></div></div></div><p>
					As a part of the disk image building process, the director requires a base image and registration details to obtain packages for the new overcloud image. Define these attributes with the following Linux environment variables.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The image building process temporarily registers the image with a Red Hat subscription and unregisters the system when the image building process completes.
					</p></div></div><p>
					To build a disk image, set Linux environment variables that suit your environment and requirements:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">DIB_LOCAL_IMAGE</span></dt><dd>
								Sets the local image that you want to use as the basis for your whole disk image.
							</dd><dt><span class="term">REG_ACTIVATION_KEY</span></dt><dd>
								Use an activation key instead of login details as part of the registration process.
							</dd><dt><span class="term">REG_AUTO_ATTACH</span></dt><dd>
								Defines whether to attach the most compatible subscription automatically.
							</dd><dt><span class="term">REG_BASE_URL</span></dt><dd>
								The base URL of the content delivery server that contains packages for the image. The default Customer Portal Subscription Management process uses <code class="literal">https://cdn.redhat.com</code>. If you use a Red Hat Satellite 6 server, set this parameter to the base URL of your Satellite server.
							</dd><dt><span class="term">REG_ENVIRONMENT</span></dt><dd>
								Registers to an environment within an organization.
							</dd><dt><span class="term">REG_METHOD</span></dt><dd>
								Sets the method of registration. Use <code class="literal">portal</code> to register a system to the Red Hat Customer Portal. Use <code class="literal">satellite</code> to register a system with Red Hat Satellite 6.
							</dd><dt><span class="term">REG_ORG</span></dt><dd>
								The organization where you want to register the images.
							</dd><dt><span class="term">REG_POOL_ID</span></dt><dd>
								The pool ID of the product subscription information.
							</dd><dt><span class="term">REG_PASSWORD</span></dt><dd>
								Sets the password for the user account that registers the image.
							</dd><dt><span class="term">REG_REPOS</span></dt><dd><p class="simpara">
								A comma-separated string of repository names. Each repository in this string is enabled through <code class="literal">subscription-manager</code>.
							</p><p class="simpara">
								Use the following repositories for a security hardened whole disk image:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">rhel-8-for-x86_64-baseos-eus-rpms</code>
									</li><li class="listitem">
										<code class="literal">rhel-8-for-x86_64-appstream-eus-rpms</code>
									</li><li class="listitem">
										<code class="literal">rhel-8-for-x86_64-highavailability-eus-rpms</code>
									</li><li class="listitem">
										<code class="literal">ansible-2.9-for-rhel-8-x86_64-rpms</code>
									</li><li class="listitem">
										<code class="literal">openstack-16.1-for-rhel-8-x86_64-rpms</code>
									</li></ul></div></dd><dt><span class="term">REG_SAT_URL</span></dt><dd>
								The base URL of the Satellite server to register overcloud nodes. Use the Satellite HTTP URL and not the HTTPS URL for this parameter. For example, use <a class="link" href="http://satellite.example.com">http://satellite.example.com</a> and not <a class="link" href="https://satellite.example.com">https://satellite.example.com</a>.
							</dd><dt><span class="term">REG_SERVER_URL</span></dt><dd>
								Sets the host name of the subscription service to use. The default host name is for the Red Hat Customer Portal at <code class="literal">subscription.rhn.redhat.com</code>. If you use a Red Hat Satellite 6 server, set this parameter to the host name of your Satellite server.
							</dd><dt><span class="term">REG_USER</span></dt><dd>
								Sets the user name for the account that registers the image.
							</dd></dl></div><p>
					Use the following set of example commands to export a set of environment variables and temporarily register a local QCOW2 image to the Red Hat Customer Portal:
				</p><pre class="screen">$ export DIB_LOCAL_IMAGE=./rhel-8.0-x86_64-kvm.qcow2
$ export REG_METHOD=portal
$ export REG_USER="[your username]"
$ export REG_PASSWORD="[your password]"
$ export REG_REPOS="rhel-8-for-x86_64-baseos-eus-rpms \
    rhel-8-for-x86_64-appstream-eus-rpms \
    rhel-8-for-x86_64-highavailability-eus-rpms \
    ansible-2.9-for-rhel-8-x86_64-rpms \
    openstack-16.1-for-rhel-8-x86_64-rpms"</pre></section><section class="section" id="customizing-the-disk-layout"><div class="titlepage"><div><div><h2 class="title">23.5. Customizing the disk layout</h2></div></div></div><p>
					The default security hardened image size is 20G and uses predefined partitioning sizes. However, you must modify the partitioning layout to accommodate overcloud container images. Complete the steps in the following sections to increase the image size to 40G. You can modify the partitioning layout and disk size to further suit your needs.
				</p><p>
					To modify the partitioning layout and disk size, perform the following steps:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Modify the partitioning schema using the <code class="literal">DIB_BLOCK_DEVICE_CONFIG</code> environment variable.
						</li><li class="listitem">
							Modify the global size of the image by updating the <code class="literal">DIB_IMAGE_SIZE</code> environment variable.
						</li></ul></div></section><section class="section" id="modifying-the-partitioning-schema"><div class="titlepage"><div><div><h2 class="title">23.6. Modifying the partitioning schema</h2></div></div></div><p>
					You can modify the partitioning schema to alter the partitioning size, create new partitions, or remove existing partitions. Use the following environment variable to define a new partitioning schema:
				</p><pre class="screen">$ export DIB_BLOCK_DEVICE_CONFIG='&lt;yaml_schema_with_partitions&gt;'</pre><p>
					The following YAML structure represents the modified logical volume partitioning layout to accommodate enough space to pull overcloud container images:
				</p><pre class="screen">export DIB_BLOCK_DEVICE_CONFIG='''
- local_loop:
    name: image0
- partitioning:
    base: image0
    label: mbr
    partitions:
      - name: root
        flags: [ boot,primary ]
        size: 40G
- lvm:
    name: lvm
    base: [ root ]
    pvs:
        - name: pv
          base: root
          options: [ "--force" ]
    vgs:
        - name: vg
          base: [ "pv" ]
          options: [ "--force" ]
    lvs:
        - name: lv_root
          base: vg
          extents: 23%VG
        - name: lv_tmp
          base: vg
          extents: 4%VG
        - name: lv_var
          base: vg
          extents: 45%VG
        - name: lv_log
          base: vg
          extents: 23%VG
        - name: lv_audit
          base: vg
          extents: 4%VG
        - name: lv_home
          base: vg
          extents: 1%VG
- mkfs:
    name: fs_root
    base: lv_root
    type: xfs
    label: "img-rootfs"
    mount:
        mount_point: /
        fstab:
            options: "rw,relatime"
            fsck-passno: 1
- mkfs:
    name: fs_tmp
    base: lv_tmp
    type: xfs
    mount:
        mount_point: /tmp
        fstab:
            options: "rw,nosuid,nodev,noexec,relatime"
            fsck-passno: 2
- mkfs:
    name: fs_var
    base: lv_var
    type: xfs
    mount:
        mount_point: /var
        fstab:
            options: "rw,relatime"
            fsck-passno: 2
- mkfs:
    name: fs_log
    base: lv_log
    type: xfs
    mount:
        mount_point: /var/log
        fstab:
            options: "rw,relatime"
            fsck-passno: 3
- mkfs:
    name: fs_audit
    base: lv_audit
    type: xfs
    mount:
        mount_point: /var/log/audit
        fstab:
            options: "rw,relatime"
            fsck-passno: 4
- mkfs:
    name: fs_home
    base: lv_home
    type: xfs
    mount:
        mount_point: /home
        fstab:
            options: "rw,nodev,relatime"
            fsck-passno: 2
'''</pre><p>
					Use this sample YAML content as a basis for the partition schema of your image. Modify the partition sizes and layout to suit your needs.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You must define the correct partition sizes for the image because you cannot resize them after the deployment.
					</p></div></div></section><section class="section" id="modifying-the-image-size"><div class="titlepage"><div><div><h2 class="title">23.7. Modifying the image size</h2></div></div></div><p>
					The global sum of the modified partitioning schema might exceed the default disk size (20G). In this situation, you might need to modify the image size. To modify the image size, edit the configuration files that create the image.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a copy of the <code class="literal">/usr/share/openstack-tripleo-common/image-yaml/overcloud-hardened-images-python3.yaml</code>:
						</p><pre class="screen"># cp /usr/share/openstack-tripleo-common/image-yaml/overcloud-hardened-images-python3.yaml \
/home/stack/overcloud-hardened-images-python3-custom.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								For UEFI whole disk images, use <code class="literal">/usr/share/openstack-tripleo-common/image-yaml/overcloud-hardened-images-uefi-python3.yaml</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Edit the <code class="literal">DIB_IMAGE_SIZE</code> in the configuration file and adjust the values as necessary:
						</p><pre class="screen">...

environment:
DIB_PYTHON_VERSION: '3'
DIB_MODPROBE_BLACKLIST: 'usb-storage cramfs freevxfs jffs2 hfs hfsplus squashfs udf vfat bluetooth'
DIB_BOOTLOADER_DEFAULT_CMDLINE: 'nofb nomodeset vga=normal console=tty0 console=ttyS0,115200 audit=1 nousb'
DIB_IMAGE_SIZE: '40' <span id="CO2-1"><!--Empty--></span><span class="callout">1</span>
COMPRESS_IMAGE: '1'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Adjust this value to the new total disk size.
								</div></dd></dl></div></li><li class="listitem">
							Save the file.
						</li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When you deploy the overcloud, the director creates a RAW version of the overcloud image. This means your undercloud must have enough free space to accommodate the RAW image. For example, if you set the security hardened image size to 40G, you must have 40G of space available on the undercloud hard disk.
					</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						When director writes the image to the physical disk, it creates a 64MB configuration drive primary partition at the end of the disk. When you create your whole disk image, ensure that the size of the physical disk accommodates this extra partition.
					</p></div></div></section><section class="section" id="building-the-whole-disk-image"><div class="titlepage"><div><div><h2 class="title">23.8. Building the whole disk image</h2></div></div></div><p>
					After you set the environment variables and customize the image, create the image using the <code class="literal">openstack overcloud image build</code> command.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack overcloud image build</code> command with all necessary configuration files.
						</p><pre class="screen"># openstack overcloud image build \
--image-name overcloud-hardened-full \
--config-file /home/stack/overcloud-hardened-images-python3-custom.yaml \ <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
--config-file /usr/share/openstack-tripleo-common/image-yaml/overcloud-hardened-images-rhel8.yaml <span id="CO3-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									This is the custom configuration file that contains the new disk size. If you are not using a different custom disk size, use the original <code class="literal">/usr/share/openstack-tripleo-common/image-yaml/overcloud-hardened-images-python3.yaml</code> file instead. For standard UEFI whole disk images, use <code class="literal">overcloud-hardened-images-uefi-python3.yaml</code>.
								</div></dd><dt><a href="index.html#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									For UEFI whole disk images, use <code class="literal">overcloud-hardened-images-uefi-rhel8.yaml</code>.
								</div></dd></dl></div><p class="simpara">
							This command creates an image called <code class="literal">overcloud-hardened-full.qcow2</code>, which contains all the necessary security features.
						</p></li></ol></div></section><section class="section" id="uploading-the-whole-disk-image"><div class="titlepage"><div><div><h2 class="title">23.9. Uploading the whole disk image</h2></div></div></div><p>
					Upload the image to the OpenStack Image (glance) service and start using it from the Red Hat OpenStack Platform director. To upload a security hardened image, complete the following steps:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Rename the newly generated image and move the image to your <code class="literal">images</code> directory:
						</p><pre class="screen"># mv overcloud-hardened-full.qcow2 ~/images/overcloud-full.qcow2</pre></li><li class="listitem"><p class="simpara">
							Remove all the old overcloud images:
						</p><pre class="screen"># openstack image delete overcloud-full
# openstack image delete overcloud-full-initrd
# openstack image delete overcloud-full-vmlinuz</pre></li><li class="listitem"><p class="simpara">
							Upload the new overcloud image:
						</p><pre class="screen"># openstack overcloud image upload --image-path /home/stack/images --whole-disk</pre></li></ol></div><p>
					If you want to replace an existing image with the security hardened image, use the <code class="literal">--update-existing</code> flag. This flag overwrites the original <code class="literal">overcloud-full</code> image with a new security hardened image.
				</p></section></section><section class="chapter" id="configuring_direct_deploy"><div class="titlepage"><div><div><h2 class="title">Chapter 24. Configuring Direct Deploy</h2></div></div></div><p>
				When provisioning nodes, director mounts the overcloud base operating system image on an iSCSI mount and then copies the image to disk on each node. Direct deploy is an alternative method that writes disk images from a HTTP location directly to disk on bare metal nodes.
			</p><section class="section" id="configuring-direct-deploy-on-the-undercloud"><div class="titlepage"><div><div><h2 class="title">24.1. Configuring the direct deploy interface on the undercloud</h2></div></div></div><p>
					The iSCSI deploy interface is the default deploy interface. However, you can enable the direct deploy interface to download an image from a HTTP location to the target disk.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Your overcloud node memory <code class="literal">tmpfs</code> must have at least 8GB of RAM.
					</p></div></div><h4 id="procedure">Procedure</h4><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create or modify a custom environment file <code class="literal">/home/stack/undercloud_custom_env.yaml</code> and specify the <code class="literal">IronicDefaultDeployInterface</code>.
						</p><pre class="screen">parameter_defaults:
  IronicDefaultDeployInterface: direct</pre></li><li class="listitem"><p class="simpara">
							By default, the Bare Metal service (ironic) agent on each node obtains the image stored in the Object Storage service (swift) through a HTTP link. Alternatively, ironic can stream this image directly to the node through the <code class="literal">ironic-conductor</code> HTTP server. To change the service that provides the image, set the <code class="literal">IronicImageDownloadSource</code> to <code class="literal">http</code> in the <code class="literal">/home/stack/undercloud_custom_env.yaml</code> file:
						</p><pre class="screen">parameter_defaults:
  IronicDefaultDeployInterface: direct
  IronicImageDownloadSource: http</pre></li><li class="listitem"><p class="simpara">
							Include the custom environment file in the <code class="literal">DEFAULT</code> section of the <code class="literal">undercloud.conf</code> file.
						</p><pre class="screen">custom_env_files = /home/stack/undercloud_custom_env.yaml</pre></li><li class="listitem"><p class="simpara">
							Perform the undercloud installation:
						</p><pre class="screen">$ openstack undercloud install</pre></li></ol></div></section></section><section class="chapter" id="creating-virtualized-control-planes"><div class="titlepage"><div><div><h2 class="title">Chapter 25. Creating virtualized control planes</h2></div></div></div><p>
				A virtualized control plane is a control plane located on virtual machines (VMs) rather than on bare metal. Use a virtualized control plane reduce the number of bare metal machines that you require for the control plane.
			</p><p>
				This chapter explains how to virtualize your Red Hat OpenStack Platform (RHOSP) control plane for the overcloud using RHOSP and Red Hat Virtualization.
			</p><section class="section" id="virtualized-control-planes"><div class="titlepage"><div><div><h2 class="title">25.1. Virtualized control plane architecture</h2></div></div></div><p>
					Use director to provision an overcloud using Controller nodes that are deployed in a Red Hat Virtualization cluster. You can then deploy these virtualized controllers as the virtualized control plane nodes.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Virtualized Controller nodes are supported only on Red Hat Virtualization.
					</p></div></div><p>
					The following architecture diagram illustrates how to deploy a virtualized control plane. Distribute the overcloud with the Controller nodes running on VMs on Red Hat Virtualization and run the Compute and Storage nodes on bare metal.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Run the OpenStack virtualized undercloud on Red Hat Virtualization.
					</p></div></div><div class="formalpara"><p class="title"><strong>Virtualized control plane architecture</strong></p><p>
						<span class="inlinemediaobject"><img src="Virtualized_control_plane_arch.png" alt="Virtualized control plane architecture"/></span>

					</p></div><p>
					The OpenStack Bare Metal Provisioning service (ironic) includes a driver for Red Hat Virtualization VMs, <a class="link" href="index.html#sect-Red_Hat_Virtualization"><code class="literal">staging-ovirt</code></a>. You can use this driver to manage virtual nodes within a Red Hat Virtualization environment. You can also use it to deploy overcloud controllers as virtual machines within a Red Hat Virtualization environment.
				</p></section><section class="section" id="benefits_and_limitations_of_virtualizing_your_rhosp_overcloud_control_plane"><div class="titlepage"><div><div><h2 class="title">25.2. Benefits and limitations of virtualizing your RHOSP overcloud control plane</h2></div></div></div><p>
					Although there are a number of benefits to virtualizing your RHOSP overcloud control plane, this is not an option in every configuration.
				</p><div class="formalpara"><p class="title"><strong>Benefits</strong></p><p>
						Virtualizing the overloud control plane has a number of benefits that prevent downtime and improve performance.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You can allocate resources to the virtualized controllers dynamically, using hot add and hot remove to scale CPU and memory as required. This prevents downtime and facilitates increased capacity as the platform grows.
						</li><li class="listitem">
							You can deploy additional infrastructure VMs on the same Red Hat Virtualization cluster. This minimizes the server footprint in the data center and maximizes the efficiency of the physical nodes.
						</li><li class="listitem">
							You can use composable roles to define more complex RHOSP control planes and allocate resources to specific components of the control plane.
						</li><li class="listitem">
							You can maintain systems without service interruption with the VM live migration feature.
						</li><li class="listitem">
							You can integrate third-party or custom tools that Red Hat Virtualization supports.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Limitations</strong></p><p>
						Virtualized control planes limit the types of configurations that you can use.
					</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Virtualized Ceph Storage nodes and Compute nodes are not supported.
						</li><li class="listitem">
							Block Storage (cinder) image-to-volume is not supported for back ends that use Fiber Channel. Red Hat Virtualization does not support N_Port ID Virtualization (NPIV). Therefore, Block Storage (cinder) drivers that need to map LUNs from a storage back end to the controllers, where <span class="strong strong"><strong>cinder-volume</strong></span> runs by default, do not work. You must create a dedicated role for <span class="strong strong"><strong>cinder-volume</strong></span> instead of including it on the virtualized controllers. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#Roles">Composable Services and Custom Roles</a>.
						</li></ul></div></section><section class="section" id="provisioning-virtualized-controllers-using-RHV-driver"><div class="titlepage"><div><div><h2 class="title">25.3. Provisioning virtualized controllers using the Red Hat Virtualization driver</h2></div></div></div><p>
					Complete the following steps to provision a virtualized RHOSP control plane for the overcloud using RHOSP and Red Hat Virtualization.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You must have a 64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions.
						</li><li class="listitem"><p class="simpara">
							You must have the following software already installed and configured:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Red Hat Virtualization. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/">Red Hat Virtualization Documentation Suite</a>.
								</li><li class="listitem">
									Red Hat OpenStack Platform (RHOSP). For more information, see <a class="link" href="index.html">Director Installation and Usage</a>.
								</li></ul></div></li><li class="listitem">
							You must have the virtualized Controller nodes prepared in advance. These requirements are the same as for bare metal Controller nodes. For more information, see <a class="link" href="index.html#controller-node-requirements">Controller Node Requirements</a>.
						</li><li class="listitem">
							You must have the bare metal nodes being used as overcloud Compute nodes, and the storage nodes, prepared in advance. For hardware specifications, see the <a class="link" href="index.html#compute-node-requirements">Compute Node Requirements</a> and <a class="link" href="index.html#ceph-storage-node-requirements">Ceph Storage Node Requirements</a>. To deploy overcloud Compute nodes on POWER (ppc64le) hardware, see <a class="link" href="index.html#appe-OSP_on_POWER">Red Hat OpenStack Platform for POWER</a>.
						</li><li class="listitem">
							You must have the logical networks created, and your cluster of host networks ready to use network isolation with multiple networks. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/html/administration_guide/chap-logical_networks">Logical Networks</a>.
						</li><li class="listitem">
							You must have the internal BIOS clock of each node set to UTC to prevent issues with future-dated file timestamps when hwclock synchronizes the BIOS clock before applying the timezone offset.
						</li></ul></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
					To avoid performance bottlenecks, use composable roles and keep the data plane services on the bare metal Controller nodes.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To enable the <code class="literal">staging-ovirt</code> driver in director, add the driver to the <code class="literal">enabled_hardware_types</code> parameter in the <code class="literal">undercloud.conf</code> configuration file:
						</p><pre class="screen">enabled_hardware_types = ipmi,redfish,ilo,idrac,staging-ovirt</pre></li><li class="listitem"><p class="simpara">
							Verify that the undercloud contains the <code class="literal">staging-ovirt</code> driver:
						</p><pre class="screen">(undercloud) [stack@undercloud ~]$ openstack baremetal driver list</pre><p class="simpara">
							If you have configured the undercloud correctly, this command returns the following result:
						</p><pre class="literallayout"> +---------------------+-----------------------+
 | Supported driver(s) | Active host(s)        |
 +---------------------+-----------------------+
 | idrac               | localhost.localdomain |
 | ilo                 | localhost.localdomain |
 | ipmi                | localhost.localdomain |
 | pxe_drac            | localhost.localdomain |
 | pxe_ilo             | localhost.localdomain |
 | pxe_ipmitool        | localhost.localdomain |
 | redfish             | localhost.localdomain |
 | staging-ovirt       | localhost.localdomain |</pre></li><li class="listitem"><p class="simpara">
							Update the overcloud node definition template, for example, <code class="literal">nodes.json</code>, to register the VMs hosted on Red Hat Virtualization with director. For more information, see <a class="link" href="index.html#sect-Registering_Nodes_for_the_Overcloud">Registering Nodes for the Overcloud</a>. Use the following key:value pairs to define aspects of the VMs that you want to deploy with your overcloud:
						</p><div class="table" id="idm140301165720384"><p class="title"><strong>Table 25.1. Configuring the VMs for the overcloud</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301165715536" scope="col">Key</th><th align="left" valign="top" id="idm140301165714448" scope="col">Set to this value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301165715536"> <p>
											<code class="literal">pm_type</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140301165714448"> <p>
											OpenStack Bare Metal Provisioning (ironic) service driver for oVirt/RHV VMs, <code class="literal">staging-ovirt</code>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140301165715536"> <p>
											<code class="literal">pm_user</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140301165714448"> <p>
											Red Hat Virtualization Manager username.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140301165715536"> <p>
											<code class="literal">pm_password</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140301165714448"> <p>
											Red Hat Virtualization Manager password.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140301165715536"> <p>
											<code class="literal">pm_addr</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140301165714448"> <p>
											Hostname or IP of the Red Hat Virtualization Manager server.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm140301165715536"> <p>
											<code class="literal">pm_vm_name</code>
										</p>
										 </td><td align="left" valign="top" headers="idm140301165714448"> <p>
											Name of the virtual machine in Red Hat Virtualization Manager where the controller is created.
										</p>
										 </td></tr></tbody></table></div></div><p class="simpara">
							For example:
						</p><pre class="screen">{
      "nodes": [
          {
              "name":"osp13-controller-0",
              "pm_type":"staging-ovirt",
              "mac":[
                  "00:1a:4a:16:01:56"
              ],
              "cpu":"2",
              "memory":"4096",
              "disk":"40",
              "arch":"x86_64",
              "pm_user":"admin@internal",
              "pm_password":"password",
              "pm_addr":"rhvm.example.com",
              "pm_vm_name":"{osp_curr_ver}-controller-0",
              "capabilities": "profile:control,boot_option:local"
          },
          ...
  }</pre><p class="simpara">
							Configure one Controller on each Red Hat Virtualization Host
						</p></li><li class="listitem">
							Configure an affinity group in Red Hat Virtualization with "soft negative affinity" to ensure high availability is implemented for your controller VMs. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/html-single/virtual_machine_management_guide/index#sect-Affinity_Groups">Affinity Groups</a>.
						</li><li class="listitem">
							Open the Red Hat Virtualization Manager interface, and use it to map each VLAN to a separate logical vNIC in the controller VMs. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/html/administration_guide/chap-logical_networks">Logical Networks</a>.
						</li><li class="listitem">
							Set <code class="literal">no_filter</code> in the vNIC of the director and controller VMs, and restart the VMs, to disable the MAC spoofing filter on the networks attached to the controller VMs. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.3/html-single/administration_guide/index#sect-Virtual_Network_Interface_Cards">Virtual Network Interface Cards</a>.
						</li><li class="listitem"><p class="simpara">
							Deploy the overcloud to include the new virtualized controller nodes in your environment:
						</p><pre class="screen">(undercloud) [stack@undercloud ~]$ openstack overcloud deploy --templates</pre></li></ol></div></section></section></div><div class="part" id="troubleshooting_and_tips"><div class="titlepage"><div><div><h1 class="title">Part V. Troubleshooting and tips</h1></div></div></div><section class="chapter" id="troubleshooting-director-errors"><div class="titlepage"><div><div><h2 class="title">Chapter 26. Troubleshooting director errors</h2></div></div></div><p>
				Errors can occur at certain stages of the director processes. This section contains some information about diagnosing common problems.
			</p><section class="section" id="troubleshooting-node-registration"><div class="titlepage"><div><div><h2 class="title">26.1. Troubleshooting node registration</h2></div></div></div><p>
					Issues with node registration usually occur due to issues with incorrect node details. In these situations, validate the template file containing your node details and correct the imported node details.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the node import command with the <code class="literal">--validate-only</code> option. This option validates your node template without performing an import:
						</p><pre class="screen">(undercloud) $ openstack overcloud node import --validate-only ~/nodes.json
Waiting for messages on queue 'tripleo' with no timeout.

Successfully validated environment file</pre></li><li class="listitem"><p class="simpara">
							To fix incorrect details with imported nodes, run the <code class="literal">openstack baremetal</code> commands to update node details. The following example shows how to change networking details:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Identify the assigned port UUID for the imported node:
								</p><pre class="screen">$ source ~/stackrc
(undercloud) $ openstack baremetal port list --node [NODE UUID]</pre></li><li class="listitem"><p class="simpara">
									Update the MAC address:
								</p><pre class="screen">(undercloud) $ openstack baremetal port set --address=[NEW MAC] [PORT UUID]</pre></li><li class="listitem"><p class="simpara">
									Configure a new IPMI address on the node:
								</p><pre class="screen">(undercloud) $ openstack baremetal node set --driver-info ipmi_address=[NEW IPMI ADDRESS] [NODE UUID]</pre></li></ol></div></li></ol></div></section><section class="section" id="troubleshooting-hardware-introspection"><div class="titlepage"><div><div><h2 class="title">26.2. Troubleshooting hardware introspection</h2></div></div></div><p>
					You must run the introspection process to completion. However, <code class="literal">ironic-inspector</code> times out after a default one hour period if the inspection ramdisk does not respond. Sometimes this indicates a bug in the inspection ramdisk but usually this time-out occurs due to an environment misconfiguration, particularly BIOS boot settings.
				</p><p>
					To diagnose and resolve common environment misconfiguration issues, complete the following steps:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Director uses OpenStack Object Storage (swift) to save the hardware data that it obtains during the introspection process. If this service is not running, the introspection can fail. Check all services related to OpenStack Object Storage to ensure that the service is running:
						</p><pre class="screen">(undercloud) $ sudo systemctl list-units tripleo_swift*</pre></li><li class="listitem"><p class="simpara">
							Ensure that your nodes are in a <code class="literal">manageable</code> state. The introspection does not inspect nodes in an <code class="literal">available</code> state, which is meant for deployment. If you want to inspect nodes that are in an <code class="literal">available</code> state, change the node status to <code class="literal">manageable</code> state before introspection:
						</p><pre class="screen">(undercloud) $ openstack baremetal node manage [NODE UUID]</pre></li><li class="listitem"><p class="simpara">
							Configure temporary access to the introspection ramdisk. You can provide either a temporary password or an SSH key to access the node during introspection debugging. Complete the following procedure to configure ramdisk access:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Run the <code class="literal">openssl passwd -1</code> command with a temporary password to generate an MD5 hash:
								</p><pre class="screen">(undercloud) $ openssl passwd -1 mytestpassword
$1$enjRSyIw$/fYUpJwr6abFy/d.koRgQ/</pre></li><li class="listitem"><p class="simpara">
									Edit the <code class="literal">/var/lib/ironic/httpboot/inspector.ipxe</code> file, find the line starting with <code class="literal">kernel</code>, and append the <code class="literal">rootpwd</code> parameter and the MD5 hash:
								</p><pre class="screen">kernel http://192.2.0.1:8088/agent.kernel ipa-inspection-callback-url=http://192.168.0.1:5050/v1/continue ipa-inspection-collectors=default,extra-hardware,logs systemd.journald.forward_to_console=yes BOOTIF=${mac} ipa-debug=1 ipa-inspection-benchmarks=cpu,mem,disk rootpwd="$1$enjRSyIw$/fYUpJwr6abFy/d.koRgQ/" selinux=0</pre><p class="simpara">
									Alternatively, append your public SSH key to the <code class="literal">sshkey</code> parameter.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										Include quotation marks for both the <code class="literal">rootpwd</code> and <code class="literal">sshkey</code> parameters.
									</p></div></div></li></ol></div></li><li class="listitem"><p class="simpara">
							Run the introspection on the node:
						</p><pre class="screen">(undercloud) $ openstack overcloud node introspect [NODE UUID] --provide</pre><p class="simpara">
							Use the <code class="literal">--provide</code> option to change the node state to <code class="literal">available</code> after the introspection completes.
						</p></li><li class="listitem"><p class="simpara">
							Identify the IP address of the node from the <code class="literal">dnsmasq</code> logs:
						</p><pre class="screen">(undercloud) $ sudo tail -f /var/log/containers/ironic-inspector/dnsmasq.log</pre></li><li class="listitem"><p class="simpara">
							If an error occurs, access the node using the root user and temporary access details:
						</p><pre class="screen">$ ssh root@192.168.24.105</pre><p class="simpara">
							Access the node during introspection to run diagnostic commands and troubleshoot the introspection failure.
						</p></li><li class="listitem"><p class="simpara">
							To stop the introspection process, run the following command:
						</p><pre class="screen">(undercloud) $ openstack baremetal introspection abort [NODE UUID]</pre><p class="simpara">
							You can also wait until the process times out.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Red Hat OpenStack Platform director retries introspection three times after the initial abort. Run the <code class="literal">openstack baremetal introspection abort</code> command at each attempt to abort the introspection completely.
							</p></div></div></li></ol></div></section><section class="section" id="troubleshooting-workflows-and-executions"><div class="titlepage"><div><div><h2 class="title">26.3. Troubleshooting workflows and executions</h2></div></div></div><p>
					The OpenStack Workflow (mistral) service groups multiple OpenStack tasks into workflows. Red Hat OpenStack Platform uses a set of these workflows to perform common functions across the director, including bare metal node control, validations, plan management, and overcloud deployment.
				</p><p>
					For example, when you run the <code class="literal">openstack overcloud deploy</code> command, the OpenStack Workflow service executes two workflows. The first workflow uploads the deployment plan:
				</p><pre class="screen">Removing the current plan files
Uploading new plan files
Started Mistral Workflow. Execution ID: aef1e8c6-a862-42de-8bce-073744ed5e6b
Plan updated</pre><p>
					The second workflow starts the overcloud deployment:
				</p><pre class="screen">Deploying templates in the directory /tmp/tripleoclient-LhRlHX/tripleo-heat-templates
Started Mistral Workflow. Execution ID: 97b64abe-d8fc-414a-837a-1380631c764d
2016-11-28 06:29:26Z [overcloud]: CREATE_IN_PROGRESS  Stack CREATE started
2016-11-28 06:29:26Z [overcloud.Networks]: CREATE_IN_PROGRESS  state changed
2016-11-28 06:29:26Z [overcloud.HeatAuthEncryptionKey]: CREATE_IN_PROGRESS  state changed
2016-11-28 06:29:26Z [overcloud.ServiceNetMap]: CREATE_IN_PROGRESS  state changed
...</pre><p>
					The OpenStack Workflow service uses the following objects to track the workflow:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Actions</span></dt><dd>
								A particular instruction that OpenStack performs when an associated task runs. Examples include running shell scripts or performing HTTP requests. Some OpenStack components have in-built actions that OpenStack Workflow uses.
							</dd><dt><span class="term">Tasks</span></dt><dd>
								Defines the action to run and the result of running the action. These tasks usually have actions or other workflows associated with them. When a task completes, the workflow directs to another task, usually depending on whether the task succeeded or failed.
							</dd><dt><span class="term">Workflows</span></dt><dd>
								A set of tasks grouped together and executed in a specific order.
							</dd><dt><span class="term">Executions</span></dt><dd>
								Defines a particular action, task, or workflow running.
							</dd></dl></div><p>
					OpenStack Workflow also provides robust logging of executions, which helps to identify issues with certain command failures. For example, if a workflow execution fails, you can identify the point of failure.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							List the workflow executions that have the failed state <code class="literal">ERROR</code>:
						</p><pre class="screen">(undercloud) $ openstack workflow execution list | grep "ERROR"</pre></li><li class="listitem"><p class="simpara">
							Get the UUID of the failed workflow execution (for example, <code class="literal">dffa96b0-f679-4cd2-a490-4769a3825262</code>) and view the execution and output:
						</p><pre class="screen">(undercloud) $ openstack workflow execution show dffa96b0-f679-4cd2-a490-4769a3825262
(undercloud) $ openstack workflow execution output show dffa96b0-f679-4cd2-a490-4769a3825262</pre></li><li class="listitem"><p class="simpara">
							These commands return information about the failed task in the execution. The <code class="literal">openstack workflow execution show</code> command also displays the workflow that was used for the execution (for example, <code class="literal">tripleo.plan_management.v1.publish_ui_logs_to_swift</code>). You can view the full workflow definition with the following command:
						</p><pre class="screen">(undercloud) $ openstack workflow definition show tripleo.plan_management.v1.publish_ui_logs_to_swift</pre><p class="simpara">
							This is useful for identifying where in the workflow a particular task occurs.
						</p></li><li class="listitem"><p class="simpara">
							View action executions and their results using a similar command syntax:
						</p><pre class="screen">(undercloud) $ openstack action execution list
(undercloud) $ openstack action execution show 8a68eba3-0fec-4b2a-adc9-5561b007e886
(undercloud) $ openstack action execution output show 8a68eba3-0fec-4b2a-adc9-5561b007e886</pre><p class="simpara">
							This is useful for identifying a specific action that causes issues.
						</p></li></ol></div></section><section class="section" id="troubleshooting-overcloud-creation-and-deployment"><div class="titlepage"><div><div><h2 class="title">26.4. Troubleshooting overcloud creation and deployment</h2></div></div></div><p>
					The initial creation of the overcloud occurs with the OpenStack Orchestration (heat) service. If an overcloud deployment fails, use the OpenStack clients and service log files to diagnose the failed deployment.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Run the deployment failures command:
						</p><pre class="screen">$ openstack overcloud failures</pre></li><li class="listitem"><p class="simpara">
							Run the following command to display the details of the failure:
						</p><pre class="screen">(undercloud) $ openstack stack failures list &lt;OVERCLOUD_NAME&gt; --long</pre><p class="simpara">
							Replace <code class="literal">&lt;OVERCLOUD_NAME&gt;</code> with the name of your overcloud.
						</p></li><li class="listitem"><p class="simpara">
							Run the following command to identify the stacks that failed:
						</p><pre class="screen">(undercloud) $ openstack stack list --nested --property status=FAILED</pre></li></ol></div></section><section class="section" id="troubleshooting-node-provisioning"><div class="titlepage"><div><div><h2 class="title">26.5. Troubleshooting node provisioning</h2></div></div></div><p>
					The OpenStack Orchestration (heat) service controls the provisioning process. If node provisioning fails, use the OpenStack clients and service log files to diagnose the issues.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Check the bare metal service to see all registered nodes and their current status:
						</p><pre class="screen">(undercloud) $ openstack baremetal node list

+----------+------+---------------+-------------+-----------------+-------------+
| UUID     | Name | Instance UUID | Power State | Provision State | Maintenance |
+----------+------+---------------+-------------+-----------------+-------------+
| f1e261...| None | None          | power off   | available       | False       |
| f0b8c1...| None | None          | power off   | available       | False       |
+----------+------+---------------+-------------+-----------------+-------------+</pre><p class="simpara">
							All nodes available for provisioning should have the following states set:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<span class="strong strong"><strong>Maintenance</strong></span> set to <code class="literal">False</code>.
								</li><li class="listitem">
									<span class="strong strong"><strong>Provision State</strong></span> set to <code class="literal">available</code> before provisioning.
								</li></ul></div><p class="simpara">
							The following table outlines some common provisioning failure scenarios.
						</p></li></ol></div><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 20%; " class="col_1"><!--Empty--></col><col style="width: 40%; " class="col_2"><!--Empty--></col><col style="width: 40%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301166230816" scope="col">Problem</th><th align="left" valign="top" id="idm140301166229728" scope="col">Cause</th><th align="left" valign="top" id="idm140301166228640" scope="col">Solution</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301166230816"> <p>
									<span class="strong strong"><strong>Maintenance</strong></span> sets itself to <code class="literal">True</code> automatically.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166229728"> <p>
									The director cannot access the power management for the nodes.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166228640"> <p>
									Check the credentials for node power management.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166230816"> <p>
									<span class="strong strong"><strong>Provision State</strong></span> is set to <code class="literal">available</code> but nodes do not provision.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166229728"> <p>
									The problem occurred before bare metal deployment started.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166228640"> <p>
									Check the node details including the profile and flavor mapping. Check that the node hardware details are within the requirements for the flavor.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166230816"> <p>
									<span class="strong strong"><strong>Provision State</strong></span> is set to <code class="literal">wait call-back</code> for a node.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166229728"> <p>
									The node provisioning process has not yet finished for this node.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166228640"> <p>
									Wait until this status changes. Otherwise, connect to the virtual console of the node and check the output.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166230816"> <p>
									<span class="strong strong"><strong>Provision State</strong></span> is <code class="literal">active</code> and <span class="strong strong"><strong>Power State</strong></span> is <code class="literal">power on</code> but the nodes do not respond.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166229728"> <p>
									The node provisioning has finished successfully and there is a problem during the post-deployment configuration step.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166228640"> <p>
									Diagnose the node configuration process. Connect to the virtual console of the node and check the output.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166230816"> <p>
									<span class="strong strong"><strong>Provision State</strong></span> is <code class="literal">error</code> or <code class="literal">deploy failed</code>.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166229728"> <p>
									Node provisioning has failed.
								</p>
								 </td><td align="left" valign="top" headers="idm140301166228640"> <p>
									View the bare metal node details with the <code class="literal">openstack baremetal node show</code> command and check the <code class="literal">last_error</code> field, which contains error description.
								</p>
								 </td></tr></tbody></table></div></section><section class="section" id="sect-Troubleshooting_IP_Address_Conflicts_on_the_Provisioning_Network"><div class="titlepage"><div><div><h2 class="title">26.6. Troubleshooting IP address conflicts during provisioning</h2></div></div></div><p>
					Introspection and deployment tasks fail if the destination hosts are allocated an IP address that is already in use. To prevent these failures, you can perform a port scan of the Provisioning network to determine whether the discovery IP range and host IP range are free.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Install <code class="literal">nmap</code>:
						</p><pre class="screen">$ sudo dnf install nmap</pre></li><li class="listitem"><p class="simpara">
							Use <code class="literal">nmap</code> to scan the IP address range for active addresses. This example scans the 192.168.24.0/24 range, replace this with the IP subnet of the Provisioning network (using CIDR bitmask notation):
						</p><pre class="screen">$ sudo nmap -sn 192.168.24.0/24</pre></li><li class="listitem"><p class="simpara">
							Review the output of the <code class="literal">nmap</code> scan. For example, you should see the IP address of the undercloud, and any other hosts that are present on the subnet:
						</p><pre class="screen">$ sudo nmap -sn 192.168.24.0/24

Starting Nmap 6.40 ( http://nmap.org ) at 2015-10-02 15:14 EDT
Nmap scan report for 192.168.24.1
Host is up (0.00057s latency).
Nmap scan report for 192.168.24.2
Host is up (0.00048s latency).
Nmap scan report for 192.168.24.3
Host is up (0.00045s latency).
Nmap scan report for 192.168.24.5
Host is up (0.00040s latency).
Nmap scan report for 192.168.24.9
Host is up (0.00019s latency).
Nmap done: 256 IP addresses (5 hosts up) scanned in 2.45 seconds</pre><p class="simpara">
							If any of the active IP addresses conflict with the IP ranges in undercloud.conf, you must either change the IP address ranges or release the IP addresses before you introspect or deploy the overcloud nodes.
						</p></li></ol></div></section><section class="section" id="sect-Troubleshooting_No_Valid_Host_Found_Errors"><div class="titlepage"><div><div><h2 class="title">26.7. Troubleshooting "No Valid Host Found" errors</h2></div></div></div><p>
					Sometimes the <code class="literal">/var/log/nova/nova-conductor.log</code> contains the following error:
				</p><pre class="screen">NoValidHost: No valid host was found. There are not enough hosts available.</pre><p>
					This error occurs when the Compute Scheduler cannot find a bare metal node that is suitable for booting the new instance. This usually means that there is a mismatch between resources that the Compute service expects to find and resources that the Bare Metal service advertised to Compute. To check that there is a mismatch error, complete the following steps:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Check that the introspection succeeded on the node. If the introspection fails, check that each node contains the required ironic node properties:
						</p><pre class="screen">(undercloud) $ openstack baremetal node show [NODE UUID]</pre><p class="simpara">
							Check that the <code class="literal">properties</code> JSON field has valid values for keys <code class="literal">cpus</code>, <code class="literal">cpu_arch</code>, <code class="literal">memory_mb</code> and <code class="literal">local_gb</code>.
						</p></li><li class="listitem"><p class="simpara">
							Ensure that the Compute flavor that is mapped to the node does not exceed the node properties for the required number of nodes:
						</p><pre class="screen">(undercloud) $ openstack flavor show [FLAVOR NAME]</pre></li><li class="listitem">
							Run the <code class="literal">openstack baremetal node list</code> command to ensure that there are sufficient nodes in the available state. Nodes in <code class="literal">manageable</code> state usually signify a failed introspection.
						</li><li class="listitem"><p class="simpara">
							Run the <code class="literal">openstack baremetal node list</code> command and ensure that the nodes are not in maintenance mode. If a node changes to maintenance mode automatically, the likely cause is an issue with incorrect power management credentials. Check the power management credentials and then remove maintenance mode:
						</p><pre class="screen">(undercloud) $ openstack baremetal node maintenance unset [NODE UUID]</pre></li><li class="listitem">
							If you are using automatic profile tagging, check that you have enough nodes that correspond to each flavor and profile. Run the <code class="literal">openstack baremetal node show</code> command on a node and check the <code class="literal">capabilities</code> key in the <code class="literal">properties</code> field. For example, a node tagged for the Compute role contains the <code class="literal">profile:compute</code> value.
						</li><li class="listitem"><p class="simpara">
							You must wait for node information to propagate from Bare Metal to Compute after introspection. However, if you performed some steps manually, there might be a short period of time when nodes are not available to the Compute service (nova). Use the following command to check the total resources in your system:
						</p><pre class="screen">(undercloud) $ openstack hypervisor stats show</pre></li></ol></div></section><section class="section" id="troubleshooting-overcloud-configuration"><div class="titlepage"><div><div><h2 class="title">26.8. Troubleshooting overcloud configuration</h2></div></div></div><p>
					OpenStack Platform director uses Ansible to configure the overcloud. Complete the following steps to diagnose Ansible playbook errors (<code class="literal">config-download</code>) on the overcloud.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Ensure that the <code class="literal">stack</code> user has access to the files in the <code class="literal">/var/lib/mistral</code> directory on the <code class="literal">undercloud</code>:
						</p><pre class="screen">$ sudo setfacl -R -m u:stack:rwx /var/lib/mistral</pre><p class="simpara">
							This command retains <code class="literal">mistral</code> user access to the directory.
						</p></li><li class="listitem"><p class="simpara">
							Change to the working directory for the <code class="literal">config-download</code> files. This is usually <code class="literal">/var/lib/mistral/overcloud/</code>.
						</p><pre class="screen">$ cd /var/lib/mistral/overcloud/</pre></li><li class="listitem"><p class="simpara">
							Search the <code class="literal">ansible.log</code> file for the point of failure.
						</p><pre class="screen">$ less ansible.log</pre><p class="simpara">
							Make a note of the step that failed.
						</p></li><li class="listitem">
							Find the step that failed in the <code class="literal">config-download</code> playbooks within the working directory to identify the action that ocurred.
						</li></ol></div></section><section class="section" id="troubleshooting-container-configuration"><div class="titlepage"><div><div><h2 class="title">26.9. Troubleshooting container configuration</h2></div></div></div><p>
					OpenStack Platform director uses <code class="literal">paunch</code> to launch containers, <code class="literal">podman</code> to manage containers, and <code class="literal">puppet</code> to create container configuration. This procedure shows how to diagnose a container when errors occur.
				</p><div class="orderedlist"><p class="title"><strong>Accessing the host</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Get the IP address of the node with the container failure.
						</p><pre class="screen">(undercloud) $ openstack server list</pre></li><li class="listitem"><p class="simpara">
							Log in to the node:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.24.60</pre></li><li class="listitem"><p class="simpara">
							Change to the root user:
						</p><pre class="screen">$ sudo -i</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Identifying failed containers</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							View all containers:
						</p><pre class="screen">$ podman ps --all</pre><p class="simpara">
							Identify the failed container. The failed container usually exits with a non-zero status.
						</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Checking container logs</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Each container retains standard output from its main process. Use this output as a log to help determine what actually occurs during a container run. For example, to view the log for the <code class="literal">keystone</code> container, run the following command:
						</p><pre class="screen">$ sudo podman logs keystone</pre><p class="simpara">
							In most cases, this log contains information about the cause of a container failure.
						</p></li><li class="listitem"><p class="simpara">
							The host also retains the <code class="literal">stdout</code> log for the failed service. You can find the <code class="literal">stdout</code> logs in <code class="literal">/var/log/containers/stdouts/</code>. For example, to view the log for a failed <code class="literal">keystone</code> container, run the following command:
						</p><pre class="screen">$ cat /var/log/containers/stdouts/keystone.log</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Inspecting containers</strong></p><p>
						In some situations, you might need to verify information about a container. For example, use the following command to view <code class="literal">keystone</code> container data:
					</p></div><pre class="screen">$ sudo podman inspect keystone</pre><p>
					This command returns a JSON object containing low-level configuration data. You can pipe the output to the <code class="literal">jq</code> command to parse specific data. For example, to view the container mounts for the <code class="literal">keystone</code> container, run the following command:
				</p><pre class="screen">$ sudo podman inspect keystone | jq .[0].Mounts</pre><p>
					You can also use the <code class="literal">--format</code> option to parse data to a single line, which is useful for running commands against sets of container data. For example, to recreate the options used to run the <code class="literal">keystone</code> container, use the following <code class="literal">inspect</code> command with the <code class="literal">--format</code> option:
				</p><pre class="screen">$ sudo podman inspect --format='{{range .Config.Env}} -e "{{.}}" {{end}} {{range .Mounts}} -v {{.Source}}:{{.Destination}}:{{ join .Options "," }}{{end}} -ti {{.Config.Image}}' keystone</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">--format</code> option uses Go syntax to create queries.
					</p></div></div><p>
					Use these options in conjunction with the <code class="literal">podman run</code> command to recreate the container for troubleshooting purposes:
				</p><pre class="screen">$ OPTIONS=$( sudo podman inspect --format='{{range .Config.Env}} -e "{{.}}" {{end}} {{range .Mounts}} -v {{.Source}}:{{.Destination}}{{if .Mode}}:{{.Mode}}{{end}}{{end}} -ti {{.Config.Image}}' keystone )
$ sudo podman run --rm $OPTIONS /bin/bash</pre><div class="formalpara"><p class="title"><strong>Running commands in a container</strong></p><p>
						In some cases, you might need to obtain information from within a container through a specific Bash command. In this situation, use the following <code class="literal">podman</code> command to execute commands within a running container. For example, run the <code class="literal">podman exec</code> command to run a command inside the <code class="literal">keystone</code> container:
					</p></div><pre class="screen">$ sudo podman exec -ti keystone &lt;COMMAND&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">-ti</code> options run the command through an interactive pseudoterminal.
					</p></div></div><p>
					Replace <code class="literal">&lt;COMMAND&gt;</code> with the command you want to run. For example, each container has a health check script to verify the service connection. You can run the health check script for <code class="literal">keystone</code> with the following command:
				</p><pre class="screen">$ sudo podman exec -ti keystone /openstack/healthcheck</pre><p>
					To access the container shell, run <code class="literal">podman exec</code> using <code class="literal">/bin/bash</code> as the command you want to run inside the container:
				</p><pre class="screen">$ sudo podman exec -ti keystone /bin/bash</pre><div class="orderedlist"><p class="title"><strong>Viewing a container filesystem</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To view the file system for the failed container, run the <code class="literal">podman mount</code> command. For example, to view the file system for a failed <code class="literal">keystone</code> container, run the following command:
						</p><pre class="screen">$ podman mount keystone</pre><p class="simpara">
							This provides a mounted location to view the filesystem contents:
						</p><pre class="screen">/var/lib/containers/storage/overlay/78946a109085aeb8b3a350fc20bd8049a08918d74f573396d7358270e711c610/merged</pre><p class="simpara">
							This is useful for viewing the Puppet reports within the container. You can find these reports in the <code class="literal">var/lib/puppet/</code> directory within the container mount.
						</p></li></ol></div><div class="formalpara"><p class="title"><strong>Exporting a container</strong></p><p>
						When a container fails, you might need to investigate the full contents of the file. In this case, you can export the full file system of a container as a <code class="literal">tar</code> archive. For example, to export the <code class="literal">keystone</code> container file system, run the following command:
					</p></div><pre class="screen">$ sudo podman export keystone -o keystone.tar</pre><p>
					This command creates the <code class="literal">keystone.tar</code> archive, which you can extract and explore.
				</p></section><section class="section" id="sect-Compute_Service_Failures"><div class="titlepage"><div><div><h2 class="title">26.10. Troubleshooting Compute node failures</h2></div></div></div><p>
					Compute nodes use the Compute service to perform hypervisor-based operations. This means the main diagnosis for Compute nodes revolves around this service.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Source the <code class="literal">stackrc</code> file:
						</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem"><p class="simpara">
							Get the IP address of the Compute node that contains the failure:
						</p><pre class="screen">(undercloud) $ openstack server list</pre></li><li class="listitem"><p class="simpara">
							Log in to the node:
						</p><pre class="screen">(undercloud) $ ssh heat-admin@192.168.24.60</pre></li><li class="listitem"><p class="simpara">
							Change to the root user:
						</p><pre class="screen">$ sudo -i</pre></li><li class="listitem"><p class="simpara">
							View the status of the container:
						</p><pre class="screen">$ sudo podman ps -f name=nova_compute</pre></li><li class="listitem">
							The primary log file for Compute nodes is <code class="literal">/var/log/containers/nova/nova-compute.log</code>. If issues occur with Compute node communication, use this file to begin the diagnosis.
						</li><li class="listitem">
							If you perform maintenance on the Compute node, migrate the existing instances from the host to an operational Compute node, then disable the node.
						</li></ol></div></section><section class="section" id="creating-an-sosreport"><div class="titlepage"><div><div><h2 class="title">26.11. Creating an sosreport</h2></div></div></div><p>
					If you need to contact Red Hat for support with Red Hat OpenStack Platform, you might need to generate an <code class="literal">sosreport</code>. For more information about creating an <code class="literal">sosreport</code>, see:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<a class="link" href="https://access.redhat.com/solutions/2055933">"How to collect all required logs for Red Hat Support to investigate an OpenStack issue"</a>
						</li></ul></div></section><section class="section" id="log-locations"><div class="titlepage"><div><div><h2 class="title">26.12. Log locations</h2></div></div></div><p>
					Use the following logs to gather information about the undercloud and overcloud when you troubleshoot issues.
				</p><div class="table" id="idm140301167005280"><p class="title"><strong>Table 26.1. Logs on both the undercloud and overcloud nodes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301167000416" scope="col">Information</th><th align="left" valign="top" id="idm140301166999328" scope="col">Log location</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301167000416"> <p>
									Containerized service logs
								</p>
								 </td><td align="left" valign="top" headers="idm140301166999328"> <p>
									<code class="literal">/var/log/containers/</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167000416"> <p>
									Standard output from containerized services
								</p>
								 </td><td align="left" valign="top" headers="idm140301166999328"> <p>
									<code class="literal">/var/log/containers/stdouts</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167000416"> <p>
									Ansible configuration logs
								</p>
								 </td><td align="left" valign="top" headers="idm140301166999328"> <p>
									<code class="literal">/var/lib/mistral/overcloud/ansible.log</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140301166984768"><p class="title"><strong>Table 26.2. Additional logs on the undercloud node</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301166979920" scope="col">Information</th><th align="left" valign="top" id="idm140301166978832" scope="col">Log location</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301166979920"> <p>
									Command history for <code class="literal">openstack overcloud deploy</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301166978832"> <p>
									<code class="literal">/home/stack/.tripleo/history</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301166979920"> <p>
									Undercloud installation log
								</p>
								 </td><td align="left" valign="top" headers="idm140301166978832"> <p>
									<code class="literal">/home/stack/install-undercloud.log</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140301164598080"><p class="title"><strong>Table 26.3. Additional logs on the overcloud nodes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301164593232" scope="col">Information</th><th align="left" valign="top" id="idm140301164592144" scope="col">Log location</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301164593232"> <p>
									Cloud-Init Log
								</p>
								 </td><td align="left" valign="top" headers="idm140301164592144"> <p>
									<code class="literal">/var/log/cloud-init.log</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164593232"> <p>
									High availability log
								</p>
								 </td><td align="left" valign="top" headers="idm140301164592144"> <p>
									<code class="literal">/var/log/pacemaker.log</code>
								</p>
								 </td></tr></tbody></table></div></div></section></section><section class="chapter" id="tips-for-undercloud-and-overcloud-services"><div class="titlepage"><div><div><h2 class="title">Chapter 27. Tips for undercloud and overcloud services</h2></div></div></div><p>
				This section provides advice on tuning and managing specific OpenStack services on the undercloud.
			</p><section class="section" id="review-the-database-flush-intervals"><div class="titlepage"><div><div><h2 class="title">27.1. Review the database flush intervals</h2></div></div></div><p>
					Some services use a <code class="literal">cron</code> container to flush old content from the database.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							OpenStack Identity (keystone): Flush expired tokens.
						</li><li class="listitem">
							OpenStack Orchestration (heat): Flush expired deleted template data.
						</li><li class="listitem">
							OpenStack Compute (nova): Flush expired deleted instance data.
						</li></ul></div><p>
					The default flush periods for each service are listed in this table:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301164567744" scope="col">Service</th><th align="left" valign="top" id="idm140301164566656" scope="col">Database content flushed</th><th align="left" valign="top" id="idm140301164565552" scope="col">Default flush period</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301164567744"> <p>
									OpenStack Identity (keystone)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164566656"> <p>
									Expired tokens
								</p>
								 </td><td align="left" valign="top" headers="idm140301164565552"> <p>
									Every hour
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164567744"> <p>
									OpenStack Orchestration (heat)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164566656"> <p>
									Deleted template data that has expired and is older than 30 days
								</p>
								 </td><td align="left" valign="top" headers="idm140301164565552"> <p>
									Every day
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164567744"> <p>
									OpenStack Compute (nova)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164566656"> <p>
									Archive deleted instance data
								</p>
								 </td><td align="left" valign="top" headers="idm140301164565552"> <p>
									Every day
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301164567744"> <p>
									OpenStack Compute (nova)
								</p>
								 </td><td align="left" valign="top" headers="idm140301164566656"> <p>
									Flush archived data older than 14 days
								</p>
								 </td><td align="left" valign="top" headers="idm140301164565552"> <p>
									Every day
								</p>
								 </td></tr></tbody></table></div><p>
					The following tables outline the parameters that you can use to control these <code class="literal">cron</code> jobs.
				</p><div class="table" id="idm140301167634128"><p class="title"><strong>Table 27.1. OpenStack Identity (keystone) cron parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301167629264" scope="col">Parameter</th><th align="left" valign="top" id="idm140301167628176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301167629264"> <p>
									<code class="literal">KeystoneCronTokenFlushMinute</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301167628176"> <p>
									Cron to purge expired tokens - Minute. The default value is: <code class="literal">1</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167629264"> <p>
									<code class="literal">KeystoneCronTokenFlushHour</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301167628176"> <p>
									Cron to purge expired tokens - Hour. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167629264"> <p>
									<code class="literal">KeystoneCronTokenFlushMonthday</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301167628176"> <p>
									Cron to purge expired tokens - Month Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167629264"> <p>
									<code class="literal">KeystoneCronTokenFlushMonth</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301167628176"> <p>
									Cron to purge expired tokens - Month. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301167629264"> <p>
									<code class="literal">KeystoneCronTokenFlushWeekday</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301167628176"> <p>
									Cron to purge expired tokens - Week Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140301167602736"><p class="title"><strong>Table 27.2. OpenStack Orchestration (heat) cron parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301169102048" scope="col">Parameter</th><th align="left" valign="top" id="idm140301169100960" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedAge</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Age. The default value is: <code class="literal">30</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedAgeType</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Age type. The default value is: <code class="literal">days</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedMinute</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Minute. The default value is: <code class="literal">1</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedHour</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Hour. The default value is: <code class="literal">0</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedMonthday</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Month Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedMonth</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Month. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140301169102048"> <p>
									<code class="literal">HeatCronPurgeDeletedWeekday</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140301169100960"> <p>
									Cron to purge database entries marked as deleted and older than $age - Week Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr></tbody></table></div></div><div class="table" id="idm140301169065952"><p class="title"><strong>Table 27.3. OpenStack Compute (nova) cron parameters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top"> <p>
									Parameter
								</p>
								 </td><td align="left" valign="top"> <p>
									Description
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsMaxRows</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Max Rows. The default value is: <code class="literal">100</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsPurge</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Purge shadow tables immediately after scheduled archiving. The default value is: <code class="literal">False</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsMinute</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Minute. The default value is: <code class="literal">1</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsHour</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Hour. The default value is: <code class="literal">0</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsMonthday</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Month Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsMonth</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Month. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsWeekday</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Week Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronArchiveDeleteRowsUntilComplete</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to move deleted instances to another table - Until complete. The default value is: <code class="literal">True</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesAge</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Age This will define the retention policy when purging the shadow tables in days. 0 means, purge data older than today in shadow tables. The default value is: <code class="literal">14</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesMinute</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Minute. The default value is: <code class="literal">0</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesHour</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Hour. The default value is: <code class="literal">5</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesMonthday</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Month Day. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesMonth</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Month. The default value is: <code class="literal">*</code>
								</p>
								 </td></tr><tr><td align="left" valign="top"> <p>
									<code class="literal">NovaCronPurgeShadowTablesWeekday</code>
								</p>
								 </td><td align="left" valign="top"> <p>
									Cron to purge shadow tables - Week Day. The default value is: <code class="literal">*`</code>
								</p>
								 </td></tr></tbody></table></div></div><p>
					To adjust these intervals, create an environment file that contains your token flush interval for the respective services and add this file to the <code class="literal">custom_env_files</code> parameter in your <code class="literal">undercloud.conf</code> file. For example, to change the OpenStack Identity (keystone) token flush to 30 minutes, use the following snippets
				</p><div class="formalpara"><p class="title"><strong>keystone-cron.yaml</strong></p><p>
						
<pre class="screen">parameter_defaults:
  KeystoneCronTokenFlushMinute: '0/30'</pre>

					</p></div><div class="formalpara"><p class="title"><strong>undercloud.yaml</strong></p><p>
						
<pre class="screen">custom_env_files: keystone-cron.yaml</pre>

					</p></div><p>
					Then rerun the <code class="literal">openstack undercloud install</code> command.
				</p><pre class="screen">$ openstack undercloud install</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use these parameters for your overcloud. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/overcloud_parameters/index"><span class="emphasis"><em>Overcloud Parameters</em></span></a> guide.
					</p></div></div></section><section class="section" id="tuning-deployment-performance"><div class="titlepage"><div><div><h2 class="title">27.2. Tuning deployment performance</h2></div></div></div><p>
					OpenStack Platform director uses OpenStack Orchestration (heat) to conduct the main deployment and provisioning functions. Heat uses a series of workers to execute deployment tasks. To calculate the default number of workers, the director heat configuration halves the total CPU thread count of the undercloud. In this instance, thread count refers to the number of CPU cores multiplied by the hyper-threading value]. For example, if your undercloud has a CPU with 16 threads, heat spawns 8 workers by default. The director configuration also uses a minimum and maximum cap by default:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140301165573024" scope="col">Service</th><th align="left" valign="top" id="idm140301165571936" scope="col">Minimum</th><th align="left" valign="top" id="idm140301165570848" scope="col">Maximum</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140301165573024"> <p>
									OpenStack Orchestration (heat)
								</p>
								 </td><td align="left" valign="top" headers="idm140301165571936"> <p>
									4
								</p>
								 </td><td align="left" valign="top" headers="idm140301165570848"> <p>
									24
								</p>
								 </td></tr></tbody></table></div><p>
					However, you can set the number of workers manually with the <code class="literal">HeatWorkers</code> parameter in an environment file:
				</p><div class="formalpara"><p class="title"><strong>heat-workers.yaml</strong></p><p>
						
<pre class="screen">parameter_defaults:
  HeatWorkers: 16</pre>

					</p></div><div class="formalpara"><p class="title"><strong>undercloud.yaml</strong></p><p>
						
<pre class="screen">custom_env_files: heat-workers.yaml</pre>

					</p></div></section><section class="section" id="running-swift-ringbuilder-in-a-container"><div class="titlepage"><div><div><h2 class="title">27.3. Running swift-ring-builder in a container</h2></div></div></div><p>
					To manage your Object Storage (swift) rings, use the <code class="literal">swift-ring-builder</code> commands inside the server containers:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">swift_object_server</code>
						</li><li class="listitem">
							<code class="literal">swift_container_server</code>
						</li><li class="listitem">
							<code class="literal">swift_account_server</code>
						</li></ul></div><p>
					For example, to view information about your swift object rings, run the following command:
				</p><pre class="screen">$ sudo podman exec -ti -u swift swift_object_server swift-ring-builder /etc/swift/object.builder</pre><p>
					You can run this command on both the undercloud and overcloud nodes.
				</p></section><section class="section" id="changing-the-ssl-tls-cipher-rules-for-haproxy"><div class="titlepage"><div><div><h2 class="title">27.4. Changing the SSL/TLS cipher rules for HAProxy</h2></div></div></div><p>
					If you enabled SSL/TLS in the undercloud (see <a class="xref" href="index.html#director-configuration-parameters" title="4.2. Director configuration parameters">Section 4.2, “Director configuration parameters”</a>), you might want to harden the SSL/TLS ciphers and rules that are used with the HAProxy configuration. This hardening helps to avoid SSL/TLS vulnerabilities, such as the <a class="link" href="https://access.redhat.com/solutions/1291123">POODLE vulnerability</a>.
				</p><p>
					Set the following hieradata using the <code class="literal">hieradata_override</code> undercloud configuration option:
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">tripleo::haproxy::ssl_cipher_suite</span></dt><dd>
								The cipher suite to use in HAProxy.
							</dd><dt><span class="term">tripleo::haproxy::ssl_options</span></dt><dd>
								The SSL/TLS rules to use in HAProxy.
							</dd></dl></div><p>
					For example, you might want to use the following cipher and rules:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Cipher: <code class="literal">ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS</code>
						</li><li class="listitem">
							Rules: <code class="literal">no-sslv3 no-tls-tickets</code>
						</li></ul></div><p>
					Create a hieradata override file (<code class="literal">haproxy-hiera-overrides.yaml</code>) with the following content:
				</p><pre class="screen">tripleo::haproxy::ssl_cipher_suite: ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS
tripleo::haproxy::ssl_options: no-sslv3 no-tls-tickets</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The cipher collection is one continuous line.
					</p></div></div><p>
					Set the <code class="literal">hieradata_override</code> parameter in the <code class="literal">undercloud.conf</code> file to use the hieradata override file you created before you ran <code class="literal">openstack undercloud install</code>:
				</p><pre class="screen">[DEFAULT]
...
hieradata_override = haproxy-hiera-overrides.yaml
...</pre></section></section></div><div class="part" id="appendices"><div class="titlepage"><div><div><h1 class="title">Part VI. Appendices</h1></div></div></div><section class="appendix" id="appe-Power_Management_Drivers"><div class="titlepage"><div><div><h2 class="title">Appendix A. Power management drivers</h2></div></div></div><p>
				Although IPMI is the main method that director uses for power management control, director also supports other power management types. This appendix contains a list of the power management features that director supports. Use these power management settings when you register nodes for the overcloud. For more information, see <a class="link" href="index.html#sect-Registering_Nodes_for_the_Overcloud">Registering nodes for the overcloud</a>.
			</p><section class="section" id="sect-IPMI"><div class="titlepage"><div><div><h2 class="title">A.1. Intelligent Platform Management Interface (IPMI)</h2></div></div></div><p>
					The standard power management method when you use a baseboard management controller (BMC).
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">ipmi</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The IPMI username and password.
							</dd><dt><span class="term">pm_addr</span></dt><dd>
								The IP address of the IPMI controller.
							</dd><dt><span class="term">pm_port (Optional)</span></dt><dd>
								The port to connect to the IPMI controller.
							</dd></dl></div></section><section class="section" id="sect-Redfish"><div class="titlepage"><div><div><h2 class="title">A.2. Redfish</h2></div></div></div><p>
					A standard RESTful API for IT infrastructure developed by the Distributed Management Task Force (DMTF)
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">redfish</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The Redfish username and password.
							</dd><dt><span class="term">pm_addr</span></dt><dd>
								The IP address of the Redfish controller.
							</dd><dt><span class="term">pm_system_id</span></dt><dd>
								The canonical path to the system resource. This path must include the root service, version, and the path/unqiue ID for the system. For example: <code class="literal">/redfish/v1/Systems/CX34R87</code>.
							</dd><dt><span class="term">redfish_verify_ca</span></dt><dd>
								If the Redfish service in your baseboard management controller (BMC) is not configured to use a valid TLS certificate signed by a recognized certificate authority (CA), the Redfish client in ironic fails to connect to the BMC. Set the <code class="literal">redfish_verify_ca</code> option to <code class="literal">false</code> to mute the error. However, be aware that disabling BMC authentication compromises the access security of your BMC.
							</dd></dl></div></section><section class="section" id="sect-Dell_Remote_Access_Controller_DRAC"><div class="titlepage"><div><div><h2 class="title">A.3. Dell Remote Access Controller (DRAC)</h2></div></div></div><p>
					DRAC is an interface that provides out-of-band remote management features including power management and server monitoring.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">idrac</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The DRAC username and password.
							</dd><dt><span class="term">pm_addr</span></dt><dd>
								The IP address of the DRAC host.
							</dd></dl></div></section><section class="section" id="sect-Integrated_LightsOut_iLO"><div class="titlepage"><div><div><h2 class="title">A.4. Integrated Lights-Out (iLO)</h2></div></div></div><p>
					iLO from Hewlett-Packard is an interface that provides out-of-band remote management features including power management and server monitoring.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">ilo</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The iLO username and password.
							</dd><dt><span class="term">pm_addr</span></dt><dd><p class="simpara">
								The IP address of the iLO interface.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To enable this driver, add <code class="literal">ilo</code> to the <code class="literal">enabled_hardware_types</code> option in your <code class="literal">undercloud.conf</code> and rerun <code class="literal">openstack undercloud install</code>.
									</li><li class="listitem"><p class="simpara">
										Director also requires an additional set of utilities for iLo. Install the <code class="literal">python3-proliantutils</code> package and restart the <code class="literal">tripleo-ironic-conductor</code> service:
									</p><pre class="screen">$ sudo dnf install python3-proliantutils
$ sudo systemctl restart tripleo-ironic-conductor.service</pre></li><li class="listitem">
										HP nodes must have a minimum ILO firmware version of 1.85 (May 13 2015) for successful introspection. Director has been successfully tested with nodes using this ILO firmware version.
									</li><li class="listitem">
										Using a shared iLO port is not supported.
									</li></ul></div></dd></dl></div></section><section class="section" id="sect-Fujitsu_Integrated_Remote_Management_Controller_iRMC"><div class="titlepage"><div><div><h2 class="title">A.5. Fujitsu Integrated Remote Management Controller (iRMC)</h2></div></div></div><p>
					Fujitsu iRMC is a Baseboard Management Controller (BMC) with integrated LAN connection and extended functionality. This driver focuses on the power management for bare metal systems connected to the iRMC.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						iRMC S4 or higher is required.
					</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">irmc</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The username and password for the iRMC interface.
							</dd><dt><span class="term">pm_addr</span></dt><dd>
								The IP address of the iRMC interface.
							</dd><dt><span class="term">pm_port (Optional)</span></dt><dd>
								The port to use for iRMC operations. The default is 443.
							</dd><dt><span class="term">pm_auth_method (Optional)</span></dt><dd>
								The authentication method for iRMC operations. Use either <code class="literal">basic</code> or <code class="literal">digest</code>. The default is <code class="literal">basic</code>
							</dd><dt><span class="term">pm_client_timeout (Optional)</span></dt><dd>
								Timeout (in seconds) for iRMC operations. The default is 60 seconds.
							</dd><dt><span class="term">pm_sensor_method (Optional)</span></dt><dd><p class="simpara">
								Sensor data retrieval method. Use either <code class="literal">ipmitool</code> or <code class="literal">scci</code>. The default is <code class="literal">ipmitool</code>.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To enable this driver, add <code class="literal">irmc</code> to the <code class="literal">enabled_hardware_types</code> option in your <code class="literal">undercloud.conf</code> and rerun the <code class="literal">openstack undercloud install</code> command.
									</li><li class="listitem"><p class="simpara">
										If you enable SCCI as the sensor method, you must also install an additional set of utilities. Install the <code class="literal">python3-scciclient</code> package and restart the <code class="literal">tripleo-ironic-conductor</code> service:
									</p><pre class="screen">$ dnf install python3-scciclient
$ sudo systemctl restart tripleo-ironic-conductor.service</pre></li></ul></div></dd></dl></div></section><section class="section" id="sect-Red_Hat_Virtualization"><div class="titlepage"><div><div><h2 class="title">A.6. Red Hat Virtualization</h2></div></div></div><p>
					This driver provides control over virtual machines in Red Hat Virtualization (RHV) through its RESTful API.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd>
								Set this option to <code class="literal">staging-ovirt</code>.
							</dd><dt><span class="term">pm_user; pm_password</span></dt><dd>
								The username and password for your RHV environment. The username also includes the authentication provider. For example: <code class="literal">admin@internal</code>.
							</dd><dt><span class="term">pm_addr</span></dt><dd>
								The IP address of the RHV REST API.
							</dd><dt><span class="term">pm_vm_name</span></dt><dd>
								The name of the virtual machine to control.
							</dd><dt><span class="term">mac</span></dt><dd><p class="simpara">
								A list of MAC addresses for the network interfaces on the node. Use only the MAC address for the Provisioning NIC of each system.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										To enable this driver, add <code class="literal">staging-ovirt</code> to the <code class="literal">enabled_hardware_types</code> option in your <code class="literal">undercloud.conf</code> and rerun the <code class="literal">openstack undercloud install</code> command.
									</li></ul></div></dd></dl></div></section><section class="section" id="sect-manual-management-driver"><div class="titlepage"><div><div><h2 class="title">A.7. manual-management Driver</h2></div></div></div><p>
					Use the <code class="literal">manual-management</code> driver to control bare metal devices that do not have power management. Director does not control the registered bare metal devices, and you must perform manual power operations at certain points in the introspection and deployment processes.
				</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						This option is available only for testing and evaluation purposes. It is not recommended for Red Hat OpenStack Platform enterprise environments.
					</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">pm_type</span></dt><dd><p class="simpara">
								Set this option to <code class="literal">manual-management</code>.
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										This driver does not use any authentication details because it does not control power management.
									</li><li class="listitem">
										To enable this driver, add <code class="literal">manual-management</code> to the <code class="literal">enabled_hardware_types</code> option in your <code class="literal">undercloud.conf</code> and rerun the <code class="literal">openstack undercloud install</code> command.
									</li><li class="listitem">
										In your <code class="literal">instackenv.json</code> node inventory file, set the <code class="literal">pm_type</code> to <code class="literal">manual-management</code> for the nodes that you want to manage manually.
									</li><li class="listitem">
										When performing introspection on nodes, manually start the nodes after running the <code class="literal">openstack overcloud node introspect</code> command.
									</li><li class="listitem">
										When performing overcloud deployment, check the node status with the <code class="literal">ironic node-list</code> command. Wait until the node status changes from <code class="literal">deploying</code> to <code class="literal">deploy wait-callback</code> and then manually start the nodes.
									</li><li class="listitem">
										After the overcloud provisioning process completes, reboot the nodes. To check the completion of provisioning, check the node status with the <code class="literal">openstack baremetal node list</code> command, wait until the node status changes to <code class="literal">active</code>, then manually reboot all overcloud nodes.
									</li></ul></div></dd></dl></div></section></section><section class="appendix" id="appe-OSP_on_POWER"><div class="titlepage"><div><div><h2 class="title">Appendix B. Red Hat OpenStack Platform for POWER</h2></div></div></div><p>
				In a new Red Hat OpenStack Platform installation, you can deploy overcloud Compute nodes on POWER (ppc64le) hardware. For the Compute node cluster, you can use the same architecture, or use a combination of x86_64 and ppc64le systems. The undercloud, Controller nodes, Ceph Storage nodes, and all other systems are supported only on x86_64 hardware.
			</p><section class="section" id="ceph_storage"><div class="titlepage"><div><div><h2 class="title">B.1. Ceph Storage</h2></div></div></div><p>
					When you configure access to external Ceph in a multi-architecture cloud, set the <code class="literal">CephAnsiblePlaybook</code> parameter to <code class="literal">/usr/share/ceph-ansible/site.yml.sample</code> and include your client key and other Ceph-specific parameters.
				</p><p>
					For example:
				</p><pre class="screen">parameter_defaults:
  CephAnsiblePlaybook: /usr/share/ceph-ansible/site.yml.sample
  CephClientKey: AQDLOh1VgEp6FRAAFzT7Zw+Y9V6JJExQAsRnRQ==
  CephClusterFSID: 4b5c8c0a-ff60-454b-a1b4-9747aa737d19
  CephExternalMonHost: 172.16.1.7, 172.16.1.8</pre></section><section class="section" id="composable_services"><div class="titlepage"><div><div><h2 class="title">B.2. Composable services</h2></div></div></div><p>
					The following services typically form part of the Controller node and are available for use in custom roles as Technology Preview:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Block Storage service (cinder)
						</li><li class="listitem">
							Image service (glance)
						</li><li class="listitem">
							Identity service (keystone)
						</li><li class="listitem">
							Networking service (neutron)
						</li><li class="listitem">
							Object Storage service (swift)
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Red Hat does not support features in Technology Preview.
					</p></div></div><p>
					For more information about composable services, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/#Roles">composable services and custom roles</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide. Use the following example to understand how to move the listed services from the Controller node to a dedicated ppc64le node:
				</p><pre class="screen">(undercloud) [stack@director ~]$ rsync -a /usr/share/openstack-tripleo-heat-templates/. ~/templates
(undercloud) [stack@director ~]$ cd ~/templates/roles
(undercloud) [stack@director roles]$ cat &lt;&lt;EO_TEMPLATE &gt;ControllerPPC64LE.yaml
###############################################################################
# Role: ControllerPPC64LE                                                     #
###############################################################################
- name: ControllerPPC64LE
  description: |
    Controller role that has all the controller services loaded and handles
    Database, Messaging and Network functions.
  CountDefault: 1
  tags:
    - primary
    - controller
  networks:
    - External
    - InternalApi
    - Storage
    - StorageMgmt
    - Tenant
  # For systems with both IPv4 and IPv6, you may specify a gateway network for
  # each, such as ['ControlPlane', 'External']
  default_route_networks: ['External']
  HostnameFormatDefault: '%stackname%-controllerppc64le-%index%'
  ImageDefault: ppc64le-overcloud-full
  ServicesDefault:
    - OS::TripleO::Services::Aide
    - OS::TripleO::Services::AuditD
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CephClient
    - OS::TripleO::Services::CephExternal
    - OS::TripleO::Services::CertmongerUser
    - OS::TripleO::Services::CinderApi
    - OS::TripleO::Services::CinderBackendDellPs
    - OS::TripleO::Services::CinderBackendDellSc
    - OS::TripleO::Services::CinderBackendDellEMCUnity
    - OS::TripleO::Services::CinderBackendDellEMCVMAXISCSI
    - OS::TripleO::Services::CinderBackendDellEMCVNX
    - OS::TripleO::Services::CinderBackendDellEMCXTREMIOISCSI
    - OS::TripleO::Services::CinderBackendNetApp
    - OS::TripleO::Services::CinderBackendScaleIO
    - OS::TripleO::Services::CinderBackendVRTSHyperScale
    - OS::TripleO::Services::CinderBackup
    - OS::TripleO::Services::CinderHPELeftHandISCSI
    - OS::TripleO::Services::CinderScheduler
    - OS::TripleO::Services::CinderVolume
    - OS::TripleO::Services::Collectd
    - OS::TripleO::Services::Docker
    - OS::TripleO::Services::Fluentd
    - OS::TripleO::Services::GlanceApi
    - OS::TripleO::Services::GlanceRegistry
    - OS::TripleO::Services::Ipsec
    - OS::TripleO::Services::Iscsid
    - OS::TripleO::Services::Kernel
    - OS::TripleO::Services::Keystone
    - OS::TripleO::Services::LoginDefs
    - OS::TripleO::Services::MySQLClient
    - OS::TripleO::Services::NeutronApi
    - OS::TripleO::Services::NeutronBgpVpnApi
    - OS::TripleO::Services::NeutronSfcApi
    - OS::TripleO::Services::NeutronCorePlugin
    - OS::TripleO::Services::NeutronDhcpAgent
    - OS::TripleO::Services::NeutronL2gwAgent
    - OS::TripleO::Services::NeutronL2gwApi
    - OS::TripleO::Services::NeutronL3Agent
    - OS::TripleO::Services::NeutronLbaasv2Agent
    - OS::TripleO::Services::NeutronLbaasv2Api
    - OS::TripleO::Services::NeutronLinuxbridgeAgent
    - OS::TripleO::Services::NeutronMetadataAgent
    - OS::TripleO::Services::NeutronML2FujitsuCfab
    - OS::TripleO::Services::NeutronML2FujitsuFossw
    - OS::TripleO::Services::NeutronOvsAgent
    - OS::TripleO::Services::NeutronVppAgent
    - OS::TripleO::Services::Ntp
    - OS::TripleO::Services::ContainersLogrotateCrond
    - OS::TripleO::Services::OpenDaylightOvs
    - OS::TripleO::Services::Rhsm
    - OS::TripleO::Services::RsyslogSidecar
    - OS::TripleO::Services::Securetty
    - OS::TripleO::Services::SensuClient
    - OS::TripleO::Services::SkydiveAgent
    - OS::TripleO::Services::Snmp
    - OS::TripleO::Services::Sshd
    - OS::TripleO::Services::SwiftProxy
    - OS::TripleO::Services::SwiftDispersion
    - OS::TripleO::Services::SwiftRingBuilder
    - OS::TripleO::Services::SwiftStorage
    - OS::TripleO::Services::Timezone
    - OS::TripleO::Services::TripleoFirewall
    - OS::TripleO::Services::TripleoPackages
    - OS::TripleO::Services::Tuned
    - OS::TripleO::Services::Vpp
    - OS::TripleO::Services::OVNController
    - OS::TripleO::Services::OVNMetadataAgent
    - OS::TripleO::Services::Ptp
EO_TEMPLATE
(undercloud) [stack@director roles]$ sed -i~ -e '/OS::TripleO::Services::\(Cinder\|Glance\|Swift\|Keystone\|Neutron\)/d' Controller.yaml
(undercloud) [stack@director roles]$ cd ../
(undercloud) [stack@director templates]$ openstack overcloud roles generate \
    --roles-path roles -o roles_data.yaml \
    Controller Compute ComputePPC64LE ControllerPPC64LE BlockStorage ObjectStorage CephStorage</pre></section></section></div><div><div xml:lang="en-US" class="legalnotice" id="idm140301166572880"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2020 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div>
  
  </div>
  </div>
</div>
<div id="comments-footer" class="book-comments">
  </div>
<meta itemscope="" itemref="md1">

    <!-- Display: Next/Previous Nav -->
          
      </div>
</article>


  

            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="index.html#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

            <div role="navigation">
                <h3>Quick Links</h3>
                <ul>
                    <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li><a class="manage-subscriptions" href="https://access.redhat.com/management/subscriptions/#active">Subscriptions</a></li>
                    <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                    <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                    <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Help</h3>
                <ul>
                    <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                    <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                    <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Site Info</h3>
                <ul>
                  <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                  <li><a class="browser-support-policy" href="https://access.redhat.com/help/browsers/">Browser Support Policy</a></li>
                  <li><a class="accessibility" href="https://access.redhat.com/help/accessibility/">Accessibility</a></li>
                  <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                  <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Related Sites</h3>
                <ul>
                    <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                    <li><a href="https://www.openshift.com" class="openshift-com">openshift.com</a></li>
                    <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                    <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>About</h3>
                <ul>
                    <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                    <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                    <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                </ul>
            </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                          <span class="status-description"></span>
                          <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2020 Red Hat, Inc.</div>

                        <div role="navigation" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://access.redhat.com/help/terms/" class="terms-of-use">Customer Portal Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                      <img src="rh-summit-red-a.svg" alt="Red Hat Summit" />
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHatSupport" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>
                        <a href="https://www.facebook.com/RedHatSupport" class="sm-icon facebook"><span class="nicon-facebook"></span><span class="offscreen">Facebook</span></a>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
  
  <div id="formatHelp" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="formatTitle" aria-hidden="true"><div class="modal-dialog"><div class="modal-content">
    <div class="modal-header">
      <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
      <h3 id="formatTitle">Formatting Tips</h3>
    </div>
    <div class="modal-body">
      <p>Here are the common uses of Markdown.</p><dl class="formatting-help">
        <dt class="codeblock">Code blocks</dt><dd class="codeblock"><pre><code>~~~
Code surrounded in tildes is easier to read
~~~</code></pre></dd>
        <dt class="urls">Links/URLs</dt><dd class="urls"><code>[Red Hat Customer Portal](https://access.redhat.com)</code></dd>
       </dl>
    </div>
    <div class="modal-footer">
      <a target="_blank" href="https://access.redhat.com/help/markdown" class="btn btn-primary">Learn more</a>
      <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
    </div>
  </div></div></div></body>
</html>

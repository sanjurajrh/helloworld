<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video#" >

<head profile="http://www.w3.org/1999/xhtml/vocab">
	  <!--[if IE]><![endif]-->
<meta charset="utf-8" />
<meta name="revision" title="2c6974ff-d99c-4104-bac9-ea4dabd183b5" product="7eff92b4-effc-4326-8604-6f852e3e9747" revision="c2973328ba7082bc1ee91cdd08745139619ae7de:en-us" page="2afaaa4f-4525-4e0d-87d4-e3d15ccee9fc" />
<meta name="generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/index" />
<meta property="og:title" content="Instances and Images Guide Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta property="og:description" content="The Instances and Images guide provides procedures for the management of instances, images of a Red Hat OpenStack Platform environment." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/index" />
<meta name="twitter:title" content="Instances and Images Guide Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta name="twitter:description" content="The Instances and Images guide provides procedures for the management of instances, images of a Red Hat OpenStack Platform environment." />
<meta name="twitter:image:src" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
  <title>Instances and Images Guide Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal</title>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="kcs07.web.prod.ext.phx2.redhat.com" />
<meta name="avalon-version" content="fc526cb5" />
<meta name="cp-chrome-build-date" content="2020-10-15T21:45:53.836Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />

 

<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en",  
        version   : "fc526cb5",
        builddate : "2020-10-15T21:45:53.836Z",
        fetchdate : "2020-10-19T10:38:31-0400",
        nrid      : "14615289",
        nrlk      : "2a497fa56f"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="favicon.ico" />

<link media="all" rel="stylesheet" type="text/css" href="bootstrap.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="bootstrap-grid.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="main.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="components.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="pages.css%3Fv=fc526cb5.css" />

<link href="chosen.css%3Fv=fc526cb5.css" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]-->

<noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>

<!-- /cssInclude -->
<script type="text/javascript" src="require.js%3Fv=fc526cb5" data-main="/webassets/avalon/j/"></script>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->

  <!-- TrustArc -->
  <script src="https://static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>

  <link type="text/css" rel="stylesheet" href="css__c2Nkkx_5vYh8rvZbfBAGB4EMzMtH5ouFJDBlDsnhSR8__-ax0_8bam15ZDJT9j7LilCfJrDyEhGGAgc0KC8HYvJg__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__Jy3BSr8TrxptaufAzQDT1skBUlX2CnL_wm6BizzYuGw__-YZvqB8yuA4kK_iKklbk5HdZCoG2qAgz1l-8Qi2NFH4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="messages.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__m76iIFREtc70Nmw5xe1ZwHbNBlwOP2Zjc3DcacNWnFQ__STEh8aY3w8E_bKzhB4Xke2WOQ9XMDQquHIP5B8SeDIY__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__4sM4s6XOQ2Nm0pdkrWRLvrgtwpTmHAFzR_LcdesKYj8__vJ0jjVEhoPh3KBeLZfkqg51T3AFxhQCuXg1reipa__k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="print" />
  <link rel="stylesheet" type="text/css" media="all" href="list.css" />
  <script src="js__ZyeOaiFuDejQQbhUV7yg7atYZnj4WLfH77o0scv4068__MZdWWgUEYpsEWLcU0RqkaXMsEyksbpCgnf4XwXRkqz0__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__5ysXPc5KIyMmizxqRY68ILfrEGrj0P29WBIifnPTJvQ__Cap0DACEVMsefumg1gS1APLLd8stDkdGfp6c1uswMo4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>chrometwo_require(["analytics/attributes"], function(attributes) {
attributes.set('ResourceID',       '2c6974ff-d99c-4104-bac9-ea4dabd183b5');
attributes.set('ResourceTitle',    'Instances and Images Guide');
attributes.set('Language',         'en-us');
attributes.set('RevisionId',       'c2973328ba7082bc1ee91cdd08745139619ae7de:en-us');
attributes.set('PublicationState', ['active','published']);
attributes.set('Product',          'Red Hat OpenStack Platform');
attributes.set('ProductVersion',   '16.1');
attributes.set('ProductId',        'Red Hat OpenStack Platform 16.1');
});</script>
<script>breadcrumbs = [ ["Products &amp; Services", "/products/"], ["Product Documentation", "/documentation/"], ["Red Hat OpenStack Platform", "/documentation/en-us/red_hat_openstack_platform/"], ["16.1", "/documentation/en-us/red_hat_openstack_platform/16.1/"], ["Instances and Images Guide", "/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/"] ];</script>
<script>window.siteMapState = "products";</script>
<script src="js__i6ieGBO-OPmIKm_f0srsb6gM7QELIWrBpKh6ub_yj8A__wjRg_duSK4rEZCRV2uwrCIPMl80Z_LJ9ew61H5hE-ZI__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__vGeEtorfXGI3aPAp71YA74N5p8wfpYVhnoIqwaJrOiQ__8abcou7ybLtGOLy-V9E-NQlDejOMZmdStjlYfyX9W-k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"kcs","theme_token":"a35rhFZbCqtnSfDB067h4TwGsi-F1PkX9aELQXp45TE","js":{"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.jquery.min.js":1,"modules\/chosen\/chosen.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/prism.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/underscore.js":1,"sites\/all\/themes\/kcs\/js\/kcs_base.js":1,"sites\/all\/themes\/kcs\/js\/showdown.js":1,"sites\/all\/themes\/kcs\/js\/case_links.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/book\/book.css":1,"modules\/comment\/comment.css":1,"modules\/date\/date_api\/date.css":1,"modules\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/user_prune\/css\/user_prune.css":1,"modules\/views\/css\/views.css":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.css":1,"modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/panels\/css\/panels.css":1,"sites\/all\/modules\/custom\/rate\/rate.css":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/plugins\/layouts\/docs_page\/docs_page.css":1,"https:\/\/access.redhat.com\/webassets\/avalon\/s\/messages.css":1,"sites\/all\/themes\/zen\/system.base.css":1,"sites\/all\/themes\/zen\/system.menus.css":1,"sites\/all\/themes\/zen\/system.messages.css":1,"sites\/all\/themes\/zen\/system.theme.css":1,"sites\/all\/themes\/zen\/comment.css":1,"sites\/all\/themes\/zen\/node.css":1,"sites\/all\/themes\/kcs\/css\/html-reset.css":1,"sites\/all\/themes\/kcs\/css\/wireframes.css":1,"sites\/all\/themes\/kcs\/css\/layout-fixed.css":1,"sites\/all\/themes\/kcs\/css\/page-backgrounds.css":1,"sites\/all\/themes\/kcs\/css\/tabs.css":1,"sites\/all\/themes\/kcs\/css\/pages.css":1,"sites\/all\/themes\/kcs\/css\/blocks.css":1,"sites\/all\/themes\/kcs\/css\/navigation.css":1,"sites\/all\/themes\/kcs\/css\/views-styles.css":1,"sites\/all\/themes\/kcs\/css\/nodes.css":1,"sites\/all\/themes\/kcs\/css\/comments.css":1,"sites\/all\/themes\/kcs\/css\/forms.css":1,"sites\/all\/themes\/kcs\/css\/fields.css":1,"sites\/all\/themes\/kcs\/css\/kcs.css":1,"sites\/all\/themes\/kcs\/css\/print.css":1}},"chosen":{"selector":"#edit-field-kcs-component-select-und, #edit-field-kcs-sbr-select-und, #edit-field-kcs-product-select-und, #edit-field-kcs-type-select-und, #edit-language, #edit-field-kcs-a-category-select-und, #edit-field-kcs-state-select-und, #edit-field-kcs-tags-select-und, #edit-field-kcs-state-select-value, #edit-field-vid-reference-product-und, #edit-state, #edit-product, #edit-category, #edit-field-kcs-product-select-tid, #edit-field-kcs-type-select-tid, #edit-field-kcs-a-category-select-tid, #edit-sort-bef-combine, #edit-kcs-state, #edit-field-category-tid, #edit-field-product-tid, #edit-field-tags-tid, #edit-field-category-und, #edit-field-product-und, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-category, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-product, #views-exposed-form-questions-list-questions-filter-block #edit-field-tags, #edit-field-mega-menu-tab-und, #edit-field-internal-tags-tid, #edit-tags, #edit-field-kcs-tags-select-tid, #edit-subscriptions-and-choose-field-select, #edit-subscriptions-and-type, #edit-vid-13, #edit-vid-4, #edit-vid-5, #edit-vid-53, #edit-vid-1, #edit-vid-3, #edit-group-blog, #edit-subscriptions-and-choose-field-select-two, #edit-subscriptions-and-type-two, #edit-vid-13-two, #edit-vid-4-two, #edit-vid-5-two, #edit-vid-53-two, #edit-vid-1-two, #edit-vid-3-two, #edit-group-blog-two, #edit-subscriptions-and-edit-choose-field-select, #edit-subscriptions-and-type-edit, #edit-subscriptions-and-edit-type-two, #edit-vid-13-edit-two, #edit-vid-4-edit-two, #edit-vid-5-edit-two, #edit-vid-53-edit-two, #edit-vid-1-edit-two, #edit-vid-3-edit-two, #edit-subscriptions-and-edit-choose-field-select-two, #edit-vid-13-edit, #edit-vid-4-edit, #edit-vid-5-edit, #edit-vid-53-edit, #edit-vid-1-edit, #edit-vid-3-edit, #edit-group-blog-edit, #edit-field-supported-languages-tid, #edit-field-geography-und, #edit-field-supported-products-und, #edit-field-supported-languages-und, #edit-field-vendor-und, #edit-field-errata-type-text-und, #edit-field-errata-severity-text-und, #edit-field-software-partner-level-und, #edit-field-scert-product-category-und, #edit-field-certifications-und-0-field-product-und, #edit-kcs-article-type, #edit-field-eco-industry-tag-select-tid, #edit-field-eco-software-catego-select-tid, #edit-field-ecosystem-tag-select-und, #edit-field-eco-industry-tag-select-und, #edit-field-eco-software-catego-select-und, #edit-field-vendor-tsanet-member-ref-und, #edit-field-og-vendor-ref-und-0-default, #edit-field-eco-products-enabled-col-und-0-field-eco-subscription-model-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-support-level-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-archite-select-und, #edit-field-eco-certifications-select-und-0-field-eco-certificati-lvl-select-und, #edit-field-eco-certifications-select-und-0-field-eco-hypervisor-str-und, #edit-field-eco-supported-language-ref-und, #edit-field-eco-region-ref-und, #edit-field-cs-product-category-str-und, #edit-field-eco-certifications-select-und-0-field-eco-format-ref-und, #edit-field-eco-group-access-ref-und, #edit-field-hw-category-tag-ref-und, #edit-field-profile-industry-und, #edit-field-profile-tech-interests-und, #edit-field-product-page-features-ref-und, #edit-field-eco-product-select-und, #edit-field-base-product-ref-und, #edit-field-eco-product-archite-select-und, #edit-field-eco-format-ref-und, #edit-field-certification-status-ref-und, #edit-field-certification-result-ref-und, #edit-field-og-certified-product-ref-und-0-default, #edit-field-eco-subscription-model-ref-und, #edit-field-eco-support-level-ref-und, #edit-field-eco-certificati-lvl-select-und, #edit-field-eco-hypervisor-str-und, #edit-field-ccp-thirdparty-cert-select-und, .use_, #edit-field-eco-cert-product-tag-und, #edit-field-documentation-location-ref-und, #edit-field-documentation-title-und, #edit-field-accelerator-products-und","minimum":"0"},"rh_doc_fetcher":{"page_type":"single"},"section":"","kcs":{"nodeType":null,"nodeId":null}});</script>
    <!--[if lt IE 9]>
  <script src="https://access.redhat.com/sites/all/themes/kcs/js/html5shiv.js"></script>
  <![endif]-->
  
      


    <!--kcs06-->
<script type="text/javascript">
  Drupal.portal = {"version":{"redhat_portal":"package drupal7-redhat_portal is not installed"}};
  Drupal.portal.currentUser = {};
  </script>

</head>

<body class="portal-page  kcs_external" >
  
  <div id="page-wrap" class="page-wrap">
    <div class="top-page-wrap">

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <script>
    chrometwo_require(["wc"], function(wc){    
        wc.include("@cpelements/cp-search-autocomplete/dist/cp-search-autocomplete.umd");
    }); 
</script>

<!-- Accessibility Nav & Header -->
<div class="accessibility-nav sr-only">
    <a href="https://access.redhat.com/">
        <h1>Red Hat <span>Customer </span><span>Portal</span></h1>
    </a>
    <p><a href="index.html#cp-main">Skip to main content</a></p>
    <nav aria-labelledby="accessibility-nav-heading">
        <h2 id="accessibility-nav-heading">Main Navigation</h2>
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="accessibility-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="accessibility-accountUser">
                    <div class="account-name"><strong id="accessibility-userFullName"></strong></div>
                    <div class="account-org"><span id="accessibility-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="accessibility-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="accessibility-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="accessibility-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="accessibility-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="accessibility-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="accessibility-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- Mobile Header -->
<nav class="mobile-nav-bar hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <button id="menu-btn" class="menu menu-white" type="button" aria-label="Toggle Navigation">
        <span class="lines"></span>
    </button>
    <a class="logo" href="https://access.redhat.com/">
        <span class="logo-crop">
          <!-- logo -->
          <span class="sr-only">Red Hat Customer Portal</span>
          <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
            <defs>
            <style>
              .rh-logo-type {
                fill:#fff;
              }
              .rh-logo-hat {
                fill:#e00;
              }
            </style>
            </defs>
            <g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
          </svg>
        </span>
            </a>
    <button class="btn btn-search btn-utility" data-target="#site-search">
        <span class="web-icon-search"></span>
        <span class="link-text">Search</span>
    </button>
</nav>

<!-- Mobile Menu Drawer -->
<div class="nav-drawer mobile-nav-drawer hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <nav class="nav-container">
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="mobile-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="mobile-accountUser">
                    <div class="account-name"><strong id="mobile-userFullName"></strong></div>
                    <div class="account-org"><span id="mobile-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="mobile-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="mobile-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="mobile-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="mobile-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="mobile-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="mobile-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- White Utility Menu for large screens -->
<div class="utility-wrap" aria-hidden="true">
    <div class="utility-container">
        <div class="utility-bar hidden-xs">
            <div role="navigation" class="top-nav">
                <ul>
                    <li id="nav-subscription" data-portal-tour-1="1"><a class="top-nav-subscriptions" href="https://access.redhat.com/management/">Subscriptions</a></li>
                    <li id="nav-downloads" data-portal-tour-1="2"><a class="top-nav-downloads" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li id="nav-containers"><a class="top-nav-containers" href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
                    <li id="nav-support" data-portal-tour-1="3"><a class="top-nav-support-cases" href="https://access.redhat.com/support/cases/">Support Cases</a></li>
                </ul>
            </div>

            <div role="navigation" class="utility-nav">
                <ul>
                    <li id="searchABTestHide">
                        <a class="btn-search" data-target="#site-search" title="Search" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-search" aria-label="search"></span>
                            <span class="link-text">Search</span>
                        </a>
                    </li>
                    <!-- AB Test -->
                    <li id="searchABTestShow">
                        <form id="topSearchFormABTest" name="topSearchFormABTest">
                            <label for="topSearchInputABTest" class="sr-only">Search</label>
                            <input id="topSearchInputABTest" name="keyword" placeholder="Search" value="" type="text" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" class="form-control">
                            <button type="submit" class="btn btn-app btn-sm btn-link"><span class="web-icon-search" aria-label="search"></span></button>
                        </form>
                    </li>
                    <!-- End of AB Test -->
                    <li>
                        <a class="btn-profile" data-target="#account-info" data-portal-tour-1="4a" title="Account" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-user" aria-label="log in"></span>
                            <span class="link-text">Log In</span>
                            <span class="account-user" id="accountUserName"></span>
                        </a>
                    </li>
                    <li>
                        <a class="btn-language" data-target="#language" data-portal-tour-1="6" title="Language" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-globe" aria-label="language"></span>
                            <span class="link-text">Language</span>
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- Utility Tray -->
        <div class="utility-tray-container">
            <div class="utility-tray">
                <div id="site-search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="https://access.redhat.com/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>
                            <div class="input-group push-bottom">
                                <input class="form-control searchField" id="topSearchInput" name="keyword" value="" placeholder="Enter your search term" type="text">
                                <span class="input-group-btn">
                                    <button type="submit" class="btn btn-primary">Search</button>
                                </span>
                            </div>
                            <div>Or <a href="https://access.redhat.com/support/cases/#/troubleshoot">troubleshoot an issue</a>.</div>
                        </form>
                    </div>
                </div>
                <div id="account-info" class="utility-link account-info">
                    <div class="content">

                        <!-- Account Unauthenticated -->
                        <div id="accountLinksLoggedOut" class="unauthenticated">
                            <h2 class="utility-header">Log in to Your Red Hat Account</h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border">
                                    <p><a href="https://access.redhat.com/login" id="accountLogin" class="btn btn-primary">Log In</a></p>
                                    <p>Your Red Hat account gives you access to your profile, preferences, and services, depending on your status.</p>
                                </div>
                                <div class="col-sm-6 col-border col-border-left">
                                    <p><a href="https://www.redhat.com/wapps/ugc/register.html" class="btn btn-primary">Register</a></p>
                                    <p>If you are a new customer, register now for access to product evaluations and purchasing capabilities. </p>

                                    <strong>Need access to an account?</strong><p>If your company has an existing Red Hat account, your organization administrator can grant you access.</p>

                                    <p><a href="https://access.redhat.com/support/contact/customerService/">If you have any questions, please contact customer service.</a></p>
                                </div>
                            </div>
                        </div>

                        <!-- Account Authenticated -->
                        <div id="accountLinksLoggedIn" class="authenticated">
                            <h2 class="utility-header"><span id="userFirstName"></span></h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border col-border-right">
                                    <div class="account-info" id="accountUser">
                                        <div class="avatar"><!-- placeholder--></div>
                                        <div class="account-name"><strong id="userFullName"></strong></div>
                                        <div class="account-org"><span id="userOrg"></span></div>
                                        <div class="account-number accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                                    </div>

                                    <div class="row account-settings">
                                        <div class="col-md-6" data-portal-tour-1="4">
                                            <h3>Red Hat Account</h3>
                                            <ul class="reset">
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
                                                <!-- <li><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
                                                <li><a href="https://access.redhat.com/account-team">Account Team</a></li>
                                            </ul>
                                        </div>
                                        <div class="col-md-6" data-portal-tour-1="5">
                                            <h3>Customer Portal</h3>
                                            <ul class="reset">
                                                <li><a href="https://access.redhat.com/user">My Profile</a></li>
                                                <li><a href="https://access.redhat.com/user" id="userNotificationsLink">Notifications</a></li>
                                                <li><a href="https://access.redhat.com/help/">Help</a></li>
                                            </ul>
                                        </div>
                                    </div>

                                </div>
                                <div class="col-sm-6 col-border">
                                    <p>For your security, if you’re on a public computer and have finished using your Red Hat services, please be sure to log out.</p>
                                    <p><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout" id="accountLogout" class="btn btn-primary">Log Out</a></p>
                                </div>
                            </div>
                        </div>

                    </div>
                </div>
                <div id="language" class="utility-link language">
                    <div class="content">
                        <h2 class="utility-header">Select Your Language</h2>
                        <div class="row" id="localesMenu">
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en">English</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko">한국어</a></li>
                                </ul>
                            </div>
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja">日本語</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN">中文 (中国)</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div id="scroll-anchor"></div>

<!-- Main Menu for large screens -->
<div class="header-nav visible-sm visible-md visible-lg" aria-hidden="true">
    <div id="header-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">

                    <a href="https://access.redhat.com/" class="logo">
                      <span class="sr-only">Red Hat Customer Portal</span>
                      <span class="logo-crop">
                        <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
                          <defs>
                          <style>
                            .rh-logo-type {
                              fill:#fff;
                            }
                            .rh-logo-hat {
                              fill:#e00;
                            }
                          </style>
                          </defs>
                          <title>Red Hat Customer Portal</title><g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
                        </svg>
                      </span>
                                            </a>

                    <nav class="primary-nav hidden-sm">
                        <ul>
                            <li id="nav-products"><a class="products" data-link="mega" data-target="products-menu" href="https://access.redhat.com/products/" id="products-menu">Products &amp; Services</a></li>
                            <li id="nav-tools"><a class="tools" data-link="mega" data-target="tools-menu" href="https://access.redhat.com/labs/" id="tools-menu">Tools</a></li>
                            <li id="nav-security"><a class="security" data-link="mega" data-target="security-menu" href="https://access.redhat.com/security/" id="security-menu">Security</a></li>
                            <li id="nav-community"><a class="community" data-link="mega" data-target="community-menu" href="https://access.redhat.com/community/" id="community-menu">Community</a></li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Main Mega Menu for large screens -->
<div class="mega-wrap visible-sm visible-md visible-lg" aria-hidden="true">
    <nav class="mega">
        <div class="container">
            <div class="mega-menu-wrap">

                <!-- Products Menu -->
                <div aria-labelledby="products-menu" class="products-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-md-6 col-sm-8">
                            <div class="root clearfix" data-portal-tour-1="7">
                                <ul class="subnav subnav-root">
                                    <li data-target="infrastructure-menu" class="active">
                                        <h3>Infrastructure and Management</h3>
                                    </li>
                                    <li data-target="cloud-menu">
                                        <h3>Cloud Computing</h3>
                                    </li>
                                    <li data-target="storage-menu">
                                        <h3>Storage</h3>
                                    </li>
                                    <li data-target="jboss-dev-menu">
                                        <h3>Runtimes</h3>
                                    </li>
                                    <li data-target="jboss-int-menu">
                                        <h3>Integration and Automation</h3>
                                    </li>
                                </ul>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="infrastructure-menu" style="display: block;">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="cloud-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="storage-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a>
                                        </li>
                                        <li>
                                          <a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-dev-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                                        </li>
                                    </ul>
                                </div>
                                <!-- <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                        </li>
                                    </ul>
                                </div> -->
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul class="border-bottom" id="portal-menu-border-bottom">
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                      </li>
                                    </ul>
                                    <ul>
                                      <li>
                                        <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                      </li>
                                    </ul>
                                </div>
                            </div>
                            <a href="https://access.redhat.com/products" class="btn btn-primary">View All Products</a>
                        </div>

                        <div class="col-md-6 col-sm-4 pull-right" data-portal-tour-1="8">
                            <div class="row">
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/support" class="cta-link cta-link-darkbg">Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
                                        <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>
                                    </ul>
                                    <h4 class="nav-title">Services</h4>
                                    <ul>
                                        <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
                                        <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>
                                    </ul>
                                </div>
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/documentation" class="cta-link cta-link-darkbg">Documentation</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>
                                    </ul>
                                    <ul>
                                        
                                        <li><a href="https://catalog.redhat.com/" class="cta-link cta-link-darkbg">Ecosystem Catalog</a></li>
                                        <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
                                        <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Tools Menu -->
                <div aria-labelledby="tools-menu" class="tools-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row" data-portal-tour-1="9">
                        <div class="col-sm-12">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h3 class="nav-title">Tools</h3>
                                    <ul>
                                        <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
                                        <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
                                        <li><a href="https://access.redhat.com/errata/">Errata</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <ul class="list-flat">
                                        <li><a href="https://access.redhat.com/labs/" class="cta-link cta-link-darkbg">Customer Portal Labs</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <div class="card card-dark-grey">
                                        <h4 class="card-heading">Red Hat Insights</h4>
                                        <p class="text-white">Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                        <ul class="list-flat rh-l-grid rh-m-gutters rh-m-all-6-col-on-md">
                                          <li><a href="https://www.redhat.com/en/technologies/management/insights" class="cta-link cta-link-md cta-link-darkbg">Learn more</a></li>
                                          <li><a href="https://cloud.redhat.com/insights" class="cta-link cta-link-md cta-link-darkbg">Go to Insights</a></li>
                                        </ul>
                                    </div>
                                </div>

                            </div>
                        </div>

                    </div>
                </div>

                <!-- Security Menu -->
                <div aria-labelledby="security-menu" class="security-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="10">

                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Red Hat Product Security Center</h2>
                                    <p class="text-white">Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.</p>
                                    <p><a href="https://access.redhat.com/security/" class="btn btn-primary">Product Security Center</a></p>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Security Updates</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
                                    </ul>
                                    <p class="text-white">Keep your systems secure with Red Hat&#039;s specialized responses for high-priority security vulnerabilities.</p>
                                    <ul>
                                        <li class="more-link"><a href="https://access.redhat.com/security/vulnerability">View Responses</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Resources</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
                                        <li><a href="https://www.redhat.com/en/blog/channel/security">Security Blog</a></li>
                                        <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
                                        <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Community Menu -->
                <div aria-labelledby="community-menu" class="community-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="11">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Portal Community</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
                                        <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
                                        <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
                                        <p class="push-top"><a href="https://access.redhat.com/community/" class="btn btn-primary">Community Activity</a></p>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Events</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
                                        <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Stories</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
                                        <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
                                        <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <!--<a href="https://access.redhat.com/community/" class="btn btn-primary">Explore Community</a>-->
                    </div>
                </div>
            </div>
        </div>
    </nav>
</div>

            <!--[if IE 8]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
            </div>
            <![endif]-->
            <!--[if IE 9]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
            </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">
  <script type="text/javascript">
    chrometwo_require(['jquery-ui']);
  </script>


      <article class="rh_docs">
  <div class="container">
        <!-- Display: Book Page Content -->
    
  

<div class="row">
  <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
  <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
  


<a class="toc-toggle toc-show" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
  <span class="sr-only">Show Table of Contents</span>
  <span class="web-icon-mobile-menu" aria-hidden="true"></span>
</a>
<nav id="toc-main" class="toc-main collapse in">
  <div class="toc-menu affix-top">
    <a class="toc-toggle toc-hide" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
      <span class="sr-only">Hide Table of Contents</span>
      <span class="icon-remove" aria-hidden="true"></span>
    </a>
    <div class="doc-options">
      <div class="doc-language btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          English <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="index.html">English</a></li>
                      <li><a href="https://access.redhat.com/documentation/ja-jp/red_hat_openstack_platform/16.1/html/instances_and_images_guide/">日本語</a></li>
                  </ul>
      </div>
      <div class="doc-format btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          Single-page HTML <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/">Multi-page HTML</a></li>
                                <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/pdf/instances_and_images_guide/Red_Hat_OpenStack_Platform-16.1-Instances_and_Images_Guide-en-US.pdf">PDF</a></li>
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/epub/instances_and_images_guide/Red_Hat_OpenStack_Platform-16.1-Instances_and_Images_Guide-en-US.epub">ePub</a></li>
                  </ul>
      </div>
    </div>
    <ol class="menu"><li class=" leaf"><a href="index.html">Instances and Images Guide</a></li><li class=" leaf"><a href="index.html#idm139747284839744">Preface</a></li><li class=" leaf"><a href="index.html#ch-image-service">1. Image service</a><ol class="menu"><li class=" leaf"><a href="index.html#sect-understanding">1.1. Understanding the Image service</a><ol class="menu"><li class=" leaf"><a href="index.html#section-service-supported-back-ends">1.1.1. Supported Image service (glance) back ends</a></li><li class=" leaf"><a href="index.html#section-image-sign-verify">1.1.2. Image signing and verification</a></li><li class=" leaf"><a href="index.html#section-image-conversion">1.1.3. Image conversion</a></li><li class=" leaf"><a href="index.html#section-image-introspection">1.1.4. Image introspection</a></li><li class=" leaf"><a href="index.html#section-image-import">1.1.5. Interoperable image import</a></li><li class=" leaf"><a href="index.html#section-image-service-caching">1.1.6. Improving scalability with Image service caching</a></li><li class=" leaf"><a href="index.html#section-changing_default_interval_image_caching">1.1.7. Image pre-caching</a></li></ol></li><li class=" leaf"><a href="index.html#section-manage_images">1.2. Manage images</a><ol class="menu"><li class=" leaf"><a href="index.html#section-create-images">1.2.1. Creating an Image</a></li><li class=" leaf"><a href="index.html#section-upload-image">1.2.2. Upload an image</a></li><li class=" leaf"><a href="index.html#section-update-image">1.2.3. Update an image</a></li><li class=" leaf"><a href="index.html#section-import-image">1.2.4. Import an image</a></li><li class=" leaf"><a href="index.html#section-delete-image">1.2.5. Delete an image</a></li><li class=" leaf"><a href="index.html#section-hide-image">1.2.6. Hide or unhide an image</a></li><li class=" leaf"><a href="index.html#section-show-hidden-image">1.2.7. Show hidden images</a></li><li class=" leaf"><a href="index.html#enabling-image-conversion">1.2.8. Enabling image conversion</a></li><li class=" leaf"><a href="index.html#section-convert-image">1.2.9. Converting an image to RAW format</a></li><li class=" leaf"><a href="index.html#storing-image-in-raw-format">1.2.10. Storing an image in RAW format</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#using-image-service-with-mulitple-stores">2. Image service with multiple stores</a><ol class="menu"><li class=" leaf"><a href="index.html#requirements_of_storage_edge_architecture">2.1. Requirements of storage edge architecture</a></li><li class=" leaf"><a href="index.html#import-an-image-to-multiple-stores">2.2. Import an image to multiple stores</a><ol class="menu"><li class=" leaf"><a href="index.html#manage-image-import-failures">2.2.1. Manage image import failures</a></li><li class=" leaf"><a href="index.html#importing-image-data-to-multiple-stores">2.2.2. Importing image data to multiple stores</a></li><li class=" leaf"><a href="index.html#importing-image-data-multiple-stores-no-failure">2.2.3. Importing image data to multiple stores without failure</a></li><li class=" leaf"><a href="index.html#importing-image-data-to-single-store">2.2.4. Importing image data to a single store</a></li><li class=" leaf"><a href="index.html#checking-progress-of-image-import-operation">2.2.5. Checking the progress of the image import operation</a></li></ol></li><li class=" leaf"><a href="index.html#copy-an-existing-image-to-multiple-stores">2.3. Copy an existing image to multiple stores</a><ol class="menu"><li class=" leaf"><a href="index.html#copying-an-image-to-all-stores">2.3.1. Copying an image to all stores</a></li><li class=" leaf"><a href="index.html#copying-an-image-to-specific-stores">2.3.2. Copying an image to specific stores</a></li></ol></li><li class=" leaf"><a href="index.html#deleting-an-image-from-specific-store">2.4. Deleting an image from a specific store</a></li><li class=" leaf"><a href="index.html#understanding-locations-of-images">2.5. Understanding the locations of images</a></li></ol></li><li class=" leaf"><a href="index.html#ch-configuring-compute">3. Configuring the Compute (nova) service</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring-memory">3.1. Configuring memory for overallocation</a></li><li class=" leaf"><a href="index.html#calculating-reserved-host-memory">3.2. Calculating reserved host memory on Compute nodes</a></li><li class=" leaf"><a href="index.html#calculating-swap">3.3. Calculating swap size</a></li></ol></li><li class=" leaf"><a href="index.html#ch-manage_cells">4. Scaling deployments with Compute cells</a><ol class="menu"><li class=" leaf"><a href="index.html#concept_cell-components">4.1. Cell components</a></li><li class=" leaf"><a href="index.html#concept_cell-deployments-arch">4.2. Cell deployments architecture</a></li><li class=" leaf"><a href="index.html#concept_multi-cell-considerations">4.3. Considerations for multi-cell deployments</a></li><li class=" leaf"><a href="index.html#proc_multi-cell-deploy">4.4. Deploying a multi-cell overcloud</a></li><li class=" leaf"><a href="index.html#proc_create-launch-cell">4.5. Creating and provisioning a cell</a></li><li class=" leaf"><a href="index.html#proc_multi-cell-add-compute-node">4.6. Adding Compute nodes to a cell</a></li><li class=" leaf"><a href="index.html#proc_multi-cell-config-az">4.7. Configuring an availability zone</a></li><li class=" leaf"><a href="index.html#proc_multi-cell-delete-compute-node">4.8. Deleting a Compute node from a cell</a></li><li class=" leaf"><a href="index.html#proc_multi-cell-delete-cell">4.9. Deleting a cell</a></li></ol></li><li class=" leaf"><a href="index.html#creating-and-managing-host-aggregates">5. Creating and managing host aggregates</a><ol class="menu"><li class=" leaf"><a href="index.html#enable-aggregate-scheduling-osp">5.1. Enabling scheduling on host aggregates</a></li><li class=" leaf"><a href="index.html#creating-host-aggregate-osp">5.2. Creating a host aggregate</a></li><li class=" leaf"><a href="index.html#create-availability-zone-osp">5.3. Creating an availability zone</a></li><li class=" leaf"><a href="index.html#delete-aggregate-osp">5.4. Deleting a host aggregate</a></li><li class=" leaf"><a href="index.html#tenant-isolation-osp">5.5. Creating a tenant-isolated host aggregate</a></li></ol></li><li class=" leaf"><a href="index.html#ch-configure_compute_storage">6. Configure OpenStack Compute Storage</a><ol class="menu"><li class=" leaf"><a href="index.html#architecture_overview">6.1. Architecture Overview</a></li><li class=" leaf"><a href="index.html#configuration">6.2. Configuration</a></li></ol></li><li class=" leaf"><a href="index.html#ch-manage_instances">7. Virtual Machine Instances</a><ol class="menu"><li class=" leaf"><a href="index.html#section-instances">7.1. Manage Instances</a><ol class="menu"><li class=" leaf"><a href="index.html#section_Add-components">7.1.1. Add Components</a></li><li class=" leaf"><a href="index.html#section-launch-instance">7.1.2. Launch an Instance</a></li><li class=" leaf"><a href="index.html#update_an_instance_actions_menu">7.1.3. Update an Instance (Actions menu)</a></li><li class=" leaf"><a href="index.html#section-resize-instance">7.1.4. Resize an Instance</a></li><li class=" leaf"><a href="index.html#connect_to_an_instance">7.1.5. Connect to an Instance</a></li><li class=" leaf"><a href="index.html#view_instance_usage">7.1.6. View Instance Usage</a></li><li class=" leaf"><a href="index.html#delete_an_instance">7.1.7. Delete an Instance</a></li><li class=" leaf"><a href="index.html#manage_multiple_instances_at_once">7.1.8. Manage Multiple Instances at Once</a></li></ol></li><li class=" leaf"><a href="index.html#section-instance-security">7.2. Manage Instance Security</a><ol class="menu"><li class=" leaf"><a href="index.html#section-manage-keypair">7.2.1. Manage Key Pairs</a></li><li class=" leaf"><a href="index.html#section-create-security-group">7.2.2. Create a Security Group</a></li><li class=" leaf"><a href="index.html#section-create-assign-release-floating-IPs">7.2.3. Create, Assign, and Release Floating IP Addresses</a></li><li class=" leaf"><a href="index.html#section-Check-instance">7.2.4. Log in to an Instance</a></li><li class=" leaf"><a href="index.html#section-inject-admin-password">7.2.5. Inject an admin Password Into an Instance</a></li></ol></li><li class=" leaf"><a href="index.html#section-flavors">7.3. Manage Flavors</a><ol class="menu"><li class=" leaf"><a href="index.html#update_configuration_permissions">7.3.1. Update Configuration Permissions</a></li><li class=" leaf"><a href="index.html#create_a_flavor">7.3.2. Create a Flavor</a></li><li class=" leaf"><a href="index.html#update_general_attributes">7.3.3. Update General Attributes</a></li><li class=" leaf"><a href="index.html#section-update-flavor-metadata">7.3.4. Update Flavor Metadata</a></li></ol></li><li class=" leaf"><a href="index.html#section-scheduler">7.4. Schedule Hosts</a><ol class="menu"><li class=" leaf"><a href="index.html#section-schedule-filters">7.4.1. Configure Scheduling Filters</a></li><li class=" leaf"><a href="index.html#section-schedule-weights">7.4.2. Configure Scheduling Weights</a></li><li class=" leaf"><a href="index.html#section-placement-service">7.4.3. Configure Placement Service Traits</a></li><li class=" leaf"><a href="index.html#section-guaranteed-min-bw">7.4.4. Configuring a guaranteed minimum bandwidth QoS</a></li><li class=" leaf"><a href="index.html#section-reserve-numa-nodes">7.4.5. Reserve NUMA Nodes with PCI Devices</a></li><li class=" leaf"><a href="index.html#section-schedule-emulator-threads">7.4.6. Configure Emulator Threads to run on Dedicated Physical CPU</a></li></ol></li><li class=" leaf"><a href="index.html#section-instance-snapshots">7.5. Manage Instance Snapshots</a><ol class="menu"><li class=" leaf"><a href="index.html#section-create-instance-snapshot">7.5.1. Create an Instance Snapshot</a></li><li class=" leaf"><a href="index.html#section-manage-snapshot">7.5.2. Manage a Snapshot</a></li><li class=" leaf"><a href="index.html#section-rebuild-instance-using-snapshot">7.5.3. Rebuild an Instance to a State in a Snapshot</a></li><li class=" leaf"><a href="index.html#section-create-consistent-snapshots">7.5.4. Consistent Snapshots</a></li></ol></li><li class=" leaf"><a href="index.html#section-instance-rescue">7.6. Use Rescue Mode for Instances</a><ol class="menu"><li class=" leaf"><a href="index.html#preparing_an_image_for_a_rescue_mode_instance">7.6.1. Preparing an Image for a Rescue Mode Instance</a></li><li class=" leaf"><a href="index.html#adding_the_rescue_image_to_the_openstack_image_service">7.6.2. Adding the Rescue Image to the OpenStack Image Service</a></li><li class=" leaf"><a href="index.html#launching_an_instance_in_rescue_mode">7.6.3. Launching an Instance in Rescue Mode</a></li><li class=" leaf"><a href="index.html#unrescuing_an_instance">7.6.4. Unrescuing an Instance</a></li></ol></li><li class=" leaf"><a href="index.html#ch-configdrive">7.7. Set a Configuration Drive for Instances</a><ol class="menu"><li class=" leaf"><a href="index.html#configuration_drive_options">7.7.1. Configuration Drive Options</a></li><li class=" leaf"><a href="index.html#use_a_configuration_drive">7.7.2. Use a Configuration Drive</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#ch-compute-performance">8. Configuring Compute nodes for performance</a><ol class="menu"><li class=" leaf"><a href="index.html#ch-cpu_pinning">8.1. Configuring CPU pinning on the Compute node</a><ol class="menu"><li class=" leaf"><a href="index.html#upgrading_cpu_pinning_configuration">8.1.1. Upgrading CPU pinning configuration</a></li><li class=" leaf"><a href="index.html#launching_an_instance_with_cpu_pinning">8.1.2. Launching an instance with CPU pinning</a></li><li class=" leaf"><a href="index.html#launching_a_floating_instance">8.1.3. Launching a floating instance</a></li></ol></li><li class=" leaf"><a href="index.html#ch-huge-pages">8.2. Configuring huge pages on the Compute node</a><ol class="menu"><li class=" leaf"><a href="index.html#allocating_huge_pages_to_instances">8.2.1. Allocating huge pages to instances</a></li></ol></li><li class=" leaf"><a href="index.html#configure-file-backed-memory">8.3. Configuring Compute nodes to use file-backed memory for instances</a><ol class="menu"><li class=" leaf"><a href="index.html#change-host-disk">8.3.1. Changing the memory backing directory host disk</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#memory-encryption-for-instances">9. Configuring SEV-capable Compute nodes to provide memory encryption for instances</a><ol class="menu"><li class=" leaf"><a href="index.html#sev">9.1. Secure Encrypted Virtualization (SEV)</a></li><li class=" leaf"><a href="index.html#configure-node-for-SEV">9.2. Configuring a SEV-capable Compute node</a></li><li class=" leaf"><a href="index.html#configure-flavor-and-image-for-SEV">9.3. Creating the image and flavor for memory encryption</a><ol class="menu"><li class=" leaf"><a href="index.html#configure-image-for-SEV">9.3.1. Creating a SEV-enabled image for instances</a></li><li class=" leaf"><a href="index.html#configure-flavor-for-SEV">9.3.2. Creating a SEV-enabled flavor for instances</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#migrating-virtual-machines-between-compute-nodes">10. Migrating virtual machine instances between Compute nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#migration-types">10.1. Migration types</a></li><li class=" leaf"><a href="index.html#migration-constraints">10.2. Migration constraints</a></li><li class=" leaf"><a href="index.html#pre-migration-procedures">10.3. Preparing to migrate</a></li><li class=" leaf"><a href="index.html#cold-migrate-a-vm">10.4. Cold migrating an instance</a></li><li class=" leaf"><a href="index.html#live-migrate-a-vm">10.5. Live migrating an instance</a></li><li class=" leaf"><a href="index.html#check-migration-status">10.6. Checking migration status</a></li><li class=" leaf"><a href="index.html#evacuate-a-vm">10.7. Evacuating an instance</a><ol class="menu"><li class=" leaf"><a href="index.html#evacuate-one-vm">10.7.1. Evacuating one instance</a></li><li class=" leaf"><a href="index.html#evacuate-all-vms">10.7.2. Evacuating all instances on a host</a></li><li class=" leaf"><a href="index.html#cfg-shared-storage">10.7.3. Configuring shared storage</a></li></ol></li><li class=" leaf"><a href="index.html#troubleshooting-migration">10.8. Troubleshooting migration</a><ol class="menu"><li class=" leaf"><a href="index.html#errors_during_migration">10.8.1. Errors during migration</a></li><li class=" leaf"><a href="index.html#never_ending_live_migration">10.8.2. Never-ending live migration</a></li><li class=" leaf"><a href="index.html#instance_performance_degrades_after_migration">10.8.3. Instance performance degrades after migration</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#ch-virtual_gpu">11. Configuring virtual GPUs for instances</a><ol class="menu"><li class=" leaf"><a href="index.html#vgpu-supported-config-limitations">11.1. Supported configurations and limitations</a></li><li class=" leaf"><a href="index.html#vgpu-nvidia-overview">11.2. Configuring vGPU on the Compute nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#nvidia-build-overcloud-image">11.2.1. Building a custom GPU overcloud image</a></li><li class=" leaf"><a href="index.html#nvidia-config-role">11.2.2. Designating Compute nodes for vGPU</a></li><li class=" leaf"><a href="index.html#nvidia-prepare-config-files-deploy">11.2.3. Configuring the Compute node for vGPU and deploying the overcloud</a></li></ol></li><li class=" leaf"><a href="index.html#vgpu-instance-creation">11.3. Creating the vGPU image and flavor</a><ol class="menu"><li class=" leaf"><a href="index.html#nvidia-build-guest-image">11.3.1. Creating a custom GPU instance image</a></li><li class=" leaf"><a href="index.html#nvidia-create-gpu-flavor-instance">11.3.2. Creating a vGPU flavor for instances</a></li><li class=" leaf"><a href="index.html#nvidia-launch-test-instance">11.3.3. Launching a vGPU instance</a></li></ol></li><li class=" leaf"><a href="index.html#gpu-pci-passthru">11.4. Enabling PCI passthrough for a GPU device</a></li></ol></li><li class=" leaf"><a href="index.html#realtime-compute">12. Configuring Real-Time Compute</a><ol class="menu"><li class=" leaf"><a href="index.html#trc-preparing">12.1. Preparing Your Compute Nodes for Real-Time</a></li><li class=" leaf"><a href="index.html#rtc-deploying">12.2. Deploying the Real-time Compute Role</a></li><li class=" leaf"><a href="index.html#rtc-testing">12.3. Sample Deployment and Testing Scenario</a></li><li class=" leaf"><a href="index.html#rtc-instances">12.4. Launching and Tuning Real-Time Instances</a></li></ol></li><li class=" leaf"><a href="index.html#appx-image-config-parameters">A. Image Configuration Parameters</a></li><li class=" leaf"><a href="index.html#appx-enabling-launch-instance-wizard">B. Enabling the Launch Instance Wizard</a></li><li class=" leaf"><a href="index.html#idm139747270154416">Legal Notice</a></li></ol>  </div>
</nav>


  <div class="doc-wrapper">
    <div class="panel-pane pane-page-title"  >
  
      
  
  <h1 class="title" itemprop="name">Instances and Images Guide</h1>
  
  </div>
<div class="panel-pane pane-page-body"  >
  
      
  
  <div class="body"><div xml:lang="en-US" class="book" id="idm139747283401936"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat OpenStack Platform</span> <span class="productnumber">16.1</span></div><div><h2 class="subtitle">Managing Instances and Images</h2></div><div><div xml:lang="en-US" class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">OpenStack</span> <span class="othername">Documentation</span> <span class="surname">Team</span></h3><code class="email"><a class="email" href="mailto:rhos-docs@redhat.com">rhos-docs@redhat.com</a></code></div></div></div><div><a href="index.html#idm139747270154416">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				The Instances and Images guide provides procedures for the management of instances, images of a Red Hat OpenStack Platform environment.
			</div></div></div></div><hr/></div><section class="preface" id="idm139747284839744"><div class="titlepage"><div><div><h1 class="title">Preface</h1></div></div></div><p id="preface">
			Red Hat OpenStack Platform (RHOSP) provides the foundation to build a private or public Infrastructure-as-a-Service (IaaS) cloud on top of Red Hat Enterprise Linux. It offers a massively scalable, fault-tolerant platform for the development of cloud-enabled workloads.
		</p><p>
			This guide discusses procedures for creating and managing images, and instances. It also mentions the procedure for configuring the storage for instances for RHOSP.
		</p><p>
			You can manage the cloud by using either the RHOSP Dashboard or the command-line clients. You can use either method to perform most procedures but some of the more advanced procedures can only be executed on the command line. This guide provides procedures for the Dashboard where possible.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				For the complete suite of documentation for Red Hat OpenStack Platform, see <a class="link" href="https://access.redhat.com/documentation/en/red-hat-openstack-platform">Red Hat OpenStack Platform Documentation Suite</a>.
			</p></div></div></section><section class="chapter" id="ch-image-service"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Image service</h1></div></div></div><p>
			Manage images and storage in Red Hat OpenStack Platform (RHOSP).
		</p><p>
			A virtual machine image is a file that contains a virtual disk with a bootable operating system installed. Virtual machine images are supported in different formats. The following formats are available in RHOSP:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">RAW</code> - Unstructured disk image format.
				</li><li class="listitem">
					<code class="literal">QCOW2</code> - Disk format supported by QEMU emulator. This format includes QCOW2v3 (sometimes referred to as QCOW3), which requires QEMU 1.1 or higher.
				</li><li class="listitem">
					<code class="literal">ISO</code> - Sector-by-sector copy of the data on a disk, stored in a binary file.
				</li><li class="listitem">
					<code class="literal">AKI</code> - Indicates an Amazon Kernel Image.
				</li><li class="listitem">
					<code class="literal">AMI</code> - Indicates an Amazon Machine Image.
				</li><li class="listitem">
					<code class="literal">ARI</code> - Indicates an Amazon RAMDisk Image.
				</li><li class="listitem">
					<code class="literal">VDI</code> - Disk format supported by VirtualBox virtual machine monitor and the QEMU emulator.
				</li><li class="listitem">
					<code class="literal">VHD</code> - Common disk format used by virtual machine monitors from VMware, VirtualBox, and others.
				</li><li class="listitem">
					<code class="literal">VMDK</code> - Disk format supported by many common virtual machine monitors.
				</li></ul></div><p>
			Although <code class="literal">ISO</code> is not normally considered a virtual machine image format, because ISOs contain bootable filesystems with an installed operating system, you use them in the same way as other virtual machine image files.
		</p><p>
			To download the official Red Hat Enterprise Linux cloud images, your account must have a valid Red Hat Enterprise Linux subscription:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/downloads/content/479/ver=/rhel---8">Red Hat Enterprise Linux 8 KVM Guest Image</a>
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---7">Red Hat Enterprise Linux 7 KVM Guest Image</a>
				</li><li class="listitem">
					<a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---6/6.10/x86_64/product-software">Red Hat Enterprise Linux 6 KVM Guest Image</a>
				</li></ul></div><p>
			If you are not logged in to the Customer Portal, a prompt opens where you must enter your Red Hat account credentials.
		</p><section class="section" id="sect-understanding"><div class="titlepage"><div><div><h2 class="title">1.1. Understanding the Image service</h2></div></div></div><p>
				Red Hat OpenStack Platform (RHOSP) Image service (glance) features.
			</p><section class="section" id="section-service-supported-back-ends"><div class="titlepage"><div><div><h3 class="title">1.1.1. Supported Image service (glance) back ends</h3></div></div></div><p>
					The following Image service (glance) back end scenarios are supported:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							RBD is the default back end when you use Ceph. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/storage_configuration#sect-Configuring_Ceph_Storage">Configuring Ceph Storage</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
						</li><li class="listitem">
							RBD multi-store. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/distributed_compute_node_and_storage_deployment/assembly_deploying-storage-at-the-edge#deploying-the-central-site">Deploying the central site</a> in the <span class="emphasis"><em>Distributed compute node and storage deployment</em></span> guide.
						</li><li class="listitem">
							Object Storage (swift). For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/storage_configuration#sect-Standalone_Swift_Cluster">Using an External Object Storage Cluster</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
						</li><li class="listitem"><p class="simpara">
							Block Storage (cinder). For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/storage_configuration#sect-cinder-backend-glance">Configuring cinder back end for the Image service</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
						</p><p class="simpara">
							<span id="Note"><!--Empty--></span> The Image service uses the Block Storage type and back end as the default.
						</p></li><li class="listitem"><p class="simpara">
							NFS. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#sect-Configuring_NFS_Storage">Configuring NFS Storage</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
						</p><p id="Important" class="simpara">
							Although NFS is a supported Image service deployment option, more robust options are available.
						</p><p class="simpara">
							NFS is not native to the Image service. When you mount an NFS share on the Image service, the Image service does not manage the operation. The Image service writes data to the file system but is unaware that the back end is an NFS share.
						</p><p class="simpara">
							In this type of deployment, the Image service cannot retry a request if the share fails. This means that when a failure occurs on the back end, the store might enter read-only mode, or it might continue to write data to the local file system, in which case you risk data loss. To recover from this situation, you must ensure that the share is mounted and in sync, and then restart the Image service. For these reasons, Red Hat does not recommend NFS as an Image service back end.
						</p><p class="simpara">
							However, if you do choose to use NFS as an Image service back end, some of the following best practices can help to mitigate risks:
						</p></li><li class="listitem">
							Use a reliable production-grade NFS back end.
						</li><li class="listitem">
							Ensure that you have a strong and reliable connection between Controller nodes and the NFS back end, L2 is recommended.
						</li><li class="listitem">
							Include monitoring and alerts for the mounted share.
						</li><li class="listitem"><p class="simpara">
							Set underlying FS permissions.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Ensure that the user and the group that the glance-api process runs on do not have write permissions on the mount point at the local file system. This means that the process can detect possible mount failure and put the store into read-only mode during a write attempt.
								</li><li class="listitem">
									The write permissions must be present in the shared file system that you use as a store.
								</li></ul></div></li></ul></div></section><section class="section" id="section-image-sign-verify"><div class="titlepage"><div><div><h3 class="title">1.1.2. Image signing and verification</h3></div></div></div><p>
					Image signing and verification protects image integrity and authenticity by enabling deployers to sign images and save the signatures and public key certificates as image properties.
				</p><p>
					By taking advantage of this feature, you can:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Sign an image using your private key and upload the image, the signature, and a reference to your public key certificate (the verification metadata). The Image service then verifies that the signature is valid.
						</li><li class="listitem">
							Create an image in the Compute service, have the Compute service sign the image, and upload the image and its verification metadata. The Image service again verifies that the signature is valid.
						</li><li class="listitem">
							Request a signed image in the Compute service. The Image service provides the image and its verification metadata, allowing the Compute service to validate the image before booting it.
						</li></ul></div><p>
					For information on image signing and verification, refer to the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/manage_secrets_with_openstack_key_manager/validate_glance_images">Validate Glance Images</a> chapter of the <span class="emphasis"><em>Manage Secrets with OpenStack Key Manager Guide</em></span>.
				</p></section><section class="section" id="section-image-conversion"><div class="titlepage"><div><div><h3 class="title">1.1.3. Image conversion</h3></div></div></div><p>
					Image conversion converts images by calling the task API while importing an image.
				</p><p>
					As part of the import workflow, a plugin provides the image conversion. This plugin can be activated or deactivated based on the deployer configuration. Therefore, the deployer needs to specify the preferred format of images for the deployment.
				</p><p>
					Internally, the Image service receives the bits of the image in a particular format. These bits are stored in a temporary location. The plugin is then triggered to convert the image to the target format and moved to a final destination. When the task is finished, the temporary location is deleted. As a result, the format uploaded initially is not retained by the Image service.
				</p><p>
					For more information about image conversion, see <a class="link" href="index.html#enabling-image-conversion" title="1.2.8. Enabling image conversion">Enabling image conversion</a>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The conversion can be triggered only when importing an image. It does not run when uploading an image. For example:
					</p><pre class="screen">$ glance image-create-via-import \
    --disk-format qcow2 \
    --container-format bare \
    --name <span class="emphasis"><em>NAME</em></span> \
    --visibility public \
    --import-method web-download \
    --uri <span class="emphasis"><em>http://server/image.qcow2</em></span></pre></div></div></section><section class="section" id="section-image-introspection"><div class="titlepage"><div><div><h3 class="title">1.1.4. Image introspection</h3></div></div></div><p>
					Every image format comes with a set of metadata embedded inside the image itself. For example, a stream optimized <code class="literal">vmdk</code> would contain the following parameters:
				</p><pre class="screen">$ head -20 so-disk.vmdk

# Disk DescriptorFile
version=1
CID=d5a0bce5
parentCID=ffffffff
createType="streamOptimized"

# Extent description
RDONLY 209714 SPARSE "generated-stream.vmdk"

# The Disk Data Base
#DDB

ddb.adapterType = "buslogic"
ddb.geometry.cylinders = "102"
ddb.geometry.heads = "64"
ddb.geometry.sectors = "32"
ddb.virtualHWVersion = "4"</pre><p>
					By introspecting this <span class="emphasis"><em>vmdk</em></span>, you can easily know that the <span class="emphasis"><em>disk_type</em></span> is <span class="emphasis"><em>streamOptimized</em></span>, and the <span class="emphasis"><em>adapter_type</em></span> is <span class="emphasis"><em>buslogic</em></span>. These metadata parameters are useful for the consumer of the image. In Compute, the workflow to instantiate a <span class="emphasis"><em>streamOptimized</em></span> disk is different from the one to instantiate a <span class="emphasis"><em>flat</em></span> disk. This new feature allows metadata extraction. You can achieve image introspection by calling the task API while importing the image. An administrator can override metadata settings.
				</p></section><section class="section" id="section-image-import"><div class="titlepage"><div><div><h3 class="title">1.1.5. Interoperable image import</h3></div></div></div><p>
					The interoperable image import workflow enables you to import images in two ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use the <code class="literal">web-download</code> (default) method to import images from a URI.
						</li><li class="listitem">
							Use the <code class="literal">glance-direct</code> method to import images from a local file system.
						</li></ul></div></section><section class="section" id="section-image-service-caching"><div class="titlepage"><div><div><h3 class="title">1.1.6. Improving scalability with Image service caching</h3></div></div></div><p>
					Use the glance-api caching mechanism to store copies of images on your local machine and retrieve them automatically to improve scalability. With Image service caching, the glance-api can run on multiple hosts. This means that it does not need to retrieve the same image from back-end storage multiple times. Image service caching does not affect any Image service operations.
				</p><p>
					To configure Image service caching with the Red Hat OpenStack Platform director (tripleo) heat templates, complete the following steps:
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In an environment file, set the value of the <code class="literal">GlanceCacheEnabled</code> parameter to <code class="literal">true</code>, which automatically sets the <code class="literal">flavor</code> value to <code class="literal">keystone+cachemanagement</code> in the <code class="literal">glance-api.conf</code> heat template:
						</p><pre class="screen">parameter_defaults:
    GlanceCacheEnabled: true</pre></li><li class="listitem">
							Include the environment file in the <code class="literal">openstack overcloud deploy</code> command when you redeploy the overcloud.
						</li><li class="listitem"><p class="simpara">
							Optional: Tune the <code class="literal">glance_cache_pruner</code> to an alternative frequency when you redeploy the overcloud. The following example shows a frequency of 5 minutes:
						</p><pre class="screen">parameter_defaults:
  ControllerExtraConfig:
    glance::cache::pruner::minute: '*/5'</pre><p class="simpara">
							Adjust the frequency according to your needs to avoid file system full scenarios. Include the following elements when you choose an alternative frequency:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The size of the files that you want to cache in your environment.
								</li><li class="listitem">
									The amount of available file system space.
								</li><li class="listitem">
									The frequency at which the environment caches images.
								</li></ul></div></li></ol></div></section><section class="section" id="section-changing_default_interval_image_caching"><div class="titlepage"><div><div><h3 class="title">1.1.7. Image pre-caching</h3></div></div></div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p><section class="section" id="configuring_the_default_interval_for_periodic_image_pre_caching"><div class="titlepage"><div><div><h4 class="title">1.1.7.1. Configuring the default interval for periodic image pre-caching</h4></div></div></div><p>
						Because the Red Hat OpenStack Platform director can now pre-cache images as part of the <code class="literal">glance-api</code> service, you no longer require <code class="literal">glance-registry</code> to pre-cache images. The default periodic interval is 300 seconds. You can increase or decrease the default interval based on your requirements.
					</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Add a new interval with the <code class="literal">ExtraConfig</code> parameter in an environment file on the undercloud according to your requirements:
							</p><pre class="screen">parameter_defaults:
  ControllerExtraConfig:
    glance::config::glance_api_config:
      DEFAULT/cache_prefetcher_interval:
        value: '&lt;300&gt;'</pre><p class="simpara">
								Replace &lt;300&gt; with the number of seconds that you want as an interval to pre-cache images.
							</p></li><li class="listitem"><p class="simpara">
								After you adjust the interval in the environment file in <code class="literal">/home/stack/templates/</code>, log in as the <code class="literal">stack</code> user and deploy the configuration:
							</p><pre class="screen">$ openstack overcloud deploy --templates \
-e /home/stack/templates/&lt;ENV_FILE&gt;.yaml</pre><p class="simpara">
								Replace &lt;ENV_FILE&gt; with the name of the environment file that contains the <code class="literal">ExtraConfig</code> settings that you added.
							</p></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you passed any extra environment files when you created the overcloud, pass them again here using the <code class="literal">-e</code> option to avoid making undesired changes to the overcloud.
						</p></div></div><p>
						For more information about the <code class="literal">openstack overcloud deploy</code> command, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#deployment-command">Deployment command</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
					</p></section><section class="section" id="section-pre-caching_an_image"><div class="titlepage"><div><div><h4 class="title">1.1.7.2. Using a periodic job to pre-cache an image</h4></div></div></div><div class="formalpara"><p class="title"><strong>Prerequisite</strong></p><p>
							To use a periodic job to pre-cache an image, you must use the <code class="literal">glance-cache-manage</code> command connected directly to the node where the <code class="literal">glance_api</code> service is running. Do not use a proxy, which hides the node that answers a service request. Because the undercloud might not have access to the network where the <code class="literal">glance_api</code> service is running, run commands on the first overcloud node, which is called <code class="literal">controller-0</code> by default.
						</p></div><p>
						Complete the following prerequisite procedure to ensure that you run commands from the correct host, have the necessary credentials, and are also running the <code class="literal">glance-cache-manage</code> commands from inside the <code class="literal">glance-api</code> container.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Log in to the undercloud as the stack user and identify the provisioning IP address of <code class="literal">controller-0</code>:
							</p><pre class="screen">(undercloud) [stack@site-undercloud-0 ~]$ openstack server list -f value -c Name -c Networks | grep controller
overcloud-controller-1 ctlplane=192.168.24.40
overcloud-controller-2 ctlplane=192.168.24.13
overcloud-controller-0 ctlplane=192.168.24.71
(undercloud) [stack@site-undercloud-0 ~]$</pre></li><li class="listitem"><p class="simpara">
								To authenticate to the overcloud, copy the credentials that are stored in <code class="literal">/home/stack/overcloudrc</code>, by default, to <code class="literal">controller-0</code>:
							</p><pre class="screen">$ scp ~/overcloudrc heat-admin@192.168.24.71:/home/heat-admin/</pre></li><li class="listitem"><p class="simpara">
								Connect to <code class="literal">controller-0</code>:
							</p><pre class="screen">$ ssh heat-admin@192.168.24.71</pre></li><li class="listitem"><p class="simpara">
								On <code class="literal">controller-0</code> as the <code class="literal">heat-admin</code> user, identify the IP address of the <code class="literal">glance_api service</code>. In the following example, the IP address is <code class="literal">172.25.1.105</code>:
							</p><pre class="screen">(overcloud) [root@controller-0 ~]# grep -A 10 '^listen glance_api' 1/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg
listen glance_api
 server central-controller0-0.internalapi.redhat.local 172.25.1.105:9292 check fall 5 inter 2000 rise 2</pre></li><li class="listitem"><p class="simpara">
								Because the <code class="literal">glance-cache-manage</code> command is only available in the <code class="literal">glance_api</code> container, create a script to exec into that container where the environment variables to authenticate to the overcloud are already set. Create a script called <code class="literal">glance_pod.sh</code> in <code class="literal">/home/heat-admin</code> on <code class="literal">controller-0</code> with the following contents:
							</p><pre class="screen">sudo podman exec -ti \
 -e NOVA_VERSION=$NOVA_VERSION \
 -e COMPUTE_API_VERSION=$COMPUTE_API_VERSION \
 -e OS_USERNAME=$OS_USERNAME \
 -e OS_PROJECT_NAME=$OS_PROJECT_NAME \
 -e OS_USER_DOMAIN_NAME=$OS_USER_DOMAIN_NAME \
 -e OS_PROJECT_DOMAIN_NAME=$OS_PROJECT_DOMAIN_NAME \
 -e OS_NO_CACHE=$OS_NO_CACHE \
 -e OS_CLOUDNAME=$OS_CLOUDNAME \
 -e no_proxy=$no_proxy \
 -e OS_AUTH_TYPE=$OS_AUTH_TYPE \
 -e OS_PASSWORD=$OS_PASSWORD \
 -e OS_AUTH_URL=$OS_AUTH_URL \
 -e OS_IDENTITY_API_VERSION=$OS_IDENTITY_API_VERSION \
 -e OS_COMPUTE_API_VERSION=$OS_COMPUTE_API_VERSION \
 -e OS_IMAGE_API_VERSION=$OS_IMAGE_API_VERSION \
 -e OS_VOLUME_API_VERSION=$OS_VOLUME_API_VERSION \
 -e OS_REGION_NAME=$OS_REGION_NAME \
glance_api /bin/bash</pre></li><li class="listitem"><p class="simpara">
								Source the <code class="literal">overcloudrc</code> file and run the <code class="literal">glance_pod.sh</code> script to exec into the <code class="literal">glance_api</code> container with the necessary environment variables to authenticate to the overcloud Controller node.
							</p><pre class="screen">[heat-admin@controller-0 ~]$ source overcloudrc
(overcloudrc) [heat-admin@central-controller-0 ~]$ bash glance_pod.sh
()[glance@controller-0 /]$</pre></li><li class="listitem"><p class="simpara">
								Use a command such as <code class="literal">glance image-list</code> to verify that the container can run authenticated commands against the overcloud.
							</p><pre class="screen">()[glance@controller-0 /]$ glance image-list
+--------------------------------------+----------------------------------+
| ID                                   | Name                             |
+--------------------------------------+----------------------------------+
| ad2f8daf-56f3-4e10-b5dc-d28d3a81f659 | cirros-0.4.0-x86_64-disk.img       |
+--------------------------------------+----------------------------------+
()[glance@controller-0 /]$</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								As the admin user, queue an image to cache:
							</p><pre class="screen">$ glance-cache-manage --host=&lt;HOST-IP&gt; queue-image &lt;IMAGE-ID&gt;</pre><p class="simpara">
								Replace &lt;HOST-IP&gt; with the IP address of the Controller node where the <code class="literal">glance-api</code> container is running, and replace &lt;IMAGE-ID&gt; with the ID of the image that you want to queue. When you have queued the images that you want to pre-cache, the <code class="literal">cache_images</code> periodic job prefetches all queued images concurrently.
							</p></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Because the image cache is local to each node, if your Red Hat OpenStack Platform is deployed with HA (with 3, 5, or 7 Controllers) then you must specify the host address with the <code class="literal">--host</code> option when you run the <code class="literal">glance-cache-manage</code> command.
						</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Run the following command to view the images in the image cache:
							</p><pre class="screen">$ glance-cache-manage --host=&lt;HOST-IP&gt; list-cached</pre><p class="simpara">
								Replace &lt;HOST-IP&gt; with the IP address of the host in your environment.
							</p></li></ol></div><div class="formalpara"><p class="title"><strong>Related information</strong></p><p>
							You can use additional <code class="literal">glance-cache-manage</code> commands for the following purposes:
						</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">list-cached</code> to list all images that are currently cached.
							</li><li class="listitem">
								<code class="literal">list-queued</code> to list all images that are currently queued for caching.
							</li><li class="listitem">
								<code class="literal">queue-image</code> to queue an image for caching.
							</li><li class="listitem">
								<code class="literal">delete-cached-image</code> to purge an image from the cache.
							</li><li class="listitem">
								<code class="literal">delete-all-cached-images</code> to remove all images from the cache.
							</li><li class="listitem">
								<code class="literal">delete-queued-image</code> to delete an image from the cache queue.
							</li><li class="listitem">
								<code class="literal">delete-all-queued-images</code> to delete all images from the cache queue.
							</li></ul></div></section></section></section><section class="section" id="section-manage_images"><div class="titlepage"><div><div><h2 class="title">1.2. Manage images</h2></div></div></div><p>
				The OpenStack Image service (glance) provides discovery, registration, and delivery services for disk and server images. It provides the ability to copy or snapshot a server image, and immediately store it away. Stored images can be used as a template to get new servers up and running quickly and more consistently than installing a server operating system and individually configuring services.
			</p><section class="section" id="section-create-images"><div class="titlepage"><div><div><h3 class="title">1.2.1. Creating an Image</h3></div></div></div><p>
					This section provides you with the steps to manually create OpenStack-compatible images in the QCOW2 format using Red Hat Enterprise Linux 7 ISO files, Red Hat Enterprise Linux 6 ISO files, or Windows ISO files.
				</p><section class="section" id="section-kvm-images"><div class="titlepage"><div><div><h4 class="title">1.2.1.1. Use a KVM Guest Image With Red Hat OpenStack Platform</h4></div></div></div><p>
						You can use a ready RHEL KVM guest QCOW2 image:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="https://access.redhat.com/downloads/content/479/ver=/rhel---8">Red Hat Enterprise Linux 8 KVM Guest Image</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---7">Red Hat Enterprise Linux 7 KVM Guest Image</a>
							</li><li class="listitem">
								<a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---6/6.10/x86_64/product-software">Red Hat Enterprise Linux 6 KVM Guest Image</a>
							</li></ul></div><p>
						These images are configured with <code class="literal">cloud-init</code> and must take advantage of ec2-compatible metadata services for provisioning SSH keys in order to function properly.
					</p><p>
						Ready Windows KVM guest QCOW2 images are not available.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For the KVM guest images:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The <code class="literal">root</code> account in the image is disabled, but <code class="literal">sudo</code> access is granted to a special user named <code class="literal">cloud-user</code>.
								</li><li class="listitem">
									There is no <code class="literal">root</code> password set for this image.
								</li></ul></div><p>
							The <code class="literal">root</code> password is locked in <code class="literal">/etc/shadow</code> by placing <code class="literal">!!</code> in the second field.
						</p></div></div><p>
						For an OpenStack instance, it is recommended that you generate an ssh keypair from the OpenStack dashboard or command line and use that key combination to perform an SSH public authentication to the instance as root.
					</p><p>
						When the instance is launched, this public key will be injected to it. You can then authenticate using the private key downloaded while creating the keypair.
					</p><p>
						If you do not want to use keypairs, you can use the <code class="literal">admin</code> password that has been set using the <a class="link" href="index.html#section-inject-admin-password" title="7.2.5. Inject an admin Password Into an Instance">Inject an <code class="literal">admin</code> Password Into an Instance</a> procedure.
					</p><p>
						If you want to create custom Red Hat Enterprise Linux or Windows images, see <a class="link" href="index.html#sect-create-rhel7-image" title="1.2.1.2.1. Create a Red Hat Enterprise Linux 7 Image">Create a Red Hat Enterprise Linux 7 Image</a>, <a class="link" href="index.html#sect-create-rhel6-image" title="1.2.1.2.2. Create a Red Hat Enterprise Linux 6 Image">Create a Red Hat Enterprise Linux 6 Image</a>, or <a class="link" href="index.html#sect-create-windows-image" title="1.2.1.2.3. Create a Windows Image">Create a Windows Image</a>.
					</p></section><section class="section" id="section-create-custom-images"><div class="titlepage"><div><div><h4 class="title">1.2.1.2. Create Custom Red Hat Enterprise Linux or Windows Images</h4></div></div></div><p>
						<span class="strong strong"><strong>Prerequisites:</strong></span>
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Linux host machine to create an image. This can be any machine on which you can install and run the Linux packages.
							</li><li class="listitem">
								libvirt, virt-manager (run command <code class="literal">dnf groupinstall -y @virtualization</code>). This installs all packages necessary for creating a guest operating system.
							</li><li class="listitem">
								Libguestfs tools (run command <code class="literal">dnf install -y libguestfs-tools-c</code>). This installs a set of tools for accessing and modifying virtual machine images.
							</li><li class="listitem">
								A Red Hat Enterprise Linux 7 or 6 ISO file (see <a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---7/7.2/x86_64/product-software/">RHEL 7.2 Binary DVD</a> or <a class="link" href="https://access.redhat.com/downloads/content/69/ver=/rhel---6/6.8/x86_64/product-software/">RHEL 6.8 Binary DVD</a>) or a Windows ISO file. If you do not have a Windows ISO file, visit the <a class="link" href="http://www.microsoft.com/en-us/evalcenter/">Microsoft TechNet Evaluation Center</a> and download an evaluation image.
							</li><li class="listitem">
								Text editor, if you want to change the <code class="literal">kickstart</code> files (RHEL only).
							</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							In the following procedures, all commands with the <code class="literal">[root@host]#</code> prompt should be run on your host machine.
						</p></div></div><section class="section" id="sect-create-rhel7-image"><div class="titlepage"><div><div><h5 class="title">1.2.1.2.1. Create a Red Hat Enterprise Linux 7 Image</h5></div></div></div><p>
							This section provides you with the steps to manually create an OpenStack-compatible image in the QCOW2 format using a Red Hat Enterprise Linux 7 ISO file.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Start the installation using <code class="literal">virt-install</code> as shown below:
								</p><pre class="screen">[root@host]# qemu-img create -f qcow2 rhel7.qcow2 8G
[root@host]# virt-install --virt-type kvm --name rhel7 --ram 2048 \
--cdrom /tmp/rhel-server-7.2-x86_64-dvd.iso \
--disk rhel7.qcow2,format=qcow2 \
--network=bridge:virbr0 --graphics vnc,listen=0.0.0.0 \
--noautoconsole --os-type=linux --os-variant=rhel7</pre><p class="simpara">
									This launches an instance and starts the installation process.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If the instance does not launch automatically, run the <code class="literal">virt-viewer</code> command to view the console:
									</p><pre class="screen">[root@host]# virt-viewer rhel7</pre></div></div></li><li class="listitem"><p class="simpara">
									Set up the virtual machine as follows:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											At the initial Installer boot menu, choose the <code class="literal">Install Red Hat Enterprise Linux 7</code>.<span class="emphasis"><em>X</em></span> option. 
											<span class="inlinemediaobject"><img src="RHEL7-Install1.png" alt="RHEL7 Install1"/></span>

										</li><li class="listitem">
											Choose the appropriate <span class="strong strong"><strong>Language</strong></span> and <span class="strong strong"><strong>Keyboard</strong></span> options.
										</li><li class="listitem">
											When prompted about which type of devices your installation uses, choose <span class="strong strong"><strong>Auto-detected installation media</strong></span>. 
											<span class="inlinemediaobject"><img src="RHEL7-Install5.png" alt="RHEL7 Install5"/></span>

										</li><li class="listitem">
											When prompted about which type of installation destination, choose <span class="strong strong"><strong>Local Standard Disks</strong></span>. 
											<span class="inlinemediaobject"><img src="RHEL7-Install6.png" alt="RHEL7 Install6"/></span>
											 For other storage options, choose <span class="strong strong"><strong>Automatically configure partitioning</strong></span>.
										</li><li class="listitem">
											For software selection, choose <span class="strong strong"><strong>Minimal Install</strong></span>.
										</li><li class="listitem">
											For network and host name, choose <code class="literal">eth0</code> for network and choose a <code class="literal">hostname</code> for your device. The default host name is <code class="literal">localhost.localdomain</code>.
										</li><li class="listitem">
											Choose the <code class="literal">root</code> password. 
											<span class="inlinemediaobject"><img src="RHEL7-Install9.png" alt="RHEL7 Install9"/></span>
											 The installation process completes and the <span class="strong strong"><strong>Complete!</strong></span> screen appears.
										</li></ol></div></li><li class="listitem">
									After the installation is complete, reboot the instance and log in as the root user.
								</li><li class="listitem"><p class="simpara">
									Update the <code class="literal">/etc/sysconfig/network-scripts/ifcfg-eth0</code> file so it only contains the following values:
								</p><pre class="screen">TYPE=Ethernet
DEVICE=eth0
ONBOOT=yes
BOOTPROTO=dhcp
NM_CONTROLLED=no</pre></li><li class="listitem">
									Reboot the machine.
								</li><li class="listitem"><p class="simpara">
									Register the machine with the Content Delivery Network.
								</p><pre class="screen"># sudo subscription-manager register
# sudo subscription-manager attach --pool=Valid-Pool-Number-123456
# sudo subscription-manager repos --enable=rhel-7-server-rpms</pre></li><li class="listitem"><p class="simpara">
									Update the system:
								</p><pre class="screen"># dnf -y update</pre></li><li class="listitem"><p class="simpara">
									Install the <code class="literal">cloud-init</code> packages:
								</p><pre class="screen"># dnf install -y cloud-utils-growpart cloud-init</pre></li><li class="listitem"><p class="simpara">
									Edit the <code class="literal">/etc/cloud/cloud.cfg</code> configuration file and under <code class="literal">cloud_init_modules</code> add:
								</p><pre class="screen">- resolv-conf</pre><p class="simpara">
									The <code class="literal">resolv-conf</code> option automatically configures the <code class="literal">resolv.conf</code> when an instance boots for the first time. This file contains information related to the instance such as <code class="literal">nameservers</code>, <code class="literal">domain</code> and other options.
								</p></li><li class="listitem"><p class="simpara">
									Add the following line to <code class="literal">/etc/sysconfig/network</code> to avoid problems accessing the EC2 metadata service:
								</p><pre class="screen">NOZEROCONF=yes</pre></li><li class="listitem"><p class="simpara">
									To ensure the console messages appear in the <code class="literal">Log</code> tab on the dashboard and the <code class="literal">nova console-log</code> output, add the following boot option to the <code class="literal">/etc/default/grub</code> file:
								</p><pre class="screen">GRUB_CMDLINE_LINUX_DEFAULT="console=tty0 console=ttyS0,115200n8"</pre><p class="simpara">
									Run the <code class="literal">grub2-mkconfig</code> command:
								</p><pre class="screen"># grub2-mkconfig -o /boot/grub2/grub.cfg</pre><p class="simpara">
									The output is as follows:
								</p><pre class="screen">Generating grub configuration file ...
Found linux image: /boot/vmlinuz-3.10.0-229.7.2.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-229.7.2.el7.x86_64.img
Found linux image: /boot/vmlinuz-3.10.0-121.el7.x86_64
Found initrd image: /boot/initramfs-3.10.0-121.el7.x86_64.img
Found linux image: /boot/vmlinuz-0-rescue-b82a3044fb384a3f9aeacf883474428b
Found initrd image: /boot/initramfs-0-rescue-b82a3044fb384a3f9aeacf883474428b.img
done</pre></li><li class="listitem"><p class="simpara">
									Un-register the virtual machine so that the resulting image does not contain the same subscription details for every instance cloned based on it:
								</p><pre class="screen"># subscription-manager repos --disable=*
# subscription-manager unregister
# dnf clean all</pre></li><li class="listitem"><p class="simpara">
									Power off the instance:
								</p><pre class="screen"># poweroff</pre></li><li class="listitem"><p class="simpara">
									Reset and clean the image using the <code class="literal">virt-sysprep</code> command so it can be used to create instances without issues:
								</p><pre class="screen">[root@host]# virt-sysprep -d rhel7</pre></li><li class="listitem"><p class="simpara">
									Reduce image size using the <code class="literal">virt-sparsify</code> command. This command converts any free space within the disk image back to free space within the host:
								</p><pre class="screen">[root@host]# virt-sparsify --compress /tmp/rhel7.qcow2 rhel7-cloud.qcow2</pre><p class="simpara">
									This creates a new <code class="literal">rhel7-cloud.qcow2</code> file in the location from where the command is run.
								</p></li></ol></div><p>
							The <code class="literal">rhel7-cloud.qcow2</code> image file is ready to be uploaded to the Image service. For more information on uploading this image to your OpenStack deployment using the dashboard, see <a class="link" href="index.html#section-upload-image" title="1.2.2. Upload an image">Upload an Image</a>.
						</p></section><section class="section" id="sect-create-rhel6-image"><div class="titlepage"><div><div><h5 class="title">1.2.1.2.2. Create a Red Hat Enterprise Linux 6 Image</h5></div></div></div><p>
							This section provides you with the steps to manually create an OpenStack-compatible image in the QCOW2 format using a Red Hat Enterprise Linux 6 ISO file.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Start the installation using <code class="literal">virt-install</code>:
								</p><pre class="screen">[root@host]# qemu-img create -f qcow2 rhel6.qcow2 4G
[root@host]# virt-install --connect=qemu:///system --network=bridge:virbr0 \
--name=rhel6 --os-type linux --os-variant rhel6 \
--disk path=rhel6.qcow2,format=qcow2,size=10,cache=none \
--ram 4096 --vcpus=2 --check-cpu --accelerate \
--hvm --cdrom=rhel-server-6.8-x86_64-dvd.iso</pre><p class="simpara">
									This launches an instance and starts the installation process.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If the instance does not launch automatically, run the <code class="literal">virt-viewer</code> command to view the console:
									</p><pre class="screen">[root@host]# virt-viewer rhel6</pre></div></div></li><li class="listitem"><p class="simpara">
									Set up the virtual machines as follows:
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
											At the initial Installer boot menu, choose the <span class="strong strong"><strong>Install or upgrade an existing system</strong></span> option. 
											<span class="inlinemediaobject"><img src="RHEL6-Install1.png" alt="RHEL6 Install1"/></span>
											 Step through the installation prompts. Accept the defaults.
										</p><p class="simpara">
											The installer checks for the disc and lets you decide whether you want to test your installation media before installation. Select <span class="strong strong"><strong>OK</strong></span> to run the test or <span class="strong strong"><strong>Skip</strong></span> to proceed without testing.
										</p></li><li class="listitem">
											Choose the appropriate <span class="strong strong"><strong>Language</strong></span> and <span class="strong strong"><strong>Keyboard</strong></span> options.
										</li><li class="listitem">
											When prompted about which type of devices your installation uses, choose <span class="strong strong"><strong>Basic Storage Devices</strong></span>. 
											<span class="inlinemediaobject"><img src="RHEL6-Install3.png" alt="RHEL6 Install3"/></span>

										</li><li class="listitem">
											Choose a <code class="literal">hostname</code> for your device. The default host name is <code class="literal">localhost.localdomain</code>.
										</li><li class="listitem">
											Set <span class="strong strong"><strong>timezone</strong></span> and <code class="literal">root</code> password.
										</li><li class="listitem">
											Based on the space on the disk, choose the type of installation. 
											<span class="inlinemediaobject"><img src="RHEL6-Install4.png" alt="RHEL6 Install4"/></span>

										</li><li class="listitem">
											Choose the <span class="strong strong"><strong>Basic Server</strong></span> install, which installs an SSH server. 
											<span class="inlinemediaobject"><img src="RHEL6-Install5.png" alt="RHEL6 Install5"/></span>

										</li><li class="listitem">
											The installation process completes and <span class="strong strong"><strong>Congratulations, your Red Hat Enterprise Linux installation is complete</strong></span> screen appears.
										</li></ol></div></li><li class="listitem">
									Reboot the instance and log in as the <code class="literal">root</code> user.
								</li><li class="listitem"><p class="simpara">
									Update the <code class="literal">/etc/sysconfig/network-scripts/ifcfg-eth0</code> file so it only contains the following values:
								</p><pre class="screen">TYPE=Ethernet
DEVICE=eth0
ONBOOT=yes
BOOTPROTO=dhcp
NM_CONTROLLED=no</pre></li><li class="listitem">
									Reboot the machine.
								</li><li class="listitem"><p class="simpara">
									Register the machine with the Content Delivery Network:
								</p><pre class="screen"># sudo subscription-manager register
# sudo subscription-manager attach --pool=Valid-Pool-Number-123456
# sudo subscription-manager repos --enable=rhel-6-server-rpms</pre></li><li class="listitem"><p class="simpara">
									Update the system:
								</p><pre class="screen"># dnf -y update</pre></li><li class="listitem"><p class="simpara">
									Install the <code class="literal">cloud-init</code> packages:
								</p><pre class="screen"># dnf install -y cloud-utils-growpart cloud-init</pre></li><li class="listitem"><p class="simpara">
									Edit the <code class="literal">/etc/cloud/cloud.cfg</code> configuration file and under <code class="literal">cloud_init_modules</code> add:
								</p><pre class="screen">- resolv-conf</pre><p class="simpara">
									The <code class="literal">resolv-conf</code> option automatically configures the <code class="literal">resolv.conf</code> configuration file when an instance boots for the first time. This file contains information related to the instance such as <code class="literal">nameservers</code>, <code class="literal">domain</code>, and other options.
								</p></li><li class="listitem"><p class="simpara">
									To prevent network issues, create the <code class="literal">/etc/udev/rules.d/75-persistent-net-generator.rules</code> file as follows:
								</p><pre class="screen"># echo "#" &gt; /etc/udev/rules.d/75-persistent-net-generator.rules</pre><p class="simpara">
									This prevents <code class="literal">/etc/udev/rules.d/70-persistent-net.rules</code> file from being created. If <code class="literal">/etc/udev/rules.d/70-persistent-net.rules</code> is created, networking may not function properly when booting from snapshots (the network interface is created as "eth1" rather than "eth0" and IP address is not assigned).
								</p></li><li class="listitem"><p class="simpara">
									Add the following line to <code class="literal">/etc/sysconfig/network</code> to avoid problems accessing the EC2 metadata service:
								</p><pre class="screen">NOZEROCONF=yes</pre></li><li class="listitem"><p class="simpara">
									To ensure the console messages appear in the <code class="literal">Log</code> tab on the dashboard and the <code class="literal">nova console-log</code> output, add the following boot option to the <code class="literal">/etc/grub.conf</code>:
								</p><pre class="screen">console=tty0 console=ttyS0,115200n8</pre></li><li class="listitem"><p class="simpara">
									Un-register the virtual machine so that the resulting image does not contain the same subscription details for every instance cloned based on it:
								</p><pre class="screen"># subscription-manager repos --disable=*
# subscription-manager unregister
# dnf clean all</pre></li><li class="listitem"><p class="simpara">
									Power off the instance:
								</p><pre class="screen"># poweroff</pre></li><li class="listitem"><p class="simpara">
									Reset and clean the image using the <code class="literal">virt-sysprep</code> command so it can be used to create instances without issues:
								</p><pre class="screen">[root@host]# virt-sysprep -d rhel6</pre></li><li class="listitem"><p class="simpara">
									Reduce image size using the <code class="literal">virt-sparsify</code> command. This command converts any free space within the disk image back to free space within the host:
								</p><pre class="screen">[root@host]# virt-sparsify --compress rhel6.qcow2 rhel6-cloud.qcow2</pre><p class="simpara">
									This creates a new <code class="literal">rhel6-cloud.qcow2</code> file in the location from where the command is run.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										You will need to manually resize the partitions of instances based on the image in accordance with the disk space in the flavor that is applied to the instance.
									</p></div></div></li></ol></div><p>
							The <code class="literal">rhel6-cloud.qcow2</code> image file is ready to be uploaded to the Image service. For more information on uploading this image to your OpenStack deployment using the dashboard, see <a class="link" href="index.html#section-upload-image" title="1.2.2. Upload an image">Upload an Image</a>
						</p></section><section class="section" id="sect-create-windows-image"><div class="titlepage"><div><div><h5 class="title">1.2.1.2.3. Create a Windows Image</h5></div></div></div><p>
							This section provides you with the steps to manually create an OpenStack-compatible image in the QCOW2 format using a Windows ISO file.
						</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
									Start the installation using <code class="literal">virt-install</code> as shown below:
								</p><pre class="screen">[root@host]# virt-install --name=<span class="emphasis"><em>name</em></span> \
--disk size=<span class="emphasis"><em>size</em></span> \
--cdrom=<span class="emphasis"><em>path</em></span> \
--os-type=windows \
--network=bridge:virbr0 \
--graphics spice \
--ram=<span class="emphasis"><em>RAM</em></span></pre><p class="simpara">
									Replace the values of the <code class="literal">virt-install</code> parameters as follows:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<span class="emphasis"><em>name</em></span> — the name that the Windows guest should have.
										</li><li class="listitem">
											<span class="emphasis"><em>size</em></span> — disk size in GB.
										</li><li class="listitem">
											<span class="emphasis"><em>path</em></span> — the path to the Windows installation ISO file.
										</li><li class="listitem"><p class="simpara">
											<span class="emphasis"><em>RAM</em></span> — the requested amount of RAM in MB.
										</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
												The <code class="literal">--os-type=windows</code> parameter ensures that the clock is set up correctly for the Windows guest, and enables its Hyper-V enlightenment features.
											</p></div></div><p class="simpara">
											Note that <code class="literal">virt-install</code> saves the guest image as <code class="literal">/var/lib/libvirt/images/</code><code class="literal"><span class="emphasis"><em>name</em></span></code>.<code class="literal">qcow2</code> by default. If you want to keep the guest image elsewhere, change the parameter of the <code class="literal">--disk</code> option as follows:
										</p><pre class="screen">--disk path=<span class="emphasis"><em>filename</em></span>,size=<span class="emphasis"><em>size</em></span></pre><p class="simpara">
											Replace <span class="emphasis"><em>filename</em></span> with the name of the file which should store the guest image (and optionally its path); for example <code class="literal">path=win8.qcow2,size=8</code> creates an 8 GB file named <code class="literal">win8.qcow2</code> in the current working directory.
										</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
											If the guest does not launch automatically, run the <code class="literal">virt-viewer</code> command to view the console:
										</p><pre class="screen">[root@host]# virt-viewer <span class="emphasis"><em>name</em></span></pre></div></div></li></ul></div></li><li class="listitem">
									Installation of Windows systems is beyond the scope of this document. For instructions on how to install Windows, see the relevant Microsoft documentation.
								</li><li class="listitem">
									To allow the newly installed Windows system to use the virtualized hardware, you might need to install <span class="emphasis"><em>virtio drivers</em></span>. To so do, first install the image, which you must attach as a CD-ROM drive to the Windows guest. To install the <code class="literal">virtio-win</code> package you must add the virtio ISO image to the guest, and install the virtio drivers. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_virtualization/index#installing-kvm-paravirtualized-drivers-for-rhel-8-virtual-machines_optimizing-windows-virtual-machines-on-rhel-8">Installing KVM paravirtualized drivers for Windows virtual machines</a> in the <span class="emphasis"><em>Configuring and managing virtualization</em></span> guide.
								</li><li class="listitem"><p class="simpara">
									To complete the setup, download and execute <a class="link" href="http://www.cloudbase.it/cloudbase-init/">Cloudbase-Init</a> on the Windows system. At the end of the installation of Cloudbase-Init, select the <code class="literal">Run Sysprep</code> and <code class="literal">Shutdown</code> check boxes. The <code class="literal">Sysprep</code> tool makes the guest unique by generating an OS ID, which is used by certain Microsoft services.
								</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
										Red Hat does not provide technical support for Cloudbase-Init. If you encounter an issue, <a class="link" href="https://cloudbase.it/about/#contact">contact Cloudbase Solutions</a>.
									</p></div></div></li></ol></div><p>
							When the Windows system shuts down, the <span class="emphasis"><em>name</em></span>.<code class="literal">qcow2</code> image file is ready to be uploaded to the Image service. For more information on uploading this image to your OpenStack deployment using the dashboard or the command line, see <a class="link" href="index.html#section-upload-image" title="1.2.2. Upload an image">Upload an Image</a>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								<span class="strong strong"><strong>libosinfo data</strong></span>
							</p><p>
								The Compute Service has deprecated support for using libosinfo data to set default device models. Instead, use the following image metadata properties to configure the optimal virtual hardware for an instance:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">os_distro</code>
									</li><li class="listitem">
										<code class="literal">os_version</code>
									</li><li class="listitem">
										<code class="literal">hw_cdrom_bus</code>
									</li><li class="listitem">
										<code class="literal">hw_disk_bus</code>
									</li><li class="listitem">
										<code class="literal">hw_scsi_model</code>
									</li><li class="listitem">
										<code class="literal">hw_vif_model</code>
									</li><li class="listitem">
										<code class="literal">hw_video_model</code>
									</li><li class="listitem">
										<code class="literal">hypervisor_type</code>
									</li></ul></div><p>
								For more information on these metadata properties, see <a class="xref" href="index.html#appx-image-metadata">Appendix A, <em>Image Configuration Parameters</em></a>.
							</p></div></div></section></section></section><section class="section" id="section-upload-image"><div class="titlepage"><div><div><h3 class="title">1.2.2. Upload an image</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Images</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Create Image</strong></span>.
						</li><li class="listitem">
							Fill out the values, and click <span class="strong strong"><strong>Create Image</strong></span> when finished.
						</li></ol></div><div class="table" id="idm139747283094016"><p class="title"><strong>Table 1.1. Image Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 29%; " class="col_1"><!--Empty--></col><col style="width: 71%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747280479216" scope="col">Field</th><th align="left" valign="top" id="idm139747280478128" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Name
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Name for the image. The name must be unique within the project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Description
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Brief description to identify the image.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Image Source
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Image source: <span class="strong strong"><strong>Image Location</strong></span> or <span class="strong strong"><strong>Image File</strong></span>. Based on your selection, the next field is displayed.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Image Location or Image File
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Select <span class="strong strong"><strong>Image Location</strong></span> option to specify the image location URL.
										</li><li class="listitem">
											Select <span class="strong strong"><strong>Image File</strong></span> option to upload an image from the local disk.
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Format
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Image format (for example, qcow2).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Architecture
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Image architecture. For example, use i686 for a 32-bit architecture or x86_64 for a 64-bit architecture.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Minimum Disk (GB)
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Minimum disk size required to boot the image. If this field is not specified, the default value is 0 (no minimum).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Minimum RAM (MB)
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									Minimum memory size required to boot the image. If this field is not specified, the default value is 0 (no minimum).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Public
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									If selected, makes the image public to all users with access to the project.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747280479216"> <p>
									Protected
								</p>
								 </td><td align="left" valign="top" headers="idm139747280478128"> <p>
									If selected, ensures only users with specific permissions can delete this image.
								</p>
								 </td></tr></tbody></table></div></div><p>
					When the image has been successfully uploaded, its status is changed to <code class="literal">active</code>, which indicates that the image is available for use. Note that the Image service can handle even large images that take a long time to upload — longer than the lifetime of the Identity service token which was used when the upload was initiated. This is due to the fact that the Image service first creates a trust with the Identity service so that a new token can be obtained and used when the upload is complete and the status of the image is to be updated.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use the <code class="literal">glance image-create</code> command with the <code class="literal">property</code> option to upload an image. More values are available on the command line. For a complete listing, see <a class="link" href="index.html#appx-image-config-parameters" title="Appendix A. Image Configuration Parameters">Image Configuration Parameters</a>.
					</p></div></div></section><section class="section" id="section-update-image"><div class="titlepage"><div><div><h3 class="title">1.2.3. Update an image</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Images</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Edit Image</strong></span> from the dropdown list.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <span class="strong strong"><strong>Edit Image</strong></span> option is available only when you log in as an <code class="literal">admin</code> user. When you log in as a <code class="literal">demo</code> user, you have the option to <span class="strong strong"><strong>Launch an instance</strong></span> or <span class="strong strong"><strong>Create Volume</strong></span>.
							</p></div></div></li><li class="listitem">
							Update the fields and click <span class="strong strong"><strong>Update Image</strong></span> when finished. You can update the following values - name, description, kernel ID, ramdisk ID, architecture, format, minimum disk, minimum RAM, public, protected.
						</li><li class="listitem">
							Click the drop-down menu and select <span class="strong strong"><strong>Update Metadata</strong></span> option.
						</li><li class="listitem">
							Specify metadata by adding items from the left column to the right one. In the left column, there are metadata definitions from the Image Service Metadata Catalog. Select <span class="strong strong"><strong>Other</strong></span> to add metadata with the key of your choice and click <span class="strong strong"><strong>Save</strong></span> when finished.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use the <code class="literal">glance image-update</code> command with the <code class="literal">property</code> option to update an image. More values are available on the command line; for a complete listing, see <a class="link" href="index.html#appx-image-config-parameters" title="Appendix A. Image Configuration Parameters">Image Configuration Parameters</a>.
					</p></div></div></section><section class="section" id="section-import-image"><div class="titlepage"><div><div><h3 class="title">1.2.4. Import an image</h3></div></div></div><p>
					You can import images into the Image service (glance) using <code class="literal command">web-download</code> to import an image from a URI and <code class="literal command">glance-direct</code> to import an image from a local file system. The <code class="literal">web-download</code> method is enabled by default.
				</p><p>
					Import methods are configured by the cloud administrator. Run the <code class="literal command">glance import-info</code> command to list available import options.
				</p><section class="section" id="section-web-download"><div class="titlepage"><div><div><h4 class="title">1.2.4.1. Import from a remote URI</h4></div></div></div><p>
						You can use the <code class="literal command">web-download</code> method to copy an image from a remote URI.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create an image and specify the URI of the image to import.
							</p><pre class="screen">glance image-create --uri &lt;URI&gt;</pre></li><li class="listitem">
								You can monitor the image’s availability using the <code class="literal command">glance image-show &lt;image-ID&gt;</code> command where the ID is the one provided during image creation.
							</li></ol></div><p>
						The Image service web-download method uses a two-stage process to perform the import. First, it creates an image record. Second, it retrieves the image the specified URI. This method provides a more secure way to import images than the deprecated <code class="literal command">copy-from</code> method used in Image API v1.
					</p><p>
						The URI is subject to optional blacklist and whitelist filtering as described in the Advanced Overcloud Customization Guide.
					</p><p>
						The Image Property Injection plugin may inject metadata properties to the image as described in the Advanced Overcloud Customization Guide. These injected properties determine which compute nodes the image instances are launched on.
					</p></section><section class="section" id="section-glance-direct"><div class="titlepage"><div><div><h4 class="title">1.2.4.2. Import from a local volume</h4></div></div></div><p>
						The <code class="literal command">glance-direct</code> method creates an image record, which generates an image ID. Once the image is uploaded to the service from a local volume, it is stored in a staging area and is made active after it passes any configured checks. The <code class="literal command">glance-direct</code> method requires a shared staging area when used in a highly available (HA) configuration.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Image uploads using the <code class="literal command">glance-direct</code> method fail in an HA environment if a common staging area is not present. In an HA active-active environment, API calls are distributed to the glance controllers. The download API call could be sent to a different controller than the API call to upload the image. For more information about configuring the staging area, refer to the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/storage_configuration">Storage Configuration</a> section in the <span class="emphasis"><em>Advanced Overcloud Customization Guide</em></span>.
						</p></div></div><p>
						The glance-direct method uses three different calls to import an image:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal command">glance image-create</code>
							</li><li class="listitem">
								<code class="literal command">glance image-stage</code>
							</li><li class="listitem">
								<code class="literal command">glance image-import</code>
							</li></ul></div><p>
						You can use the <code class="literal comand">glance image-create-via-import</code> command to perform all three of these calls in one command. In the example below, uppercase words should be replaced with the appropriate options.
					</p><pre class="screen">glance image-create-via-import --container-format FORMAT --disk-format DISKFORMAT --name NAME --file /PATH/TO/IMAGE</pre><p>
						Once the image moves from the staging area to the back end location, the image is listed. However, it may take some time for the image to become active.
					</p><p>
						You can monitor the image’s availability using the <code class="literal command">glance image-show &lt;image-ID&gt;</code> command where the ID is the one provided during image creation.
					</p></section></section><section class="section" id="section-delete-image"><div class="titlepage"><div><div><h3 class="title">1.2.5. Delete an image</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Images</strong></span>.
						</li><li class="listitem">
							Select the image you want to delete and click <span class="strong strong"><strong>Delete Images</strong></span>.
						</li></ol></div></section><section class="section" id="section-hide-image"><div class="titlepage"><div><div><h3 class="title">1.2.6. Hide or unhide an image</h3></div></div></div><p>
					You can hide public images from normal listings presented to users. For instance, you can hide obsolete CentOS 7 images and show only the latest version to simplify the user experience. Users can discover and use hidden images.
				</p><p>
					To hide an image:
				</p><pre class="screen">glance image-update &lt;image-id&gt; --hidden 'true'</pre><p>
					To create a hidden image, add the <code class="literal command">--hidden</code> argument to the <code class="literal command">glance image-create</code> command.
				</p><p>
					To unhide an image:
				</p><pre class="screen">glance image-update &lt;image-id&gt; --hidden 'false'</pre></section><section class="section" id="section-show-hidden-image"><div class="titlepage"><div><div><h3 class="title">1.2.7. Show hidden images</h3></div></div></div><p>
					To list hidden images:
				</p><pre class="screen">glance image-list --hidden 'true'</pre></section><section class="section" id="enabling-image-conversion"><div class="titlepage"><div><div><h3 class="title">1.2.8. Enabling image conversion</h3></div></div></div><p>
					With the <code class="literal">GlanceImageImportPlugins</code> parameter enabled, you can upload a QCOW2 image, and the Image service will convert it to RAW.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Image conversion is automatically enabled when you use Red Hat Ceph Storage RBD to store images and boot Nova instances.
					</p></div></div><p>
					To enable image conversion, create an environment file that contains the following parameter value and include the new environment file with the <code class="literal">-e</code> option in the <code class="literal">openstack overcloud deploy</code> command:
				</p><pre class="screen">parameter_defaults:
  GlanceImageImportPlugins:'image_conversion'</pre></section><section class="section" id="section-convert-image"><div class="titlepage"><div><div><h3 class="title">1.2.9. Converting an image to RAW format</h3></div></div></div><p>
					Red Hat Ceph Storage can store, but does not support using, QCOW2 images to host virtual machine (VM) disks.
				</p><p>
					When you upload a QCOW2 image and create a VM from it, the compute node downloads the image, converts the image to RAW, and uploads it back into Ceph, which can then use it. This process affects the time it takes to create VMs, especially during parallel VM creation.
				</p><p>
					For example, when you create multiple VMs simultaneously, uploading the converted image to the Ceph cluster may impact already running workloads. The upload process can starve those workloads of IOPS and impede storage responsiveness.
				</p><p>
					To boot VMs in Ceph more efficiently (ephemeral back end or boot from volume), the glance image format must be RAW.
				</p><p>
					Converting an image to RAW may yield an image that is larger in size than the original QCOW2 image file. Run the following command before the conversion to determine the final RAW image size:
				</p><pre class="screen">qemu-img info &lt;image&gt;.qcow2</pre><p>
					To convert an image from QCOW2 to RAW format, do the following:
				</p><pre class="screen">qemu-img convert -p -f qcow2 -O raw &lt;original qcow2 image&gt;.qcow2 &lt;new raw image&gt;.raw</pre><section class="section" id="configuring_image_service_to_accept_raw_and_iso_only"><div class="titlepage"><div><div><h4 class="title">1.2.9.1. Configuring Image Service to accept RAW and ISO only</h4></div></div></div><p>
						Optionally, to configure the Image Service to accept only RAW and ISO image formats, deploy using an additional environment file that contains the following:
					</p><pre class="screen">parameter_defaults:
  ExtraConfig:
    glance::config::api_config:
      image_format/disk_formats:
        value: "raw,iso"</pre></section></section><section class="section" id="storing-image-in-raw-format"><div class="titlepage"><div><div><h3 class="title">1.2.10. Storing an image in RAW format</h3></div></div></div><p>
					With the <code class="literal">GlanceImageImportPlugins</code> parameter enabled, run the following command to store a previously created image in RAW format:
				</p><pre class="screen">$ glance image-create-via-import \
    --disk-format qcow2 \
    --container-format bare \
    --name <span class="emphasis"><em>NAME</em></span> \
    --visibility public \
    --import-method web-download \
    --uri <span class="emphasis"><em>http://server/image.qcow2</em></span></pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							For <code class="literal">--name</code>, replace <code class="literal">NAME</code> with the name of the image; this is the name that will appear in <code class="literal">glance image-list</code>.
						</li><li class="listitem">
							For <code class="literal">--uri</code>, replace <code class="literal">http://server/image.qcow2</code> with the location and file name of the QCOW2 image.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This command example creates the image record and imports it by using the <code class="literal">web-download</code> method. The glance-api downloads the image from the <code class="literal">--uri</code> location during the import process. If <code class="literal">web-download</code> is not available, <code class="literal">glanceclient</code> cannot automatically download the image data. Run the <code class="literal">glance import-info</code> command to list the available image import methods.
					</p></div></div></section></section></section><section class="chapter" id="using-image-service-with-mulitple-stores"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Image service with multiple stores</h1></div></div></div><p>
			The Red Hat OpenStack Platform Image service (glance) supports using multiple stores with distributed edge architecture so that you can have an image pool at every edge site. You can copy images between the central site, which is also known as the hub site, and the edge sites.
		</p><p>
			The image metadata contains the location of each copy. For example, an image present on two edge sites is exposed as a single UUID with three locations: the central site plus the two edge sites. This means you can have copies of image data that share a single UUID on many stores. For more information about locations, see <a class="link" href="index.html#understanding-locations-of-images" title="2.5. Understanding the locations of images">Understanding the location of images</a>.
		</p><p>
			With an RBD image pool at every edge site, you can boot VMs quickly by using Ceph RBD copy-on-write (COW) and snapshot layering technology. This means that you can boot VMs from volumes and have live migration. For more information about layering with Ceph RBD, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/block_device_guide/index#ceph-block-device-layering_block">Ceph block device layering</a> in the <span class="emphasis"><em>Block Device Guide</em></span>.
		</p><section class="section" id="requirements_of_storage_edge_architecture"><div class="titlepage"><div><div><h2 class="title">2.1. Requirements of storage edge architecture</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						A copy of each image must exist in the Image service at the central location.
					</li><li class="listitem">
						Prior to creating an instance at an edge site, you must have a local copy of the image at that edge site.
					</li><li class="listitem">
						Source the <code class="literal">centralrc</code> authentication file to schedule workloads at edge sites as well as at the central location. Authentication files that are automatically generated for edge sites are not needed.
					</li><li class="listitem">
						Images uploaded to an edge site must be copied to the central location before they can be copied to other edge sites.
					</li><li class="listitem">
						Use the Image service RBD driver for all edge sites. Mixed architecture is not supported.
					</li><li class="listitem">
						Multistack must be used with a single stack at each site.
					</li><li class="listitem">
						RBD must be the storage driver for the Image, Compute and Block Storage services.
					</li><li class="listitem">
						For each site, you must assign the same value to the <code class="literal">NovaComputeAvailabilityZone</code> and <code class="literal">CinderStorageAvailabilityZone</code> parameters.
					</li></ul></div></section><section class="section" id="import-an-image-to-multiple-stores"><div class="titlepage"><div><div><h2 class="title">2.2. Import an image to multiple stores</h2></div></div></div><p>
				Use the interoperable image import workflow to import image data into multiple Ceph Storage clusters. You can import images into the Image service that are available on the local file system or through a web server.
			</p><p>
				If you import an image from a web server, the image can be imported into multiple stores at once. If the image is not available on a web server, you can import the image from a local file system into the central store and then copy it to additional stores. For more information, see <a class="link" href="index.html#copy-an-existing-image-to-multiple-stores" title="2.3. Copy an existing image to multiple stores">Copy an existing image to multiple stores</a>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Always store an image copy on the central site, even if there are no instances using the image at the central location. For more information about importing images into the Image service, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/distributed_compute_node_and_storage_deployment/index">Distributed compute node and storage deployment</a> guide.
				</p></div></div><section class="section" id="manage-image-import-failures"><div class="titlepage"><div><div><h3 class="title">2.2.1. Manage image import failures</h3></div></div></div><p>
					You can manage failures of the image import operation by using the <code class="literal">--allow-failure</code> parameter:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							If the value of the <code class="literal">--allow-failure</code> parameter to <code class="literal">true</code>, the image status becomes <code class="literal">active</code> after the first store successfully imports the data. This is the default setting. You can view a list of stores that failed to import the image data by using the <code class="literal">os_glance_failed_import</code> image property.
						</li><li class="listitem">
							If you set the value of the <code class="literal">--allow-failure</code> parameter to <code class="literal">false</code>, the image status only becomes <code class="literal">active</code> after all specified stores successfully import the data. Failure of any store to import the image data results in an image status of <code class="literal">failed</code>. The image is not imported into any of the specified stores.
						</li></ul></div></section><section class="section" id="importing-image-data-to-multiple-stores"><div class="titlepage"><div><div><h3 class="title">2.2.2. Importing image data to multiple stores</h3></div></div></div><p>
					Because the default setting of the <code class="literal">--allow-failure</code> parameter is <code class="literal">true</code>, you do not need to include the parameter in the command if it is acceptable for some stores to fail to import the image data.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						This procedure does not require all stores to successfully import the image data.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Import image data to multiple, specified stores:
						</p><pre class="screen">$ glance image-create-via-import \
--container-format bare \
--name <span class="emphasis"><em>IMAGE-NAME</em></span> \
--import-method web-download \
--uri <span class="emphasis"><em>URI</em></span> \
--stores <span class="emphasis"><em>STORE1</em></span>,<span class="emphasis"><em>STORE2</em></span>,<span class="emphasis"><em>STORE3</em></span></pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <span class="emphasis"><em>IMAGE-NAME</em></span> with the name of the image you want to import.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>URI</em></span> with the URI of the image.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>STORE1</em></span>, <span class="emphasis"><em>STORE2</em></span>, and <span class="emphasis"><em>STORE3</em></span> with the names of the stores to which you want to import the image data.
								</li><li class="listitem">
									Alternatively, replace <code class="literal">--stores</code> with <code class="literal">--all-stores true</code> to upload the image to all the stores.
								</li></ul></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">glance image-create-via-import</code> command, which automatically converts the QCOW2 image to RAW format, works only with the <code class="literal">web-download</code> method. The <code class="literal">glance-direct</code> method is available, but it works only in deployments with a configured shared file system. For more information, see <a class="link" href="index.html#storing-image-in-raw-format" title="1.2.10. Storing an image in RAW format">Storing an image in RAW format</a>.
					</p></div></div></section><section class="section" id="importing-image-data-multiple-stores-no-failure"><div class="titlepage"><div><div><h3 class="title">2.2.3. Importing image data to multiple stores without failure</h3></div></div></div><p>
					This procedure requires all stores to successfully import the image data.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Import image data to multiple, specified stores:
						</p><pre class="screen">$ glance image-create-via-import \
--container-format bare \
--name <span class="emphasis"><em>IMAGE-NAME</em></span> \
--import-method web-download \
--uri <span class="emphasis"><em>URI</em></span> \
--stores <span class="emphasis"><em>STORE1</em></span>,<span class="emphasis"><em>STORE2</em></span></pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <span class="emphasis"><em>IMAGE-NAME</em></span> with the name of the image you want to import.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>URI</em></span> with the URI of the image.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>STORE1</em></span>, <span class="emphasis"><em>STORE2</em></span>, and <span class="emphasis"><em>STORE3</em></span> with the names of stores to which you want to copy the image data.
								</li><li class="listitem"><p class="simpara">
									Alternatively, replace <code class="literal">--stores</code> with <code class="literal">--all-stores true</code> to upload the image to all the stores.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										With the <code class="literal">--allow-failure</code> parameter set to <code class="literal">false</code>, the Image service does not ignore stores that fail to import the image data. You can view the list of failed stores with the image property <code class="literal">os_glance_failed_import</code>. For more information see <a class="link" href="index.html#checking-progress-of-image-import-operation" title="2.2.5. Checking the progress of the image import operation">Checking the progress of image import operation</a>.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the image data was added to specific stores:
						</p><pre class="screen">$ glance image-show <span class="emphasis"><em>IMAGE-ID</em></span> | grep stores</pre><p class="simpara">
							Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the ID of the original existing image.
						</p><p class="simpara">
							The output displays a comma-delimited list of stores.
						</p></li></ol></div></section><section class="section" id="importing-image-data-to-single-store"><div class="titlepage"><div><div><h3 class="title">2.2.4. Importing image data to a single store</h3></div></div></div><p>
					You can import image data to a single store.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Import image data to a single store:
						</p><pre class="screen">$ glance image-create-via-import \
--container-format bare \
--name <span class="emphasis"><em>IMAGE-NAME</em></span> \
--import-method web-download \
--uri <span class="emphasis"><em>URI</em></span> \
--store <span class="emphasis"><em>STORE</em></span></pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <span class="emphasis"><em>IMAGE-NAME</em></span> with the name of the image you want to import.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>URI</em></span> with the URI of the image.
								</li><li class="listitem"><p class="simpara">
									Replace <span class="emphasis"><em>STORE</em></span> with the name of the store to which you want to copy the image data.
								</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
										If you do not include the options of <code class="literal">--stores</code>, <code class="literal">--all-stores</code>, or <code class="literal">--store</code> in the command, the Image service creates the image in the central store.
									</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
							Verify that the image data was added to specific store:
						</p><pre class="screen">$ glance image-show <span class="emphasis"><em>IMAGE-ID</em></span> | grep stores</pre><p class="simpara">
							Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the ID of the original existing image.
						</p><p class="simpara">
							The output displays a comma-delimited list of stores.
						</p></li></ol></div></section><section class="section" id="checking-progress-of-image-import-operation"><div class="titlepage"><div><div><h3 class="title">2.2.5. Checking the progress of the image import operation</h3></div></div></div><p>
					The interoperable image import workflow sequentially imports image data into stores. The size of the image, the number of stores, and the network speed between the central site and the edge sites impact how long it takes for the image import operation to complete.
				</p><p>
					You can follow the progress of the image import by looking at two image properties, which appear in notifications sent during the image import operation:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">os_glance_importing_to_stores</code> property lists the stores that have not imported the image data. At the beginning of the import, all requested stores show up in the list. Each time a store successfully imports the image data, the Image service removes the store from the list.
						</li><li class="listitem">
							The <code class="literal">os_glance_failed_import</code> property lists the stores that fail to import the image data. This list is empty at the beginning of the image import operation.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In the following procedure, the environment has three Ceph Storage clusters: the <code class="literal">central</code> store and two stores at the edge, <code class="literal">dcn0</code> and <code class="literal">dcn1</code>.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the image data was added to specific stores:
						</p><pre class="screen">$ glance image-show <span class="emphasis"><em>IMAGE-ID</em></span></pre><p class="simpara">
							Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the ID of the original existing image.
						</p><p class="simpara">
							The output displays a comma-delimited list of stores similar to the following example snippet:
						</p><pre class="screen">| os_glance_failed_import       |
| os_glance_importing_to_stores | central,dcn0,dcn1
| status                        | importing</pre></li><li class="listitem"><p class="simpara">
							Monitor the status of the image import operation. When you precede a command with <code class="literal">watch</code>, the command output refreshes every two seconds.
						</p><pre class="screen">$ watch glance image-show <span class="emphasis"><em>IMAGE-ID</em></span></pre><p class="simpara">
							Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the ID of the original existing image.
						</p><p class="simpara">
							The status of the operation changes as the image import operation progresses:
						</p><pre class="screen">| os_glance_failed_import       |
| os_glance_importing_to_stores | dcn0,dcn1
| status                        | importing</pre><p class="simpara">
							Output that shows that an image failed to import resembles the following example:
						</p><pre class="screen">| os_glance_failed_import       | dcn0
| os_glance_importing_to_stores | dcn1
| status                        | importing</pre><p class="simpara">
							After the operation completes, the status changes to active:
						</p><pre class="screen">| os_glance_failed_import       | dcn0
| os_glance_importing_to_stores |
| status                        | active</pre></li></ol></div></section></section><section class="section" id="copy-an-existing-image-to-multiple-stores"><div class="titlepage"><div><div><h2 class="title">2.3. Copy an existing image to multiple stores</h2></div></div></div><p>
				This feature enables you to copy existing images using Red Hat OpenStack Image service (glance) image data into multiple Ceph Storage stores at the edge by using the interoperable image import workflow.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The image must be present at the central site before you copy it to any edge sites. Only the image owner or administrator can copy existing images to newly added stores.
				</p></div></div><p>
				You can copy existing image data either by setting <code class="literal">--all-stores</code> to <code class="literal">true</code> or by specifying specific stores to receive the image data.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The default setting for the <code class="literal">--all-stores</code> option is <code class="literal">false</code>. If <code class="literal">--all-stores</code> is <code class="literal">false</code>, you must specify which stores receive the image data by using <code class="literal">--stores STORE1,STORE2</code>. If the image data is already present in any of the specified stores, the request fails.
					</li><li class="listitem">
						If you set <code class="literal">all-stores</code> to <code class="literal">true</code>, and the image data already exists in some of the stores, then those stores are excluded from the list.
					</li></ul></div><p>
				After you specify which stores receive the image data, the Image service copies data from the central site to a staging area. Then the Image service imports the image data by using the interoperable image import workflow. For more information, see <a class="link" href="index.html#import-an-image-to-multiple-stores" title="2.2. Import an image to multiple stores">Importing an image to multiple stores</a>.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Red Hat recommends that administrators carefully avoid closely timed image copy requests. Two closely timed copy-image operations for the same image causes race conditions and unexpected results. Existing image data remains as it is, but copying data to new stores fails.
				</p></div></div><section class="section" id="copying-an-image-to-all-stores"><div class="titlepage"><div><div><h3 class="title">2.3.1. Copying an image to all stores</h3></div></div></div><p>
					Use the following procedure to copy image data to all available stores.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy image data to all available stores:
						</p><pre class="screen">$ glance image-import <span class="emphasis"><em>IMAGE-ID</em></span> \
--all-stores true \
--import-method copy-image</pre><p class="simpara">
							Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the name of the image you want to copy.
						</p></li><li class="listitem"><p class="simpara">
							Confirm that the image data successfully replicated to all available stores:
						</p><pre class="screen">$ glance image-list --include-stores</pre><p class="simpara">
							For information about how to check the status of the image import operation, see <a class="link" href="index.html#checking-progress-of-image-import-operation" title="2.2.5. Checking the progress of the image import operation">Checking the progress of the image import operation</a>.
						</p></li></ol></div></section><section class="section" id="copying-an-image-to-specific-stores"><div class="titlepage"><div><div><h3 class="title">2.3.2. Copying an image to specific stores</h3></div></div></div><p>
					Use the following procedure to copy image data to specific stores.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy image data to specific stores:
						</p><pre class="screen">$ glance image-import <span class="emphasis"><em>IMAGE-ID</em></span> \
--stores <span class="emphasis"><em>STORE1</em></span>,<span class="emphasis"><em>STORE2</em></span> \
--import-method copy-image</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <span class="emphasis"><em>IMAGE-ID</em></span> with the name of the image you want to copy.
								</li><li class="listitem">
									Replace <span class="emphasis"><em>STORE1</em></span> and <span class="emphasis"><em>STORE2</em></span> with the names of the stores to which you want to copy the image data.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Confirm that the image data successfully replicated to the specified stores:
						</p><pre class="screen">$ glance image-list --include-stores</pre><p class="simpara">
							For information about how to check the status of the image import operation, see <a class="link" href="index.html#checking-progress-of-image-import-operation" title="2.2.5. Checking the progress of the image import operation">Checking the progress of the image import operation</a>.
						</p></li></ol></div></section></section><section class="section" id="deleting-an-image-from-specific-store"><div class="titlepage"><div><div><h2 class="title">2.4. Deleting an image from a specific store</h2></div></div></div><p>
				This feature enables you to delete an existing image copy on a specific store using Red Hat OpenStack Image service (glance).
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
				</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Delete an image from a specific store:
				</p></div><pre class="screen">$ glance stores-delete --store _STORE_ID_ _IMAGE_ID_</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Replace _STORE_ID with the name of the store on which the image copy should be deleted.
					</li><li class="listitem">
						Replace <span class="emphasis"><em>IMAGE_ID</em></span> with the ID of the image you want to delete.
					</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					Using <code class="literal">glance image-delete</code> will permanently delete the image across all the sites. All image copies will be deleted, as well as the image instance and metadata.
				</p></div></div></section><section class="section" id="understanding-locations-of-images"><div class="titlepage"><div><div><h2 class="title">2.5. Understanding the locations of images</h2></div></div></div><p>
				Although an image can be present on multiple sites, there is only a single UUID for a given image. The image metadata contains the locations of each copy. For example, an image present on two edge sites is exposed as a single UUID with three locations: the central site plus the two edge sites.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Show the sites on which a copy of the image exists:
					</p><pre class="screen">$ glance image-show ID | grep "stores"

| stores |  default_backend,dcn1,dcn2</pre><p class="simpara">
						In the example, the image is present on the central site, the <code class="literal">default_backend</code>, and on the two edge sites <code class="literal">dcn1</code> and <code class="literal">dcn2</code>.
					</p></li><li class="listitem"><p class="simpara">
						Alternatively, you can run the <code class="literal">glance image-list</code> command with the <code class="literal">--include-stores</code> option to see the sites where the images exist:
					</p><pre class="screen">$ glance image-list --include-stores

| ID                                   | Name    | Stores

| 2bd882e7-1da0-4078-97fe-f1bb81f61b00 | cirros | default_backend,dcn1,dcn2</pre></li><li class="listitem"><p class="simpara">
						List the image locations properties to show the details of each location:
					</p><pre class="screen">$ openstack image show ID -c properties

| properties |

(--- cut ---)
locations='[{'url': 'rbd://79b70c32-df46-4741-93c0-8118ae2ae284/images/2bd882e7-1da0-4078-97fe-f1bb81f61b00/snap', 'metadata': {'store': 'default_backend'}}, {'url': 'rbd://63df2767-8ddb-4e06-8186-8c155334f487/images/2bd882e7-1da0-4078-97fe-f1bb81f61b00/snap', 'metadata': {'store': 'dcn1'}}, {'url': 'rbd://1b324138-2ef9-4ef9-bd9e-aa7e6d6ead78/images/2bd882e7-1da0-4078-97fe-f1bb81f61b00/snap', 'metadata': {'store': 'dcn2'}}]',
(--- cut --)</pre><p class="simpara">
						The image properties show the different Ceph RBD URIs for the location of each image.
					</p><p class="simpara">
						In the example, the central image location URI is:
					</p><pre class="screen">rbd://79b70c32-df46-4741-93c0-8118ae2ae284/images/2bd882e7-1da0-4078-97fe-f1bb81f61b00/snap', 'metadata': {'store': 'default_backend'}}</pre><p class="simpara">
						The URI is composed of the following data:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">79b70c32-df46-4741-93c0-8118ae2ae284</code> corresponds to the central Ceph FSID. Each Ceph cluster has a unique FSID.
							</li><li class="listitem">
								The default value for all sites is <code class="literal">images</code>, which corresponds to the Ceph pool on which the images are stored.
							</li><li class="listitem">
								<code class="literal">2bd882e7-1da0-4078-97fe-f1bb81f61b00</code> corresponds to the image UUID. The UUID is the same for a given image regardless of its location.
							</li><li class="listitem">
								The metadata shows the glance store to which this location maps. In this example, it maps to the <code class="literal">default_backend</code>, which is the central hub site.
							</li></ul></div></li></ol></div></section></section><section class="chapter" id="ch-configuring-compute"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Configuring the Compute (nova) service</h1></div></div></div><p>
			Use environment files to customize the Compute (nova) service. Puppet generates and stores this configuration in the <code class="literal">/var/lib/config-data/puppet-generated/&lt;nova_container&gt;/etc/nova/nova.conf</code> file. Use the following configuration methods to customize the Compute service configuration:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Heat parameters</strong></span> - as detailed in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/overcloud_parameters/compute-nova-parameters">Compute (nova) Parameters</a> section in the <span class="emphasis"><em>Overcloud Parameters</em></span> guide. For example:
				</p><pre class="screen">parameter_defaults:
  NovaSchedulerDefaultFilters: AggregateInstanceExtraSpecsFilter,RetryFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter
  NovaNfsEnabled: true
  NovaNfsShare: '192.0.2.254:/export/nova'
  NovaNfsOptions: 'context=system_u:object_r:nfs_t:s0'
  NovaNfsVersion: '4.2'</pre></li><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Puppet parameters</strong></span> - as defined in <code class="literal">/etc/puppet/modules/nova/manifests/*</code>:
				</p><pre class="screen">parameter_defaults:
    ComputeExtraConfig:
        nova::compute::force_raw_images: True</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Only use this method if an equivalent heat parameter does not exist.
					</p></div></div></li><li class="listitem"><p class="simpara">
					<span class="strong strong"><strong>Manual hieradata overrides</strong></span> - for customizing parameters when no heat or Puppet parameter exists. For example, the following sets the <code class="literal">timeout_nbd</code> in the <code class="literal">[DEFAULT]</code> section on the Compute role:
				</p><pre class="screen">parameter_defaults:
    ComputeExtraConfig:
        nova::config::nova_config:
            DEFAULT/timeout_nbd:
                value: '20'</pre></li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
				If a heat parameter exists, it must be used instead of the Puppet parameter; if a Puppet parameter exists, but not a heat parameter, then the Puppet parameter must be used instead of the manual override method. The manual override method must only be used if there is no equivalent heat or Puppet parameter.
			</p></div></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
			Follow the guidance in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-configuring_base_parameters#identifying_parameters_to_modify">Identifying Parameters to Modify</a> to determine if a heat or Puppet parameter is available for customizing a particular configuration.
		</p></div></div><p>
			See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-configuring_base_parameters">Parameters</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide for further details on configuring overcloud services.
		</p><section class="section" id="configuring-memory"><div class="titlepage"><div><div><h2 class="title">3.1. Configuring memory for overallocation</h2></div></div></div><p>
				When you use memory overcommit (<code class="literal">NovaRAMAllocationRatio</code> &gt;= 1.0), you need to deploy your overcloud with enough swap space to support the allocation ratio.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If your <code class="literal">NovaRAMAllocationRatio</code> parameter is set to <code class="literal">&lt; 1</code>, follow the RHEL recommendations for swap size. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/getting-started-with-swap_managing-storage-devices#recommended-system-swap-space_getting-started-with-swap">Recommended system swap space</a> in the RHEL <span class="emphasis"><em>Managing Storage Devices</em></span> guide.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have calculated the swap size your node requires. For more information, see <a class="xref" href="index.html#calculating-swap" title="3.3. Calculating swap size">Section 3.3, “Calculating swap size”</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Copy the <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/enable-swap.yaml</code> file to your environment file directory:
					</p><pre class="screen">$ cp /usr/share/openstack-tripleo-heat-templates/environments/enable-swap.yaml /home/stack/templates/enable-swap.yaml</pre></li><li class="listitem"><p class="simpara">
						Configure the swap size by adding the following parameters to your <code class="literal">enable-swap.yaml</code> file:
					</p><pre class="screen">parameter_defaults:
  swap_size_megabytes: &lt;swap size in MB&gt;
  swap_path: &lt;full path to location of swap, default: /swap&gt;</pre></li><li class="listitem"><p class="simpara">
						To apply this configuration, add the <code class="literal">enable_swap.yaml</code> environment file to the stack with your other environment files and deploy the overcloud:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files] \
  -e /home/stack/templates/enable-swap.yaml \</pre></li></ol></div></section><section class="section" id="calculating-reserved-host-memory"><div class="titlepage"><div><div><h2 class="title">3.2. Calculating reserved host memory on Compute nodes</h2></div></div></div><p>
				To determine the total amount of RAM to reserve for host processes, you need to allocate enough memory for each of the following:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The resources that run on the node, for instance, OSD consumes 3 GB of memory.
					</li><li class="listitem">
						The emulator overhead required to visualize instances on a host.
					</li><li class="listitem">
						The hypervisor for each instance.
					</li></ul></div><p>
				After you calculate the additional demands on memory, use the following formula to help you determine the amount of memory to reserve for host processes on each node:
			</p><pre class="screen">NovaReservedHostMemory = total_RAM - ( (vm_no * (avg_instance_size + overhead)) + (resource1 * resource_ram) + (resource _n_ * resource_ram))</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Replace <code class="literal">vm_no</code> with the number of instances.
					</li><li class="listitem">
						Replace <code class="literal">avg_instance_size</code> with the average amount of memory each instance can use.
					</li><li class="listitem">
						Replace <code class="literal">overhead</code> with the hypervisor overhead required for each instance.
					</li><li class="listitem">
						Replace <code class="literal">resource1</code> with the number of a resource type on the node.
					</li><li class="listitem">
						Replace <code class="literal">resource_ram</code> with the amount of RAM each resource of this type requires.
					</li></ul></div></section><section class="section" id="calculating-swap"><div class="titlepage"><div><div><h2 class="title">3.3. Calculating swap size</h2></div></div></div><p>
				The allocated swap size must be large enough to handle any memory overcommit. You can use the following formulas to calculate the swap size your node requires:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						overcommit_ratio = <code class="literal">NovaRAMAllocationRatio</code> - 1
					</li><li class="listitem">
						Minimum swap size (MB) = <code class="literal">(total_RAM * overcommit_ratio) + RHEL_min_swap</code>
					</li><li class="listitem">
						Recommended (maximum) swap size (MB) = <code class="literal">total_RAM * (overcommit_ratio + percentage_of_RAM_to_use_for_swap)</code>
					</li></ul></div><p>
				The <code class="literal">percentage_of_RAM_to_use_for_swap</code> variable creates a buffer to account for QEMU overhead and any other resources consumed by the operating system or host services.
			</p><p>
				For instance, to use 25% of the available RAM for swap, with 64GB total RAM, and <code class="literal">NovaRAMAllocationRatio</code> set to <code class="literal">1</code>:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Recommended (maximum) swap size = 64000 MB * (0 + 0.25) = 16000 MB
					</li></ul></div><p>
				For information on how to calculate the <code class="literal">NovaReservedHostMemory</code> value, see <a class="xref" href="index.html#calculating-reserved-host-memory" title="3.2. Calculating reserved host memory on Compute nodes">Section 3.2, “Calculating reserved host memory on Compute nodes”</a>.
			</p><p>
				For information on how to determine the <code class="literal">RHEL_min_swap</code> value, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/getting-started-with-swap_managing-storage-devices#recommended-system-swap-space_getting-started-with-swap">Recommended system swap space</a> in the RHEL <span class="emphasis"><em>Managing Storage Devices</em></span> guide.
			</p></section></section><section class="chapter" id="ch-manage_cells"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Scaling deployments with Compute cells</h1></div></div></div><p>
			You can use cells to divide Compute nodes in large deployments into groups, each with a message queue and dedicated database that contains instance information.
		</p><p>
			By default, the director installs the overcloud with a single cell for all Compute nodes. This single-cell deployment contains all instances and instance metadata. For larger deployments, you can deploy the overcloud with multiple cells to accommodate a larger number of Compute nodes.
		</p><p>
			In multi-cell deployments, each cell runs standalone copies of the cell-specific components and stores instance metadata only for instances in that cell. Global information and cell mappings are stored in the global Controller cell, which helps with security and recovery in case one of the cells fails.
		</p><p>
			You can add cells to your environment when you install a new overcloud or at any time afterwards.
		</p><section class="section" id="concept_cell-components"><div class="titlepage"><div><div><h2 class="title">4.1. Cell components</h2></div></div></div><p>
				In single-cell deployments, all components are contained in the same cell. In multi-cell deployments, the global services run on the main Controller cell, and each Compute cell runs standalone copies of the cell-specific components and contains the database and message queue for the Compute nodes in that cell.
			</p><div class="formalpara"><p class="title"><strong>Global components</strong></p><p>
					The following components are deployed in a Controller cell once for each overcloud, regardless of the number of Compute cells.
				</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Compute API</span></dt><dd>
							Provides the external REST API to users.
						</dd><dt><span class="term">Scheduler</span></dt><dd>
							Determines to which Compute node to assign the instances.
						</dd><dt><span class="term">Placement service</span></dt><dd>
							Monitors and allocates Compute resources to the instances.
						</dd><dt><span class="term">API database</span></dt><dd><p class="simpara">
							Used by the Compute API and the Compute scheduler services to track location information about instances, and provides a temporary location for instances that are built but not scheduled.
						</p><p class="simpara">
							In multi-cell deployments, this database also contains <span class="emphasis"><em>cell mappings</em></span> that specify the database connection for each cell.
						</p></dd><dt><span class="term"><code class="literal">cell0</code> database</span></dt><dd>
							Dedicated database for information about instances that failed to be scheduled.
						</dd><dt><span class="term">Super conductor</span></dt><dd><p class="simpara">
							In multi-cell deployments, this service coordinates between the global services and each Compute cell, and also sends failed instance information to the <code class="literal">cell0</code> database.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								This component exists only in multi-cell deployments.
							</p></div></div></dd></dl></div><div class="formalpara"><p class="title"><strong>Cell-specific components</strong></p><p>
					The following components are deployed in each Compute cell.
				</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Cell database</span></dt><dd>
							Contains most of the information about instances. Used by the global API, the conductor, and the Compute services.
						</dd><dt><span class="term">Conductor</span></dt><dd>
							Coordinates database queries and long-running tasks from the global services, and insulates Compute nodes from direct database access.
						</dd><dt><span class="term">Message queue</span></dt><dd>
							Messaging service used by all services to communicate with each other within the cell and with the global services.
						</dd></dl></div><div class="formalpara"><p class="title"><strong>Configuration files</strong></p><p>
					The overcloud includes configuration files that define the following information for the Compute cells:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">[DEFAULT]/transport_url</code>: Message queue endpoint for each cell.
					</li><li class="listitem">
						<code class="literal">[DATABASE]/connection</code>: Database connection for each cell.
					</li><li class="listitem">
						<code class="literal">[API_DATABASE]/connection</code>: Routing and placement information for the global components.
					</li><li class="listitem">
						(Multi-cell deployments only) Cell mapping records to be stored in the global API database.
					</li></ul></div><p>
				This information is extracted from the overcloud when you deploy the multi-cell environment, as described in <a class="xref" href="index.html#proc_multi-cell-deploy" title="4.4. Deploying a multi-cell overcloud">Section 4.4, “Deploying a multi-cell overcloud”</a>.
			</p></section><section class="section" id="concept_cell-deployments-arch"><div class="titlepage"><div><div><h2 class="title">4.2. Cell deployments architecture</h2></div></div></div><p>
				Each deployment type allows you to optimize your overcloud for different use-cases.
			</p><div class="formalpara"><p class="title"><strong>Single-cell deployment architecture (default)</strong></p><p>
					The following diagram shows an example of the basic structure and interaction in a default single-cell overcloud.
				</p></div><p>
				<span class="inlinemediaobject"><img src="single-cell-deployment.png" alt="Single-cell deployment architecture"/></span>

			</p><p>
				In this deployment, all services are configured to use a single conductor to communicate between the Compute API and the Compute nodes, and a single database stores all live instance data.
			</p><p>
				In smaller deployments this configuration might be sufficient, but if any API-level (global) service or the database fails, the entire Compute deployment cannot send or receive information, regardless of high availability configurations.
			</p><div class="formalpara"><p class="title"><strong>Multi-cell deployment architecture (custom)</strong></p><p>
					The following diagram shows an example of the basic structure and interaction in a custom multi-cell overcloud.
				</p></div><p>
				<span class="inlinemediaobject"><img src="multi-cell-deployment.png" alt="Multi-cell deployment architecture"/></span>

			</p><p>
				In this deployment, the Compute nodes are divided to multiple cells, each with their own conductor, database, and message queue. The global services use the super conductor to communicate with each cell, and the global database contains only information required for the whole overcloud.
			</p><p>
				The cell-level services cannot access global services directly. This isolation provides additional security and fail-safe capabilities in case of cell failure.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					In Edge deployments, you must deploy the first cell on the central site, therefore, do not deploy the first cell on any of the edge sites. Do not run any Compute services on the first cell. Instead, deploy each new cell containing the Compute nodes separately on the edge sites.
				</p></div></div></section><section class="section" id="concept_multi-cell-considerations"><div class="titlepage"><div><div><h2 class="title">4.3. Considerations for multi-cell deployments</h2></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Maximum number of Compute nodes in a multi-cell deployment</span></dt><dd>
							The maximum number of Compute nodes is 500 across all cells.
						</dd><dt><span class="term">SSL/TLS</span></dt><dd>
							You cannot enable SSL/TLS on the overcloud.
						</dd><dt><span class="term">Cross-cell instance migrations</span></dt><dd><p class="simpara">
							Migrating an instance from a host in one cell to a host in another cell is not supported. This limitation affects the following operations:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									cold migration
								</li><li class="listitem">
									live migration
								</li><li class="listitem">
									unshelve
								</li><li class="listitem">
									resize
								</li><li class="listitem">
									evacuation
								</li></ul></div></dd><dt><span class="term">Service quotas</span></dt><dd><p class="simpara">
							Compute service quotas are calculated dynamically at each resource consumption point, instead of statically in the database. In multi-cell deployments, unreachable cells cannot provide usage information in real-time, which might cause the quotas to be exceeded when the cell is reachable again.
						</p><p class="simpara">
							You can use the Placement service and API database to configure the quota calculation to withstand failed or unreachable cells.
						</p></dd><dt><span class="term">API database</span></dt><dd>
							The Compute API database is always global for all cells and cannot be duplicated for each cell.
						</dd><dt><span class="term">Console proxies</span></dt><dd>
							You must configure console proxies for each cell, because console token authorizations are stored in cell databases. Each console proxy server needs to access the <code class="literal">database.connection</code> information of the corresponding cell database.
						</dd><dt><span class="term">Template URLs in cell mappings</span></dt><dd><p class="simpara">
							You can create templates for the <code class="literal">--database_connection</code> and <code class="literal">--transport-url</code> in cell mappings with variables that are dynamically updated each time you query the global database. The values are taken from the configuration files of the Compute nodes.
						</p><p class="simpara">
							The format of a template URL is as follows:
						</p><pre class="screen">{scheme}://{username}:{password}@{hostname}/{path}</pre><p class="simpara">
							The following table shows the variables that you can use in cell mapping URLs:
						</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 28%; " class="col_1"><!--Empty--></col><col style="width: 72%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th style="text-align: left; vertical-align: top; " id="idm139747268954144" scope="col">Variable</th><th style="text-align: left; vertical-align: top; " id="idm139747268953056" scope="col">Description</th></tr></thead><tbody><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											scheme
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Prefix before <code class="literal">://</code>
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											username
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											User name
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											password
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Password
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											hostname
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Host name or IP address
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											port
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Port number (must be specified)
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											path
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Path to the directory in the host (without leading slash)
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											query
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Full query with string arguments (without leading question mark)
										</p>
										 </td></tr><tr><td style="text-align: left; vertical-align: top; " headers="idm139747268954144"> <p>
											fragment
										</p>
										 </td><td style="text-align: left; vertical-align: top; " headers="idm139747268953056"> <p>
											Path after the first hash <code class="literal">#</code> sign
										</p>
										 </td></tr></tbody></table></div></dd><dt><span class="term">Compute metadata API</span></dt><dd><p class="simpara">
							You can run the Compute metadata API globally or in each cell. Choose one of the following:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you have networks that cover multiple cells, you need to run the metadata API globally so that it can bridge between the cells. In this case, the metadata API needs to access the <code class="literal">api_database.connection</code> information.
								</li><li class="listitem">
									If you have networks in separate segments for each cell, you can run the metadata API separately in each cell. This configuration can improve performance and data isolation. In this case, <code class="literal">neutron-metadata-agent</code> service point to the corresponding <code class="literal">nova-api-metadata</code> service.
								</li></ul></div><p class="simpara">
							You use the <code class="literal">api.local_metadata_per_cell</code> configuration option to set which method to implement. For details on configuring this option, see the <span class="emphasis"><em>Create environment files with cell parameters</em></span> section in <a class="xref" href="index.html#proc_multi-cell-deploy" title="4.4. Deploying a multi-cell overcloud">Section 4.4, “Deploying a multi-cell overcloud”</a>.
						</p></dd></dl></div></section><section class="section" id="proc_multi-cell-deploy"><div class="titlepage"><div><div><h2 class="title">4.4. Deploying a multi-cell overcloud</h2></div></div></div><p>
				Deploying a multi-cell overcloud includes the following stages:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Extracting parameter information from the default first cell in the basic overcloud. This cell becomes the global Controller after you redeploy the overcloud.
					</li><li class="listitem">
						Configuring a custom role and flavor for the cell.
					</li><li class="listitem">
						Creating an environment file with cell-specific parameters.
					</li><li class="listitem">
						Redeploying the overcloud with the new cell stack.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							This process adds one cell to the overcloud. Repeat these steps for each additional cell you want to deploy in the overcloud.
						</li><li class="listitem">
							In this procedure, the name of the new cell is <code class="literal">cell1</code>. Replace the name in all commands with the actual cell name.
						</li></ul></div></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Deploy a basic overcloud with the required number of Controller and Compute nodes.
					</li><li class="listitem">
						Review the requirements and limitations for a multi-cell overcloud as described in <a class="xref" href="index.html#concept_multi-cell-considerations" title="4.3. Considerations for multi-cell deployments">Section 4.3, “Considerations for multi-cell deployments”</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Extract parameter information from the overcloud</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a new directory for the new cell and export the contents to the new directory. For example:
					</p><pre class="screen">$ source ~/stackrc
(undercloud) $ mkdir cell1
(undercloud) $ export DIR=cell1</pre></li><li class="listitem"><p class="simpara">
						Export the <code class="literal">EndpointMap</code>, <code class="literal">HostsEntry</code>, <code class="literal">AllNodesConfig</code>, <code class="literal">GlobalConfig</code> parameters, and the password information from the overcloud to a new environment file for the cell. For example:
					</p><pre class="screen">(undercloud) $ openstack overcloud cell export cell1 -o cell1/cell1-ctrl-input.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the environment file already exists, run the command with the <code class="literal">--force-overwrite</code> or <code class="literal">-f</code> option.
						</p></div></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Configure a custom role for a cell</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Add the <code class="literal">CellController</code> role to your roles data file and regenerate the file. For example:
					</p><pre class="screen">(undercloud) $ openstack overcloud roles generate --roles-path \
               /usr/share/openstack-tripleo-heat-templates/roles \
               -o $DIR/cell_roles_data.yaml Compute CellController</pre><p class="simpara">
						The <code class="literal">CellController</code> custom role includes the services from the default <code class="literal">Compute</code> role and additional configuration for the following services:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Galera database
							</li><li class="listitem">
								RabbitMQ
							</li><li class="listitem">
								<code class="literal">nova-conductor</code>
							</li><li class="listitem">
								<code class="literal">nova novnc proxy</code>
							</li><li class="listitem">
								<code class="literal">nova metadata</code> (only in case you set the <code class="literal">NovaLocalMetadataPerCell</code> parameter)
							</li></ul></div></li><li class="listitem"><p class="simpara">
						In case you want to divide your network between the global Controller and the cells, configure network access in the roles file that you created. For example:
					</p><pre class="screen">name: Compute
 description: |
   Basic Compute Node role
 CountDefault: 1
 # Create external Neutron bridge (unset if using ML2/OVS without DVR)
 tags:
   - external_bridge
 networks:
   InternalApi:
     subnet: internal_api_cell1
   Tenant:
     subnet: tenant_subnet
   Storage:
     subnet: storage_cell1
...
- name: CellController
   description: |
     CellController role for the nova cell_v2 controller services
   CountDefault: 1
   tags:
     - primary
     - controller
   networks:
     External:
       subnet: external_cell1
     InternalApi:
       subnet: internal_api_cell1
     Storage:
       subnet: storage_cell1
     StorageMgmt:
       subnet: storage_mgmt_cell1
     Tenant:
       subnet: tenant_subnet</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Configure a flavor and tag nodes to a cell</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the <code class="literal">cellcontroller</code> flavor to tag nodes that you want to allocate to the cell. For example:
					</p><pre class="screen">(undercloud) $ openstack flavor create --id auto --ram 4096 --disk 40 --vcpus 1 cellcontroller
(undercloud) $ openstack flavor set --property "cpu_arch"="x86_64" \
                --property "capabilities:boot_option"="local" \
                --property "capabilities:profile"="cellcontroller" \
                --property "resources:CUSTOM_BAREMETAL=1" \
                --property "resources:DISK_GB=0" \
                --property "resources:MEMORY_MB=0" \
                --property "resources:VCPU=0" \
               cellcontroller</pre></li><li class="listitem"><p class="simpara">
						Tag each node that you want to assign to the cell with the <code class="literal">cellcontroller</code> profile.
					</p><pre class="screen">(undercloud) $ openstack baremetal node set --property \
               capabilities='profile:cellcontroller,boot_option:local' &lt;NODE_UUID&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;NODE_UUID&gt;</code> with the actual ID of the Compute node that you want to assign to the cell.
					</p></li></ol></div><div class="orderedlist"><p class="title"><strong>Create environment files with cell parameters</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create a new environment file in the directory for the cell, such as <span class="strong strong"><strong><span class="emphasis"><em>/cell1/cell1.yaml</em></span></strong></span>, and add the following parameters:
					</p><pre class="screen">resource_registry:
  # since the same networks are used in this example, the
  # creation of the different networks is omitted
  OS::TripleO::Network::External: OS::Heat::None
  OS::TripleO::Network::InternalApi: OS::Heat::None
  OS::TripleO::Network::Storage: OS::Heat::None
  OS::TripleO::Network::StorageMgmt: OS::Heat::None
  OS::TripleO::Network::Tenant: OS::Heat::None
  OS::TripleO::Network::Management: OS::Heat::None
  OS::TripleO::Network::Ports::OVNDBsVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/noop.yaml
  OS::TripleO::Network::Ports::RedisVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/noop.yaml

parameter_defaults:
  # CELL Parameter to reflect that this is an additional CELL
  NovaAdditionalCell: True

  # mapping of the CellController flavor to the CellController role
  CellControllerFlavor: cellcontroller

  # The DNS names for the VIPs for the cell
  CloudName: cell1.ooo.test
  CloudNameInternal: cell1.internalapi.ooo.test
  CloudNameStorage: cell1.storage.ooo.test
  CloudNameStorageManagement: cell1.storagemgmt.ooo.test
  CloudNameCtlplane: cell1.ctlplane.ooo.test

  # Flavors used for the cell controller and computes
  OvercloudCellControllerFlavor: cellcontroller
  OvercloudComputeFlavor: compute

  # Number of controllers/computes in the cell
  CellControllerCount: 3
  ComputeCount: 1

  # Compute node name (must be unique)
  ComputeHostnameFormat: 'cell1-compute-%index%'

  # default gateway
  ControlPlaneStaticRoutes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.24.1
      default: true
  DnsServers:
    - x.x.x.x</pre><p class="simpara">
						Change the parameter values in this example according to your deployment needs.
					</p></li><li class="listitem"><p class="simpara">
						Depending on your network configuration, you might need to allocate a network resource to the cell. Add the following parameter if you need to register cells to the network:
					</p><pre class="screen">resource_registry:
  OS::TripleO::CellController::Net::SoftwareConfig: single-nic-vlans/controller.yaml
  OS::TripleO::Compute::Net::SoftwareConfig: single-nic-vlans/compute.yaml</pre></li><li class="listitem"><p class="simpara">
						If you divide your network between the global Controller and the cells and want to run the Compute metadata API in each cell instead of in the global Controller, add the following parameter:
					</p><pre class="screen">parameter_defaults:
   NovaLocalMetadataPerCell: True</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The parameters in this file restrict the overcloud to use a single network for all cells.
								</li><li class="listitem">
									The Compute host names must be unique across all cells.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Copy the <span class="strong strong"><strong><span class="emphasis"><em>network_data.yaml</em></span></strong></span> file and name it according to the cell name. For example:
					</p><pre class="screen">(undercloud) $ cp /usr/share/openstack-tripleo-heat-templates/network_data.yaml cell1/network_data-ctrl.yaml</pre></li><li class="listitem"><p class="simpara">
						Add the UUIDs for the network components you want to reuse for the cells to the new network data file.
					</p><pre class="screen">external_resource_network_id: [EXISTING_NETWORK_UUID]
external_resource_subnet_id: [EXISTING_SUBNET_UUID]
external_resource_segment_id: [EXISTING_SEGMENT_UUID]
external_resource_vip_id: [EXISTING_VIP_UUID]</pre></li></ol></div><div class="formalpara"><p class="title"><strong>(Optional) Configure networking for segmented networks</strong></p><p>
					If you want to divide your network between the global Controller and the Compute cells, create an environment file such as <span class="strong strong"><strong><span class="emphasis"><em><span class="CELL-NAME CELL-NAME">routes.yaml</span></em></span></strong></span> and add the routing information and virtual IP address (VIP) information for the cell. For example:
				</p></div><pre class="screen">parameter_defaults:
  InternalApiInterfaceRoutes:
    - destination: 172.17.2.0/24
      nexthop: 172.16.2.254
  StorageInterfaceRoutes:
    - destination: 172.17.1.0/24
      nexthop: 172.16.1.254
  StorageMgmtInterfaceRoutes:
    - destination: 172.17.3.0/24
      nexthop: 172.16.3.254

parameter_defaults:
  VipSubnetMap:
    InternalApi: internal_api_cell1
    Storage: storage_cell1
    StorageMgmt: storage_mgmt_cell1
    External: external_cell1</pre><div class="formalpara"><p class="title"><strong>(Optional) Configure networking for Edge sites</strong></p><p>
					To distribute Compute nodes across Edge sites, create one environment file for the main Controller cell and separate environment files for each Compute cell in that Edge site.
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						In the primary environment file, set the <span class="strong strong"><strong>ComputeCount</strong></span> parameter to <code class="literal">0</code> in the Controller cell. This cell is separate from the Edge site Compute cells, which will contain the actual Compute nodes.
					</li><li class="listitem"><p class="simpara">
						In the Compute cell environment files, add the following parameter to disable external VIP ports:
					</p><pre class="screen">resource_registry:
  # Since the compute stack deploys only compute nodes ExternalVIPPorts are not required.
  OS::TripleO::Network::Ports::ExternalVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/noop.yaml</pre></li></ul></div><div class="formalpara"><p class="title"><strong>Deploy the overcloud</strong></p><p>
					Choose one of the following:
				</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Multi-cell deployment with a single network</span></dt><dd><p class="simpara">
							Run the <code class="literal">overcloud deploy</code> command and add the environment files that you created to configure the new cell stack. For example:
						</p><pre class="screen">$ openstack overcloud deploy \
  --templates /usr/share/openstack-tripleo-heat-templates \
  --stack cell1 \
  -r $HOME/$DIR/cell_roles_data.yaml \
  -e $HOME/$DIR/cell1-ctrl_input.yaml \
  -e $HOME/$DIR/cell1.yaml</pre></dd><dt><span class="term">Multi-cell deployment with segmented networks</span></dt><dd><p class="simpara">
							Run the <code class="literal">overcloud deploy</code> command with the additional network data environment file that you created in the previous steps.
						</p><p class="simpara">
							The following example shows the <code class="literal">overcloud deploy</code> command with the environment files that you created to designate a network segment for the cell. Edit the command according to the actual number and names of the cells that you want to deploy.
						</p><pre class="screen">openstack overcloud deploy \
  --templates /usr/share/openstack-tripleo-heat-templates \
  --stack cell1-ctrl \
  -r $HOME/$DIR/cell_roles_data.yaml \
  -n $HOME/$DIR/cell1_routes.yaml \
  -n $HOME/$DIR/network_data-ctrl.yaml \
  -e $HOME/$DIR/cell1-ctrl-input.yaml \
  -e $HOME/$DIR/cell1.yaml</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you deploy Compute cells in Edge sites, run the <code class="literal">overcloud deploy</code> command in each site with the environment files and configuration for each Compute cell in that site.
							</p></div></div></dd></dl></div></section><section class="section" id="proc_create-launch-cell"><div class="titlepage"><div><div><h2 class="title">4.5. Creating and provisioning a cell</h2></div></div></div><p>
				After you deploy the overcloud with a new cell stack as described in <a class="xref" href="index.html#proc_multi-cell-deploy" title="4.4. Deploying a multi-cell overcloud">Section 4.4, “Deploying a multi-cell overcloud”</a>, you create and provision the Compute cell.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This process must be repeated for each cell that you create and launch. You can automate the steps in an Ansible playbook. For an example of an Ansible playbook, see the <a class="link" href="https://docs.openstack.org/project-deploy-guide/tripleo-docs/latest/features/deploy_cellv2_basic.html#cell-create-cell">Create the cell and discover Compute nodes</a> section of the OpenStack community documentation. Community documentation is provided as-is and is not officially supported.
				</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Get the IP addresses of the control plane and cell controller.
					</p><pre class="screen">$ CTRL_IP=$(openstack server list -f value -c Networks --name overcloud-controller-0 | sed 's/ctlplane=//')
$ CELL_CTRL_IP=$(openstack server list -f value -c Networks --name cellcontroller-0 | sed 's/ctlplane=//')</pre></li><li class="listitem"><p class="simpara">
						Add the cell information to all Controller nodes. This information is used to connect to the cell endpoint from the undercloud.
					</p><pre class="screen">(undercloud) [stack@undercloud ~]$ CELL_INTERNALAPI_INFO=$(ssh heat-admin@${CELL_CTRL_IP} egrep \
                                    cellcontrol.*\.internalapi /etc/hosts)
(undercloud) [stack@undercloud ~]$ ansible -i /usr/bin/tripleo-ansible-inventory Controller -b \
                                    -m lineinfile -a "dest=/etc/hosts line=\"$CELL_INTERNALAPI_INFO\""</pre></li><li class="listitem"><p class="simpara">
						Get the <code class="literal">transport_url</code> and <code class="literal">database.connection</code> endpoint information from the controller cell.
					</p><pre class="screen">(undercloud) [stack@undercloud ~]$ CELL_TRANSPORT_URL=$(ssh heat-admin@${CELL_CTRL_IP} sudo \
                                    crudini --get /var/lib/config-data/nova/etc/nova/nova.conf DEFAULT transport_url)
(undercloud) [stack@undercloud ~]$ CELL_MYSQL_VIP=$(ssh heat-admin@${CELL_CTRL_IP} sudo \
                                    crudini --get /var/lib/config-data/nova/etc/nova/nova.conf database connection \
                                    | perl -nle'/(\d+\.\d+\.\d+\.\d+)/ &amp;&amp; print $1')</pre></li><li class="listitem"><p class="simpara">
						Log in to one of the global Controller nodes to create the cell based on the information that you retrieved in the previous steps. For example:
					</p><pre class="screen">$ export CONTAINERCLI='podman'

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 create_cell --name computecell1 \
  --database_connection "{scheme}://{username}:{password}@$CELL_MYSQL_VIP/nova?{query}" \
  --transport-url "$CELL_TRANSPORT_URL"</pre></li><li class="listitem"><p class="simpara">
						Check that the cell is created and appears in the cell list.
					</p><pre class="screen">$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
nova-manage cell_v2 list_cells --verbose</pre></li><li class="listitem"><p class="simpara">
						Restart the Compute services on the Controller nodes.
					</p><pre class="screen">$ ansible -i /usr/bin/tripleo-ansible-inventory Controller -b -a \
"systemctl restart tripleo_nova_api tripleo_nova_conductor tripleo_nova_scheduler"</pre></li><li class="listitem"><p class="simpara">
						Check that the cell controller services are provisioned.
					</p><pre class="screen">(overcloud) [stack@undercloud ~]$ nova service-list</pre></li></ol></div></section><section class="section" id="proc_multi-cell-add-compute-node"><div class="titlepage"><div><div><h2 class="title">4.6. Adding Compute nodes to a cell</h2></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Log into one of the Controller nodes.
					</li><li class="listitem"><p class="simpara">
						Get the IP address of the control plane for the cell and run the host discovery command to expose and assign Compute hosts to the cell.
					</p><pre class="screen">$ CTRL=overcloud-controller-0
$ CTRL_IP=$(openstack server list -f value -c Networks --name $CTRL | sed 's/ctlplane=//')

$ export CONTAINERCLI='podman'

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 discover_hosts --by-service --verbose</pre></li><li class="listitem"><p class="simpara">
						Verify that the Compute hosts were assigned to the cell.
					</p><pre class="screen">$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 list_hosts</pre></li></ol></div></section><section class="section" id="proc_multi-cell-config-az"><div class="titlepage"><div><div><h2 class="title">4.7. Configuring an availability zone</h2></div></div></div><p>
				You must assign each cell to an availability zone (AZ) to keep the Compute nodes in that cell during instance creation and migration. The Controller cell must be in a different AZ from the Compute cells.
			</p><p>
				You can use host aggregates to configure the AZ for the Compute cell. The following example shows the command to create a host aggregate for the cell <code class="literal">cell1</code>, define the AZ for the host aggregate, and add the hosts within the cell to the AZ:
			</p><pre class="screen">(undercloud)$ source ~/overcloudrc
(overcloud)$ openstack aggregate create cell1 --zone cell1
(overcloud)$ openstack aggregate add host cell1 hostA
(overcloud)$ openstack aggregate add host cell1 hostB</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You cannot use the <code class="literal">OS::TripleO::Services::NovaAZConfig</code> parameter to automatically create the AZ during deployment, because the cell is not created at this stage.
						</li><li class="listitem">
							Migrating instances between cells is not supported. To move an instance to a different cell, you must delete it from the old cell and re-create it in the new cell.
						</li></ul></div></div></div><p>
				For more information on host aggregates and availability zones, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/creating-and-managing-host-aggregates">Creating and managing host aggregates</a>.
			</p></section><section class="section" id="proc_multi-cell-delete-compute-node"><div class="titlepage"><div><div><h2 class="title">4.8. Deleting a Compute node from a cell</h2></div></div></div><p>
				To delete a Compute node from a cell, you must delete all instances from the cell and delete the host names from the Placement database.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Delete all instances from the Compute nodes in the cell.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Migrating instances between cells is not supported. You must delete the instances and re-create them in another cell.
						</p></div></div></li><li class="listitem"><p class="simpara">
						On one of the global Controllers, delete all Compute nodes from the cell.
					</p><pre class="screen">$ CTRL=overcloud-controller-0
$ CTRL_IP=$(openstack server list -f value -c Networks --name $CTRL | sed 's/ctlplane=//')

$ export CONTAINERCLI='podman'

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 list_hosts

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 delete_host --cell_uuid &lt;uuid&gt; --host &lt;compute&gt;</pre></li><li class="listitem"><p class="simpara">
						Delete the resource providers for the cell from the Placement service, to ensure that the host name is available in case you want to add Compute nodes with the same host name to another cell later. For example:
					</p><pre class="screen">(undercloud) $ source ~/overcloudrc

(overcloud) $ openstack resource provider list
            +--------------------------------------+---------------------------------------+------------+
            | uuid                                 | name                                  | generation |
            +--------------------------------------+---------------------------------------+------------+
            | 9cd04a8b-5e6c-428e-a643-397c9bebcc16 | computecell1-novacompute-0.site1.test |         11 |
            +--------------------------------------+---------------------------------------+------------+

(overcloud) $ openstack resource provider delete 9cd04a8b-5e6c-428e-a643-397c9bebcc16</pre></li></ol></div></section><section class="section" id="proc_multi-cell-delete-cell"><div class="titlepage"><div><div><h2 class="title">4.9. Deleting a cell</h2></div></div></div><p>
				To delete a cell, you must first delete all instances and Compute nodes from the cell, as described in <a class="xref" href="index.html#proc_multi-cell-delete-compute-node" title="4.8. Deleting a Compute node from a cell">Section 4.8, “Deleting a Compute node from a cell”</a>. Then, you delete the cell itself and the cell stack.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						On one of the global Controllers, delete the cell.
					</p><pre class="screen">$ CTRL=overcloud-controller-0
$ CTRL_IP=$(openstack server list -f value -c Networks --name $CTRL | sed 's/ctlplane=//')

$ export CONTAINERCLI='podman'

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 list_cells

$ ssh heat-admin@${CTRL_IP} sudo ${CONTAINERCLI} exec -i -u root nova_api \
  nova-manage cell_v2 delete_cell --cell_uuid &lt;uuid&gt;</pre></li><li class="listitem"><p class="simpara">
						Delete the cell stack from the overcloud.
					</p><pre class="screen">$ openstack stack delete &lt;stack name&gt; --wait --yes &amp;&amp; openstack overcloud plan delete &lt;STACK_NAME&gt;</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If you deployed separate cell stacks for a Controller and Compute cell, delete the Compute cell stack first and then the Controller cell stack.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="creating-and-managing-host-aggregates"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Creating and managing host aggregates</h1></div></div></div><p>
			You can partition a Compute deployment into logical groups for performance or administrative purposes. Red Hat OpenStack Platform (RHOSP) provides the following mechanisms for partitioning logical groups:
		</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Host aggregate</span></dt><dd><p class="simpara">
						A host aggregate is a grouping of Compute nodes into a logical unit based on attributes such as the hardware or performance characteristics. You can assign a Compute node to one or more host aggregates.
					</p><p class="simpara">
						You can map flavors and images to host aggregates by setting metadata on the host aggregate, and then matching flavor extra specs or image metadata properties to the host aggregate metadata. The Compute scheduler can use this metadata to schedule instances when the required filters are enabled. Metadata that you specify in a host aggregate limits the use of that host to any instance that has the same metadata specified in its flavor or image.
					</p><p class="simpara">
						You can configure weight multipliers for each host aggregate by setting the <code class="literal">xxx_weight_multiplier</code> configuration option in the host aggregate metadata.
					</p><p class="simpara">
						You can use host aggregates to handle load balancing, enforce physical isolation or redundancy, group servers with common attributes, or separate classes of hardware.
					</p><p class="simpara">
						When you create a host aggregate, you can specify a zone name. This name is presented to cloud users as an availability zone that they can select.
					</p></dd><dt><span class="term">Availability zones</span></dt><dd><p class="simpara">
						An availability zone is the cloud user view of a host aggregate. A cloud user cannot view the Compute nodes in the availability zone, or view the metadata of the availability zone. The cloud user can only see the name of the availability zone.
					</p><p class="simpara">
						You can assign each Compute node to only one availability zone. You can configure a default availability zone where instances will be scheduled when the cloud user does not specify a zone. You can direct cloud users to use availability zones that have specific capabilities.
					</p></dd></dl></div><section class="section" id="enable-aggregate-scheduling-osp"><div class="titlepage"><div><div><h2 class="title">5.1. Enabling scheduling on host aggregates</h2></div></div></div><p>
				To schedule instances on host aggregates that have specific attributes, update the configuration of the Compute scheduler to enable filtering based on the host aggregate metadata.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Add the following values to the <code class="literal">NovaSchedulerDefaultFilters</code> parameter, if they are not already present:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								<code class="literal">AggregateInstanceExtraSpecsFilter</code>: Add this value to filter Compute nodes by host aggregate metadata that match flavor extra specs.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									For this filter to perform as expected, you must scope the flavor extra specs by prefixing the <code class="literal">extra_specs</code> key with the <code class="literal">aggregate_instance_extra_specs:</code> namespace.
								</p></div></div></li><li class="listitem"><p class="simpara">
								<code class="literal">AggregateImagePropertiesIsolation</code>: Add this value to filter Compute nodes by host aggregate metadata that match image metadata properties.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									To filter host aggregate metadata using image metadata properties, the host aggregate metadata key must match a valid image metadata property. For details on valid image metadata properties, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/appx-image-config-parameters">Appendix A. Image Configuration Parameters</a>.
								</p></div></div></li><li class="listitem"><p class="simpara">
								<code class="literal">AvailabilityZoneFilter</code>: Add this value to filter by availability zone when launching an instance.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									Instead of using the <code class="literal">AvailabilityZoneFilter</code> Compute scheduler service filter, you can use the Placement service to process availability zone requests.
								</p></div></div></li></ul></div></li><li class="listitem">
						Save the updates to your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Deploy the overcloud, adding your Compute environment file to the stack along with your other environment files:
					</p><pre class="screen">(undercloud)$ openstack overcloud deploy --templates \
  -e [your environment files] \
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml</pre></li></ol></div></section><section class="section" id="creating-host-aggregate-osp"><div class="titlepage"><div><div><h2 class="title">5.2. Creating a host aggregate</h2></div></div></div><p>
				You can create as many host aggregates as you require.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To create a host aggregate, enter the following command:
					</p><pre class="screen">(overcloud)# openstack aggregate create &lt;aggregate_name&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name you want to assign to the host aggregate.
					</p></li><li class="listitem"><p class="simpara">
						Add metadata to the host aggregate:
					</p><pre class="screen">(overcloud)# openstack aggregate set --property &lt;key=value&gt; \
  --property &lt;key=value&gt; \
  &lt;aggregate_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;key=value&gt;</code> with the metadata key-value pair. If you are using the <code class="literal">AggregateInstanceExtraSpecsFilter</code> filter, the key can be any arbitrary string, for example, <code class="literal">ssd=true</code>. If you are using the <code class="literal">AggregateImagePropertiesIsolation</code> filter, the key must match a valid image metadata property. For more information on valid image metadata properties, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/appx-image-config-parameters">Appendix A. Image Configuration Parameters</a>.
							</li><li class="listitem">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the host aggregate.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Add the Compute nodes to the host aggregate:
					</p><pre class="screen">(overcloud)# openstack aggregate add host &lt;aggregate_name&gt; \
  &lt;host_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the host aggregate to add the Compute node to.
							</li><li class="listitem">
								Replace <code class="literal">&lt;host_name&gt;</code> with the name of the Compute node to add to the host aggregate.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Create a flavor or image for the host aggregate:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Create a flavor:
							</p><pre class="screen">(overcloud)$ openstack flavor create \
  --ram &lt;size-mb&gt; \
  --disk &lt;size-gb&gt; \
  --vcpus &lt;no_reserved_vcpus&gt; \
  host-agg-flavor</pre></li><li class="listitem"><p class="simpara">
								Create an image:
							</p><pre class="screen">(overcloud)$ openstack image create host-agg-image</pre><p class="simpara">
								For information on how to create an image, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/ch-image-service#section-create-images">Creating an image</a>.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Set one or more key-value pairs on the flavor or image that match the key-value pairs on the host aggregate.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To set the key-value pairs on a flavor, use the scope <code class="literal">aggregate_instance_extra_specs</code>:
							</p><pre class="screen">(overcloud)# openstack flavor set \
  --property aggregate_instance_extra_specs:ssd=true \
  host-agg-flavor</pre></li><li class="listitem"><p class="simpara">
								To set the key-value pairs on an image, use valid image metadata properties as the key:
							</p><pre class="screen">(overcloud)# openstack image set --property os_type=linux \
  host-agg-image</pre></li></ul></div></li></ol></div></section><section class="section" id="create-availability-zone-osp"><div class="titlepage"><div><div><h2 class="title">5.3. Creating an availability zone</h2></div></div></div><p>
				You can create an availability zone that cloud users can select when they create an instance.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To create an availability zone, you can create a new availability zone host aggregate, or make an existing host aggregate an availability zone:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								To create a new availability zone host aggregate, enter the following command:
							</p><pre class="screen">(overcloud)# openstack aggregate create --zone &lt;availability_zone&gt; \
  &lt;aggregate_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Replace <code class="literal">&lt;availability_zone&gt;</code> with the name you want to assign to the availability zone.
									</li><li class="listitem">
										Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name you want to assign to the host aggregate.
									</li></ul></div></li><li class="listitem"><p class="simpara">
								To make an existing host aggregate an availability zone, enter the following command:
							</p><pre class="screen">(overcloud)# openstack aggregate set --zone &lt;availability_zone&gt; \
  &lt;aggregate_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Replace <code class="literal">&lt;availability_zone&gt;</code> with the name you want to assign to the availability zone.
									</li><li class="listitem">
										Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the host aggregate.
									</li></ul></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Optional: Add metadata to the availability zone:
					</p><pre class="screen">(overcloud)# openstack aggregate set --property &lt;key=value&gt; \
  &lt;aggregate_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;key=value&gt;</code> with your metadata key-value pair. You can add as many key-value properties as required.
							</li><li class="listitem">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the availability zone host aggregate.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Add Compute nodes to the availability zone host aggregate:
					</p><pre class="screen">(overcloud)# openstack aggregate add host &lt;aggregate_name&gt; \
  &lt;host_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the availability zone host aggregate to add the Compute node to.
							</li><li class="listitem">
								Replace <code class="literal">&lt;host_name&gt;</code> with the name of the Compute node to add to the availability zone.
							</li></ul></div></li></ol></div></section><section class="section" id="delete-aggregate-osp"><div class="titlepage"><div><div><h2 class="title">5.4. Deleting a host aggregate</h2></div></div></div><p>
				To delete a host aggregate, you first remove all the Compute nodes from the host aggregate.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To view a list of all the Compute nodes assigned to the host aggregate, enter the following command:
					</p><pre class="screen">(overcloud)# openstack aggregate show &lt;aggregate_name&gt;</pre></li><li class="listitem"><p class="simpara">
						To remove all assigned Compute nodes from the host aggregate, run the following command for each Compute node:
					</p><pre class="screen">(overcloud)# openstack aggregate remove host &lt;aggregate_name&gt; \
  &lt;host_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the host aggregate to remove the Compute node from.
							</li><li class="listitem">
								Replace <code class="literal">&lt;host_name&gt;</code> with the name of the Compute node to remove from the host aggregate.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						After you remove all the Compute nodes from the host aggregate, enter the following command to delete the host aggregate:
					</p><pre class="screen">(overcloud)# openstack aggregate delete &lt;aggregate_name&gt;</pre></li></ol></div></section><section class="section" id="tenant-isolation-osp"><div class="titlepage"><div><div><h2 class="title">5.5. Creating a tenant-isolated host aggregate</h2></div></div></div><p>
				You can create a host aggregate that is available only to specific tenants. Only the tenants that you assign to the host aggregate can launch instances on the host aggregate.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Tenant isolation uses the Placement service to filter host aggregates for each tenant. This process supersedes the functionality of the <code class="literal">AggregateMultiTenancyIsolation</code> filter. You therefore do not need to use the <code class="literal">AggregateMultiTenancyIsolation</code> filter.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open your Compute environment file.
					</li><li class="listitem">
						To schedule the tenant instances on the tenant-isolated host aggregate, set the <code class="literal">NovaSchedulerLimitTenantsToPlacementAggregate</code> parameter to <code class="literal">True</code> in the Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Optional: To ensure that only the tenants that you assign to a host aggregate can create instances on your cloud, set the <code class="literal">NovaSchedulerPlacementAggregateRequiredForTenants</code> parameter to <code class="literal">True</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							<code class="literal">NovaSchedulerPlacementAggregateRequiredForTenants</code> is <code class="literal">False</code> by default. When this parameter is <code class="literal">False</code>, tenants that are not assigned to a host aggregate can create instances on any host aggregate.
						</p></div></div></li><li class="listitem">
						Save the updates to your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Deploy the overcloud, adding your Compute environment file to the stack along with your other environment files:
					</p><pre class="screen">(undercloud)$ openstack overcloud deploy --templates \
  -e [your environment files] \
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml \</pre></li><li class="listitem">
						Create the host aggregate. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/creating-and-managing-host-aggregates#creating-host-aggregate-osp">Creating a host aggregate</a>.
					</li><li class="listitem"><p class="simpara">
						Retrieve the list of tenant IDs:
					</p><pre class="screen">(overcloud)# openstack project list</pre></li><li class="listitem"><p class="simpara">
						Use the <code class="literal">filter_tenant_id&lt;suffix&gt;</code> metadata key to assign tenants to the host aggregate:
					</p><pre class="screen">(overcloud)# openstack aggregate set \
  --property filter_tenant_id&lt;ID0&gt;=&lt;tenant_id0&gt; \
  --property filter_tenant_id&lt;ID1&gt;=&lt;tenant_id1&gt; \
  ...
  --property filter_tenant_id&lt;IDn&gt;=&lt;tenant_idn&gt; \
  &lt;aggregate_name&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;ID0&gt;</code>, <code class="literal">&lt;ID1&gt;</code>, and all IDs up to <code class="literal">&lt;IDn&gt;</code> with unique values for each tenant filter that you want to create.
							</li><li class="listitem">
								Replace <code class="literal">&lt;tenant_id0&gt;</code>, <code class="literal">&lt;tenant_id1&gt;</code>, and all tenant IDs up to <code class="literal">&lt;tenant_idn&gt;</code> with the ID of each tenant that you want to assign to the host aggregate.
							</li><li class="listitem"><p class="simpara">
								Replace <code class="literal">&lt;aggregate_name&gt;</code> with the name of the tenant-isolated host aggregate.
							</p><p class="simpara">
								For example, use the following syntax to assign tenants <code class="literal">78f1</code>, <code class="literal">9d3t</code>, and <code class="literal">aa29</code> to the host aggregate <code class="literal">tenant-isolated-aggregate</code>:
							</p><pre class="screen">(overcloud) # openstack aggregate set \
  --property filter_tenant_id0=78f1 \
  --property filter_tenant_id1=9d3t \
  --property filter_tenant_id2=aa29 \
  tenant-isolated-aggregate</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								You can create a host aggregate that is available only to a single specific tenant by omitting the suffix from the <code class="literal">filter_tenant_id</code> metadata key:
							</p><pre class="screen">(overcloud) # openstack aggregate set \
  --property filter_tenant_id=78f1 \
  single-tenant-isolated-aggregate</pre></div></div></li></ul></div></li></ol></div></section></section><section class="chapter" id="ch-configure_compute_storage"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Configure OpenStack Compute Storage</h1></div></div></div><p>
			This chapter describes the architecture for the back-end storage of images in OpenStack Compute (nova), and provides basic configuration options.
		</p><section class="section" id="architecture_overview"><div class="titlepage"><div><div><h2 class="title">6.1. Architecture Overview</h2></div></div></div><p>
				In Red Hat OpenStack Platform, the OpenStack Compute service uses the KVM hypervisor to execute compute workloads. The <code class="literal">libvirt</code> driver handles all interactions with KVM, and enables the creation of virtual machines.
			</p><p>
				Two types of <code class="literal">libvirt</code> storage must be considered for Compute:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Base image, which is a cached and formatted copy of the Image service image.
					</li><li class="listitem">
						Instance disk, which is created using the <code class="literal">libvirt</code> base and is the back end for the virtual machine instance. Instance disk data can be stored either in Compute’s ephemeral storage (using the <code class="literal">libvirt</code> base) or in persistent storage (for example, using Block Storage).
					</li></ul></div><p>
				<span class="inlinemediaobject"><img src="openstack-libvirt-images.png" alt="Creation of Virtual Machines"/></span>

			</p><p>
				The steps that Compute takes to create a virtual machine instance are:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Cache the Image service’s backing image as the <code class="literal">libvirt</code> base.
					</li><li class="listitem">
						Convert the base image to the raw format (if configured).
					</li><li class="listitem">
						Resize the base image to match the VM’s flavor specifications.
					</li><li class="listitem">
						Use the base image to create the libvirt instance disk.
					</li></ol></div><p>
				In the diagram above, the #1 instance disk uses ephemeral storage; the #2 disk uses a block-storage volume.
			</p><p>
				Ephemeral storage is an empty, unformatted, additional disk available to an instance. This storage value is defined by the instance flavor. The value provided by the user must be less than or equal to the ephemeral value defined for the flavor. The default value is <code class="literal">0</code>, meaning no ephemeral storage is created.
			</p><p>
				The ephemeral disk appears in the same way as a plugged-in hard drive or thumb drive. It is available as a block device which you can check using the <code class="literal">lsblk</code> command. You can format it, mount it, and use it however you normally would a block device. There is no way to preserve or reference that disk beyond the instance it is attached to.
			</p><p>
				Block storage volume is persistant storage available to an instance regardless of the state of the running instance.
			</p></section><section class="section" id="configuration"><div class="titlepage"><div><div><h2 class="title">6.2. Configuration</h2></div></div></div><p>
				You can configure performance tuning and security for your virtual disks by customizing the Compute (nova) configuration files. Compute is configured in custom environment files and heat templates using the parameters detailed in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/overcloud_parameters/compute-nova-parameters">Compute (nova) Parameters</a> section in the <span class="emphasis"><em>Overcloud Parameters</em></span> guide. This configuration is generated and stored in the <code class="literal">/var/lib/config-data/puppet-generated/&lt;nova_container&gt;/etc/nova/nova.conf</code> file, as detailed in the following table.
			</p><div class="table" id="idm139747277086176"><p class="title"><strong>Table 6.1. Compute Image Parameters</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 18%; " class="col_1"><!--Empty--></col><col style="width: 18%; " class="col_2"><!--Empty--></col><col style="width: 46%; " class="col_3"><!--Empty--></col><col style="width: 18%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747279554528" scope="col">Section</th><th align="left" valign="top" id="idm139747279553440" scope="col">Parameter</th><th align="left" valign="top" id="idm139747279552352" scope="col">Description</th><th align="left" valign="top" id="idm139747279551264" scope="col">Default</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">force_raw_images</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Whether to convert a <code class="literal">non-raw</code> cached base image to be <code class="literal">raw</code> (boolean). If a non-raw image is converted to raw, Compute:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Disallows backing files (which might be a security issue).
									</li><li class="listitem">
										Removes existing compression (to avoid CPU bottlenecks).
									</li></ul></div>
							 <p>
								Converting the base to raw uses more space for any image that could have been used directly by the hypervisor (for example, a qcow2 image). If you have a system with slower I/O or less available space, you might want to specify <span class="emphasis"><em>false</em></span>, trading the higher CPU requirements of compression for that of minimized input bandwidth.
							</p>
							 <p>
								Raw base images are always used with <code class="literal">libvirt_images_type=lvm</code>.
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">true</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">use_cow_images</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Whether to use CoW (Copy on Write) images for <code class="literal">libvirt</code> instance disks (boolean):
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">false</code> - The raw format is used. Without CoW, more space is used for common parts of the disk image
									</li><li class="listitem">
										<code class="literal">true</code> - The cqow2 format is used. With CoW, depending on the backing store and host caching, there may be better concurrency achieved by having each VM operate on its own copy.
									</li></ul></div>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								true
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">preallocate_images</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Preallocation mode for <code class="literal">libvirt</code> instance disks. Value can be:
							</p>
							 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">none</code> - No storage is provisioned at instance start.
									</li><li class="listitem">
										<code class="literal">space</code> - Storage is fully allocated at instance start (using <code class="literal">fallocate</code>), which can help with both space guarantees and I/O performance.
									</li></ul></div>
							 <p>
								Even when not using CoW instance disks, the copy each VM gets is sparse and so the VM may fail unexpectedly at run time with ENOSPC. By running <code class="literal">fallocate(1)</code> on the instance disk images, Compute immediately and efficiently allocates the space for them in the file system (if supported). Run time performance should also be improved because the file system does not have to dynamically allocate blocks at run time (reducing CPU overhead and more importantly file fragmentation).
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								none
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">resize_fs_using_block_device</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Whether to enable direct resizing of the base image by accessing the image over a block device (boolean). This is only necessary for images with older versions of <code class="literal">cloud-init</code> (that cannot resize themselves).
							</p>
							 <p>
								Because this parameter enables the direct mounting of images which might otherwise be disabled for security reasons, it is not enabled by default.
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">false</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">default_ephemeral_format</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								The default format that is used for a new ephemeral volume. Value can be: <code class="literal">ext2</code>, <code class="literal">ext3</code>, or <code class="literal">ext4</code>. The <code class="literal">ext4</code> format provides much faster initialization times than <code class="literal">ext3</code> for new, large disks. You can also override per instance using the <code class="literal">guest_format</code> configuration option.
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">ext4</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">image_cache_manager_interval</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Number of seconds to wait between runs of the image cache manager, which impacts base caching on libvirt compute nodes. This period is used in the auto removal of unused cached images (see <code class="literal">remove_unused_base_images</code> and <code class="literal">remove_unused_original_minimum_age_seconds</code>).
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">2400</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">remove_unused_base_images</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Whether to enable the automatic removal of unused base images (checked every <code class="literal">image_cache_manager_interval</code> seconds). Images are defined as <code class="literal">unused</code> if they have not been accessed in <code class="literal">remove_unused_original_minimum_age_seconds</code> seconds.
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">true</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[DEFAULT]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">remove_unused_original_minimum_age_seconds</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								How old an unused base image must be before being removed from the <code class="literal">libvirt</code> cache (see <code class="literal">remove_unused_base_images</code>).
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">86400</code>
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747279554528"> <p>
								[<code class="literal">libvirt</code>]
							</p>
							 </td><td align="left" valign="top" headers="idm139747279553440"> <p>
								<code class="literal">images_type</code>
							</p>
							 </td><td align="left" valign="top" headers="idm139747279552352"> <p>
								Image type to use for <code class="literal">libvirt</code> instance disks (deprecates <code class="literal">use_cow_images</code>). Value can be: <code class="literal">raw</code>, <code class="literal">qcow2</code>, <code class="literal">lvm</code>, <code class="literal">rbd</code>, or <code class="literal">default</code>. If <code class="literal">default</code> is specified, the value used for the <code class="literal">use_cow_images</code> parameter is used.
							</p>
							 </td><td align="left" valign="top" headers="idm139747279551264"> <p>
								<code class="literal">default</code>
							</p>
							 </td></tr></tbody></table></div></div></section></section><section class="chapter" id="ch-manage_instances"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Virtual Machine Instances</h1></div></div></div><p>
			OpenStack Compute is the central component that provides virtual machines on demand. Compute interacts with the Identity service for authentication, Image service for images (used to launch instances), and the dashboard service for the user and administrative interface.
		</p><p>
			Red Hat OpenStack Platform allows you to easily manage virtual machine instances in the cloud. The Compute service creates, schedules, and manages instances, and exposes this functionality to other OpenStack components. This chapter discusses these procedures along with procedures to add components like key pairs, security groups, host aggregates and flavors. The term <span class="emphasis"><em>instance</em></span> is used by OpenStack to mean a virtual machine instance.
		</p><section class="section" id="section-instances"><div class="titlepage"><div><div><h2 class="title">7.1. Manage Instances</h2></div></div></div><p>
				Before you can create an instance, you need to ensure certain other OpenStack components (for example, a network, key pair and an image or a volume as the boot source) are available for the instance.
			</p><p>
				This section discusses the procedures to add these components, create and manage an instance. Managing an instance refers to updating, and logging in to an instance, viewing how the instances are being used, resizing or deleting them.
			</p><section class="section" id="section_Add-components"><div class="titlepage"><div><div><h3 class="title">7.1.1. Add Components</h3></div></div></div><p>
					Use the following sections to create a network, key pair and upload an image or volume source. These components are used in the creation of an instance and are not available by default. You will also need to create a new security group to allow SSH access to the user.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project</strong></span>.
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Network &gt; Networks</strong></span>, and ensure there is a private network to which you can attach the new instance (to create a network, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/networking_guide/#create_a_network">Create a Network</a> section in the <span class="emphasis"><em>Networking Guide</em></span>).
						</li><li class="listitem">
							Select <span class="strong strong"><strong>Compute &gt; Access &amp; Security &gt; Key Pairs</strong></span>, and ensure there is a key pair (to create a key pair, see <a class="xref" href="index.html#section-create-keypair" title="7.2.1.1. Create a Key Pair">Section 7.2.1.1, “Create a Key Pair”</a>).
						</li><li class="listitem"><p class="simpara">
							Ensure that you have either an image or a volume that can be used as a boot source:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									To view boot-source images, select the <span class="strong strong"><strong>Images</strong></span> tab (to create an image, see <a class="xref" href="index.html#section-create-images" title="1.2.1. Creating an Image">Section 1.2.1, “Creating an Image”</a>).
								</li><li class="listitem">
									To view boot-source volumes, select the <span class="strong strong"><strong>Volumes</strong></span> tab (to create a volume, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/storage_guide/#section-create-volume">Create a Volume</a> in the <span class="emphasis"><em>Storage Guide</em></span>).
								</li></ul></div></li><li class="listitem">
							Select <span class="strong strong"><strong>Compute &gt; Access &amp; Security &gt; Security Groups</strong></span>, and ensure you have created a security group rule (to create a security group, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>).
						</li></ol></div></section><section class="section" id="section-launch-instance"><div class="titlepage"><div><div><h3 class="title">7.1.2. Launch an Instance</h3></div></div></div><p>
					Launch one or more instances from the dashboard.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						By default, the Launch Instance form is used to launch instances. However, you can also enable a Launch Instance wizard that simplifies the steps required. For more information, see <a class="xref" href="index.html#appx-enabling-launch-instance-wizard" title="Appendix B. Enabling the Launch Instance Wizard">Appendix B, <em>Enabling the Launch Instance Wizard</em></a>.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Launch Instance</strong></span>.
						</li><li class="listitem">
							Fill out the fields (those marked with '* ' are required), and click <span class="strong strong"><strong>Launch</strong></span>.
						</li></ol></div><p>
					One or more instances are created, and launched based on the options provided.
				</p><section class="section" id="section-launch-instance-options"><div class="titlepage"><div><div><h4 class="title">7.1.2.1. Launch Instance Options</h4></div></div></div><p>
						The following table outlines the options available when launching a new instance using the Launch Instance form. The same options are also available in the Launch instance wizard.
					</p><div class="table" id="idm139747273297344"><p class="title"><strong>Table 7.1. Launch Instance Form Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 22%; " class="col_1"><!--Empty--></col><col style="width: 22%; " class="col_2"><!--Empty--></col><col style="width: 56%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747279614224" scope="col">Tab</th><th align="left" valign="top" id="idm139747279613136" scope="col">Field</th><th align="left" valign="top" id="idm139747279612048" scope="col">Notes</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Project and User
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Project
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										Select the project from the dropdown list.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										User
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										Select the user from the dropdown list.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Details
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Availability Zone
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										Zones are logical groupings of cloud resources in which your instance can be placed. If you are unsure, use the default zone.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Instance Name
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										A name to identify your instance.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Flavor
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										The flavor determines what resources the instance is given (for example, memory). For default flavor allocations and information on creating new flavors, see <a class="xref" href="index.html#section-flavors" title="7.3. Manage Flavors">Section 7.3, “Manage Flavors”</a>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Instance Count
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										The number of instances to create with these parameters. "1" is preselected.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Instance Boot Source
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										Depending on the item selected, new fields are displayed allowing you to select the source:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												Image sources must be compatible with OpenStack (see <a class="xref" href="index.html#section-manage_images" title="1.2. Manage images">Section 1.2, “Manage images”</a>).
											</li><li class="listitem">
												If a volume or volume source is selected, the source must be formatted using an image (see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/storage_guide/#section-volumes_basic">Basic Volume Usage and Configuration</a> in the <span class="emphasis"><em>Storage Guide</em></span>).
											</li></ul></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Access and Security
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Key Pair
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										The specified key pair is injected into the instance and is used to remotely access the instance using SSH (if neither a direct login information or a static key pair is provided). Usually one key pair per project is created.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Security Groups
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										Security groups contain firewall rules which filter the type and direction of the instance’s network traffic (for more information on configuring groups, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>).
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Networking
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Selected Networks
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										You must select at least one network. Instances are typically assigned to a private network, and then later given a floating IP address to enable external access.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Post-Creation
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Customization Script Source
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										You can provide either a set of commands or a script file, which will run after the instance is booted (for example, to set the instance host name or a user password). If <span class="emphasis"><em>Direct Input</em></span> is selected, write your commands in the Script Data field; otherwise, specify your script file.
									</p>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Any script that starts with <span class="emphasis"><em>#cloud-config</em></span> is interpreted as using the cloud-config syntax (for information on the syntax, see <a class="link" href="http://cloudinit.readthedocs.org/en/latest/topics/examples.html">http://cloudinit.readthedocs.org/en/latest/topics/examples.html</a>).
										</p></div></div>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> <p>
										Advanced Options
									</p>
									 </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Disk Partition
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										By default, the instance is built as a single partition and dynamically resized as needed. However, you can choose to manually configure the partitions yourself.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747279614224"> </td><td align="left" valign="top" headers="idm139747279613136"> <p>
										Configuration Drive
									</p>
									 </td><td align="left" valign="top" headers="idm139747279612048"> <p>
										If selected, OpenStack writes metadata to a read-only configuration drive that is attached to the instance when it boots (instead of to Compute’s metadata service). After the instance has booted, you can mount this drive to view its contents (enables you to provide files to the instance).
									</p>
									 </td></tr></tbody></table></div></div></section></section><section class="section" id="update_an_instance_actions_menu"><div class="titlepage"><div><div><h3 class="title">7.1.3. Update an Instance (Actions menu)</h3></div></div></div><p>
					You can update an instance by selecting <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>, and selecting an action for that instance in the <span class="strong strong"><strong>Actions</strong></span> column. Actions allow you to manipulate the instance in a number of ways:
				</p><div class="table" id="idm139747270474096"><p class="title"><strong>Table 7.2. Update Instance Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747270469264" scope="col">Action</th><th align="left" valign="top" id="idm139747270468176" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Create Snapshot
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Snapshots preserve the disk state of a running instance. You can create a snapshot to migrate the instance, as well as to preserve backup copies.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Associate/Disassociate Floating IP
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									You must associate an instance with a floating IP (external) address before it can communicate with external networks, or be reached by external users. Because there are a limited number of external addresses in your external subnets, it is recommended that you disassociate any unused addresses.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Edit Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Update the instance’s name and associated security groups.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Edit Security Groups
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Add and remove security groups to or from this instance using the list of available security groups (for more information on configuring groups, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Console
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									View the instance’s console in the browser (allows easy access to the instance).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									View Log
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									View the most recent section of the instance’s console log. Once opened, you can view the full log by clicking View Full Log.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Pause/Resume Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Immediately pause the instance (you are not asked for confirmation); the state of the instance is stored in memory (RAM).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Suspend/Resume Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Immediately suspend the instance (you are not asked for confirmation); like hibernation, the state of the instance is kept on disk.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Resize Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Bring up the Resize Instance window (see <a class="xref" href="index.html#section-resize-instance" title="7.1.4. Resize an Instance">Section 7.1.4, “Resize an Instance”</a>).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Soft Reboot
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Gracefully stop and restart the instance. A soft reboot attempts to gracefully shut down all processes before restarting the instance.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Hard Reboot
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Stop and restart the instance. A hard reboot effectively just shuts down the instance’s <span class="emphasis"><em>power</em></span> and then turns it back on.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Shut Off Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Gracefully stop the instance.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Rebuild Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Use new image and disk-partition options to rebuild the image (shut down, re-image, and re-boot the instance). If encountering operating system issues, this option is easier to try than terminating the instance and starting over.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747270469264"> <p>
									Terminate Instance
								</p>
								 </td><td align="left" valign="top" headers="idm139747270468176"> <p>
									Permanently destroy the instance (you are asked for confirmation).
								</p>
								 </td></tr></tbody></table></div></div><p>
					You can create and allocate an external IP address, see <a class="xref" href="index.html#section-create-assign-release-floating-IPs" title="7.2.3. Create, Assign, and Release Floating IP Addresses">Section 7.2.3, “Create, Assign, and Release Floating IP Addresses”</a>
				</p></section><section class="section" id="section-resize-instance"><div class="titlepage"><div><div><h3 class="title">7.1.4. Resize an Instance</h3></div></div></div><p>
					To resize an instance (memory or CPU count), you must select a new flavor for the instance that has the right capacity. If you are increasing the size, remember to first ensure that the host has enough space.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Ensure communication between hosts by setting up each host with SSH key authentication so that Compute can use SSH to move disks to other hosts (for example, compute nodes can share the same SSH key).
						</li><li class="listitem"><p class="simpara">
							Enable resizing on the original host by setting the <code class="literal">allow_resize_to_same_host</code> parameter to "True" in your Compute environment file.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">allow_resize_to_same_host</code> parameter does not resize the instance on the same host. Even if the parameter equals "True" on all Compute nodes, the scheduler does not force the instance to resize on the same host. This is the expected behavior.
							</p></div></div></li><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>.
						</li><li class="listitem">
							Click the instance’s <span class="strong strong"><strong>Actions</strong></span> arrow, and select <span class="strong strong"><strong>Resize Instance</strong></span>.
						</li><li class="listitem">
							Select a new flavor in the <span class="strong strong"><strong>New Flavor</strong></span> field.
						</li><li class="listitem"><p class="simpara">
							If you want to manually partition the instance when it launches (results in a faster build time):
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Select <span class="strong strong"><strong>Advanced Options</strong></span>.
								</li><li class="listitem">
									In the <span class="strong strong"><strong>Disk Partition</strong></span> field, select <span class="strong strong"><strong>Manual</strong></span>.
								</li></ol></div></li><li class="listitem">
							Click <span class="strong strong"><strong>Resize</strong></span>.
						</li></ol></div></section><section class="section" id="connect_to_an_instance"><div class="titlepage"><div><div><h3 class="title">7.1.5. Connect to an Instance</h3></div></div></div><p>
					This section discusses the different methods you can use to access an instance console using the dashboard or the command-line interface. You can also directly connect to an instance’s serial port allowing you to debug even if the network connection fails.
				</p><section class="section" id="access_an_instance_console_using_the_dashboard"><div class="titlepage"><div><div><h4 class="title">7.1.5.1. Access an Instance Console using the Dashboard</h4></div></div></div><p>
						The console allows you a way to directly access your instance within the dashboard.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Compute &gt; Instances</strong></span>.
							</li><li class="listitem">
								Click the instance’s <span class="strong strong"><strong>More</strong></span> button and select <span class="strong strong"><strong>Console</strong></span>. 
								<span class="inlinemediaobject"><img src="console-access.png" alt="console access"/></span>

							</li><li class="listitem">
								Log in using the image’s user name and password (for example, a CirrOS image uses <span class="emphasis"><em>cirros</em></span>/<span class="emphasis"><em>cubswin:)</em></span>).
							</li></ol></div></section><section class="section" id="directly_connect_to_a_vnc_console"><div class="titlepage"><div><div><h4 class="title">7.1.5.2. Directly Connect to a VNC Console</h4></div></div></div><p>
						You can directly access an instance’s VNC console using a URL returned by <code class="literal">nova get-vnc-console</code> command.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Browser</span></dt><dd><p class="simpara">
									To obtain a browser URL, use:
								</p><pre class="screen">$ nova get-vnc-console <span class="emphasis"><em>INSTANCE_ID</em></span> novnc</pre></dd><dt><span class="term">Java Client</span></dt><dd><p class="simpara">
									To obtain a Java-client URL, use:
								</p><pre class="screen">$ nova get-vnc-console <span class="emphasis"><em>INSTANCE_ID</em></span> xvpvnc</pre></dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							nova-xvpvncviewer provides a simple example of a Java client. To download the client, use:
						</p><pre class="screen"># git clone https://github.com/cloudbuilders/nova-xvpvncviewer
# cd nova-xvpvncviewer/viewer
# make</pre><p>
							Run the viewer with the instance’s Java-client URL:
						</p><pre class="screen"># java -jar VncViewer.jar <span class="emphasis"><em>URL</em></span></pre><p>
							This tool is provided only for customer convenience, and is not officially supported by Red Hat.
						</p></div></div></section></section><section class="section" id="view_instance_usage"><div class="titlepage"><div><div><h3 class="title">7.1.6. View Instance Usage</h3></div></div></div><p>
					The following usage statistics are available:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Per Project
						</p><p class="simpara">
							To view instance usage per project, select <span class="strong strong"><strong>Project &gt; Compute &gt; Overview</strong></span>. A usage summary is immediately displayed for all project instances.
						</p><p class="simpara">
							You can also view statistics for a specific period of time by specifying the date range and clicking <span class="strong strong"><strong>Submit</strong></span>.
						</p></li><li class="listitem"><p class="simpara">
							Per Hypervisor
						</p><p class="simpara">
							If logged in as an administrator, you can also view information for all projects. Click <span class="strong strong"><strong>Admin &gt; System</strong></span> and select one of the tabs. For example, the <span class="strong strong"><strong>Resource Usage</strong></span> tab offers a way to view reports for a distinct time period. You might also click <span class="strong strong"><strong>Hypervisors</strong></span> to view your current vCPU, memory, or disk statistics.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">vCPU Usage</code> value (<code class="literal">x of y</code>) reflects the number of total vCPUs of all virtual machines (x) and the total number of hypervisor cores (y).
							</p></div></div></li></ul></div></section><section class="section" id="delete_an_instance"><div class="titlepage"><div><div><h3 class="title">7.1.7. Delete an Instance</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>, and select your instance.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>Terminate Instance</strong></span>.
						</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Deleting an instance does not delete its attached volumes; you must do this separately (see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/storage_guide/#section-delete-volume">Delete a Volume</a> in the <span class="emphasis"><em>Storage Guide</em></span>).
					</p></div></div></section><section class="section" id="manage_multiple_instances_at_once"><div class="titlepage"><div><div><h3 class="title">7.1.8. Manage Multiple Instances at Once</h3></div></div></div><p>
					If you need to start multiple instances at the same time (for example, those that were down for compute or controller maintenance) you can do so easily at <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							Click the check boxes in the first column for the instances that you want to start. If you want to select all of the instances, click the check box in the first row in the table.
						</li><li class="listitem">
							Click <span class="strong strong"><strong>More Actions</strong></span> above the table and select <span class="strong strong"><strong>Start Instances</strong></span>.
						</li></ol></div><p>
					Similarly, you can shut off or soft reboot multiple instances by selecting the respective actions.
				</p></section></section><section class="section" id="section-instance-security"><div class="titlepage"><div><div><h2 class="title">7.2. Manage Instance Security</h2></div></div></div><p>
				You can manage access to an instance by assigning it the correct security group (set of firewall rules) and key pair (enables SSH user access). Further, you can assign a floating IP address to an instance to enable external network access. The sections below outline how to create and manage key pairs, security groups, floating IP addresses and logging in to an instance using SSH. There is also a procedure for injecting an <code class="literal">admin</code> password in to an instance.
			</p><p>
				For information on managing security groups, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>.
			</p><section class="section" id="section-manage-keypair"><div class="titlepage"><div><div><h3 class="title">7.2.1. Manage Key Pairs</h3></div></div></div><p>
					Key pairs provide SSH access to the instances. Each time a key pair is generated, its certificate is downloaded to the local machine and can be distributed to users. Typically, one key pair is created for each project (and used for multiple instances).
				</p><p>
					You can also import an existing key pair into OpenStack.
				</p><section class="section" id="section-create-keypair"><div class="titlepage"><div><div><h4 class="title">7.2.1.1. Create a Key Pair</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Key Pairs</strong></span> tab, click <span class="strong strong"><strong>Create Key Pair</strong></span>.
							</li><li class="listitem">
								Specify a name in the <span class="strong strong"><strong>Key Pair Name</strong></span> field, and click <span class="strong strong"><strong>Create Key Pair</strong></span>.
							</li></ol></div><p>
						When the key pair is created, a key pair file is automatically downloaded through the browser. Save this file for later connections from external machines. For command-line SSH connections, you can load this file into SSH by executing:
					</p><pre class="screen"># ssh-add ~/.ssh/os-key.pem</pre></section><section class="section" id="import_a_key_pair"><div class="titlepage"><div><div><h4 class="title">7.2.1.2. Import a Key Pair</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Key Pairs</strong></span> tab, click <span class="strong strong"><strong>Import Key Pair</strong></span>.
							</li><li class="listitem">
								Specify a name in the <span class="strong strong"><strong>Key Pair Name</strong></span> field, and copy and paste the contents of your public key into the <span class="strong strong"><strong>Public Key</strong></span> field.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Import Key Pair</strong></span>.
							</li></ol></div></section><section class="section" id="delete_a_key_pair"><div class="titlepage"><div><div><h4 class="title">7.2.1.3. Delete a Key Pair</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Key Pairs</strong></span> tab, click the key’s <span class="strong strong"><strong>Delete Key Pair</strong></span> button.
							</li></ol></div></section></section><section class="section" id="section-create-security-group"><div class="titlepage"><div><div><h3 class="title">7.2.2. Create a Security Group</h3></div></div></div><p>
					Security groups are sets of IP filter rules that can be assigned to project instances, and which define networking access to the instance. Security group are project specific; project members can edit the default rules for their security group and add new rule sets.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select the <span class="strong strong"><strong>Project</strong></span> tab, and click <span class="strong strong"><strong>Compute &gt; Access &amp; Security</strong></span>.
						</li><li class="listitem">
							On the <span class="strong strong"><strong>Security Groups</strong></span> tab, click <span class="strong strong"><strong>+ Create Security Group</strong></span>.
						</li><li class="listitem">
							Provide a name and description for the group, and click <span class="strong strong"><strong>Create Security Group</strong></span>.
						</li></ol></div><p>
					For more information on managing project security, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>.
				</p></section><section class="section" id="section-create-assign-release-floating-IPs"><div class="titlepage"><div><div><h3 class="title">7.2.3. Create, Assign, and Release Floating IP Addresses</h3></div></div></div><p>
					By default, an instance is given an internal IP address when it is first created. However, you can enable access through the public network by creating and assigning a floating IP address (external address). You can change an instance’s associated IP address regardless of the instance’s state.
				</p><p>
					Projects have a limited range of floating IP address that can be used (by default, the limit is 50), so you should release these addresses for reuse when they are no longer needed. Floating IP addresses can only be allocated from an existing floating IP pool, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/networking_guide/#create_floating_ip_pools">Create Floating IP Pools</a> in the <span class="emphasis"><em>Networking Guide</em></span>.
				</p><section class="section" id="allocate_a_floating_ip_to_the_project"><div class="titlepage"><div><div><h4 class="title">7.2.3.1. Allocate a Floating IP to the Project</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Floating IPs</strong></span> tab, click <span class="strong strong"><strong>Allocate IP to Project</strong></span>.
							</li><li class="listitem">
								Select a network from which to allocate the IP address in the <span class="strong strong"><strong>Pool</strong></span> field.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Allocate IP</strong></span>.
							</li></ol></div></section><section class="section" id="assign_a_floating_ip"><div class="titlepage"><div><div><h4 class="title">7.2.3.2. Assign a Floating IP</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Floating IPs</strong></span> tab, click the address' <span class="strong strong"><strong>Associate</strong></span> button.
							</li><li class="listitem"><p class="simpara">
								Select the address to be assigned in the IP address field.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									If no addresses are available, you can click the <code class="literal">+</code> button to create a new address.
								</p></div></div></li><li class="listitem">
								Select the instance to be associated in the <span class="strong strong"><strong>Port</strong></span> to be <span class="strong strong"><strong>Associated</strong></span> field. An instance can only be associated with one floating IP address.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Associate</strong></span>.
							</li></ol></div></section><section class="section" id="release_a_floating_ip"><div class="titlepage"><div><div><h4 class="title">7.2.3.3. Release a Floating IP</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Access &amp; Security</strong></span>.
							</li><li class="listitem">
								On the <span class="strong strong"><strong>Floating IPs</strong></span> tab, click the address' menu arrow (next to the <span class="strong strong"><strong>Associate/Disassociate</strong></span> button).
							</li><li class="listitem">
								Select <span class="strong strong"><strong>Release Floating IP</strong></span>.
							</li></ol></div></section></section><section class="section" id="section-Check-instance"><div class="titlepage"><div><div><h3 class="title">7.2.4. Log in to an Instance</h3></div></div></div><p>
					<span class="strong strong"><strong>Prerequisites</strong></span>:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Ensure that the instance’s security group has an SSH rule (see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/users_and_identity_management_guide/#project-security">Project Security Management</a> in the <span class="emphasis"><em>Users and Identity Management Guide</em></span>).
						</li><li class="listitem">
							Ensure the instance has a floating IP address (external address) assigned to it (see <a class="xref" href="index.html#section-create-assign-release-floating-IPs" title="7.2.3. Create, Assign, and Release Floating IP Addresses">Section 7.2.3, “Create, Assign, and Release Floating IP Addresses”</a>).
						</li><li class="listitem">
							Obtain the instance’s key-pair certificate. The certificate is downloaded when the key pair is created; if you did not create the key pair yourself, ask your administrator (see <a class="xref" href="index.html#section-manage-keypair" title="7.2.1. Manage Key Pairs">Section 7.2.1, “Manage Key Pairs”</a>).
						</li></ul></div><p>
					<span class="strong strong"><strong>To first load the key pair file into SSH, and then use ssh without naming it</strong></span>:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Change the permissions of the generated key-pair certificate.
						</p><pre class="screen">$ chmod 600 os-key.pem</pre></li><li class="listitem"><p class="simpara">
							Check whether <code class="literal">ssh-agent</code> is already running:
						</p><pre class="screen"># ps -ef | grep ssh-agent</pre></li><li class="listitem"><p class="simpara">
							If not already running, start it up with:
						</p><pre class="screen"># eval `ssh-agent`</pre></li><li class="listitem"><p class="simpara">
							On your local machine, load the key-pair certificate into SSH. For example:
						</p><pre class="screen">$ ssh-add ~/.ssh/os-key.pem</pre></li><li class="listitem">
							You can now SSH into the file with the user supplied by the image.
						</li></ol></div><p>
					The following example command shows how to SSH into the Red Hat Enterprise Linux guest image with the user <code class="literal">cloud-user</code>:
				</p><pre class="screen">$ ssh cloud-user@192.0.2.24</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also use the certificate directly. For example:
					</p><pre class="screen">$ ssh -i /myDir/os-key.pem cloud-user@192.0.2.24</pre></div></div></section><section class="section" id="section-inject-admin-password"><div class="titlepage"><div><div><h3 class="title">7.2.5. Inject an <code class="literal">admin</code> Password Into an Instance</h3></div></div></div><p>
					You can inject an <code class="literal">admin</code> (<code class="literal">root</code>) password into an instance using the following procedure.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the <code class="literal">/etc/openstack-dashboard/local_settings</code> file, set the <code class="literal">change_set_password</code> parameter value to <code class="literal">True</code>.
						</p><pre class="screen">can_set_password: True</pre></li><li class="listitem"><p class="simpara">
							Set the <code class="literal">inject_password</code> parameter to "True" in your Compute environment file.
						</p><pre class="screen">inject_password=true</pre></li><li class="listitem"><p class="simpara">
							Restart the Compute service.
						</p><pre class="screen"># service nova-compute restart</pre></li></ol></div><p>
					When you use the <code class="literal">nova boot</code> command to launch a new instance, the output of the command displays an <code class="literal">adminPass</code> parameter. You can use this password to log into the instance as the <code class="literal">root</code> user.
				</p><p>
					The Compute service overwrites the password value in the <code class="literal">/etc/shadow</code> file for the <code class="literal">root</code> user. This procedure can also be used to activate the <code class="literal">root</code> account for the KVM guest images. For more information on how to use KVM guest images, see <a class="xref" href="index.html#section-kvm-images" title="1.2.1.1. Use a KVM Guest Image With Red Hat OpenStack Platform">Section 1.2.1.1, “Use a KVM Guest Image With Red Hat OpenStack Platform”</a>
				</p><p>
					You can also set a custom password from the dashboard. To enable this, run the following command after you have set <code class="literal">can_set_password</code> parameter to <code class="literal">true</code>.
				</p><pre class="screen"># systemctl restart httpd.service</pre><p>
					The newly added <code class="literal">admin</code> password fields are as follows:
				</p><p>
					<span class="inlinemediaobject"><img src="dashboard.png" alt="dashboard"/></span>

				</p><p>
					These fields can be used when you launch or rebuild an instance.
				</p></section></section><section class="section" id="section-flavors"><div class="titlepage"><div><div><h2 class="title">7.3. Manage Flavors</h2></div></div></div><p>
				Each created instance is given a flavor (resource template), which determines the instance’s size and capacity. Flavors can also specify secondary ephemeral storage, swap disk, metadata to restrict usage, or special project access (none of the default flavors have these additional attributes defined).
			</p><div class="table" id="idm139747277631696"><p class="title"><strong>Table 7.3. Default Flavors</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747277625008" scope="col">Name</th><th align="left" valign="top" id="idm139747277623920" scope="col">vCPUs</th><th align="left" valign="top" id="idm139747277622832" scope="col">RAM</th><th align="left" valign="top" id="idm139747277621744" scope="col">Root Disk Size</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747277625008"> <p>
								m1.tiny
							</p>
							 </td><td align="left" valign="top" headers="idm139747277623920"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139747277622832"> <p>
								512 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139747277621744"> <p>
								1 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747277625008"> <p>
								m1.small
							</p>
							 </td><td align="left" valign="top" headers="idm139747277623920"> <p>
								1
							</p>
							 </td><td align="left" valign="top" headers="idm139747277622832"> <p>
								2048 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139747277621744"> <p>
								20 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747277625008"> <p>
								m1.medium
							</p>
							 </td><td align="left" valign="top" headers="idm139747277623920"> <p>
								2
							</p>
							 </td><td align="left" valign="top" headers="idm139747277622832"> <p>
								4096 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139747277621744"> <p>
								40 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747277625008"> <p>
								m1.large
							</p>
							 </td><td align="left" valign="top" headers="idm139747277623920"> <p>
								4
							</p>
							 </td><td align="left" valign="top" headers="idm139747277622832"> <p>
								8192 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139747277621744"> <p>
								80 GB
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747277625008"> <p>
								m1.xlarge
							</p>
							 </td><td align="left" valign="top" headers="idm139747277623920"> <p>
								8
							</p>
							 </td><td align="left" valign="top" headers="idm139747277622832"> <p>
								16384 MB
							</p>
							 </td><td align="left" valign="top" headers="idm139747277621744"> <p>
								160 GB
							</p>
							 </td></tr></tbody></table></div></div><p>
				The majority of end users will be able to use the default flavors. However, you can create and manage specialized flavors. For example, you can:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Change default memory and capacity to suit the underlying hardware needs.
					</li><li class="listitem">
						Add metadata to force a specific I/O rate for the instance or to match a host aggregate.
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Behavior set using image properties overrides behavior set using flavors (for more information, see <a class="xref" href="index.html#section-manage_images" title="1.2. Manage images">Section 1.2, “Manage images”</a>).
				</p></div></div><section class="section" id="update_configuration_permissions"><div class="titlepage"><div><div><h3 class="title">7.3.1. Update Configuration Permissions</h3></div></div></div><p>
					By default, only administrators can create flavors or view the complete flavor list (select Admin &gt; System &gt; Flavors). To allow all users to configure flavors, specify the following in the <code class="literal">/etc/nova/policy.json</code> file (nova-api server):
				</p><p>
					<code class="literal">"compute_extension:flavormanage": "",</code>
				</p></section><section class="section" id="create_a_flavor"><div class="titlepage"><div><div><h3 class="title">7.3.2. Create a Flavor</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							As an admin user in the dashboard, select <span class="strong strong"><strong>Admin &gt; System &gt; Flavors</strong></span>.
						</li><li class="listitem"><p class="simpara">
							Click <span class="strong strong"><strong>Create Flavor</strong></span>, and specify the following fields:
						</p><div class="table" id="idm139747274054688"><p class="title"><strong>Table 7.4. Flavor Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747274048928" scope="col">Tab</th><th align="left" valign="top" id="idm139747274047840" scope="col">Field</th><th align="left" valign="top" id="idm139747274046752" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747274048928"> <p>
											Flavor Information
										</p>
										 </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											Name
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Unique name.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											ID
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Unique ID. The default value, <code class="literal">auto</code>, generates a UUID4 value, but you can also manually specify an integer or UUID4 value.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											VCPUs
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Number of virtual CPUs.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											RAM (MB)
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Memory (in megabytes).
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											Root Disk (GB)
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Ephemeral disk size (in gigabytes); to use the native image size, specify <code class="literal">0</code>. This disk is not used if <span class="strong strong"><strong>Instance Boot Source=Boot from Volume</strong></span>.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											Epehemeral Disk (GB)
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Secondary ephemeral disk size (in gigabytes) available to an instance. This disk is destroyed when an instance is deleted.
										</p>
										 <p>
											The default value is <code class="literal">0</code>, which implies that no ephemeral disk is created.
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											Swap Disk (MB)
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Swap disk size (in megabytes).
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747274048928"> <p>
											Flavor Access
										</p>
										 </td><td align="left" valign="top" headers="idm139747274047840"> <p>
											Selected Projects
										</p>
										 </td><td align="left" valign="top" headers="idm139747274046752"> <p>
											Projects which can use the flavor. If no projects are selected, all projects have access (<code class="literal">Public=Yes</code>).
										</p>
										 </td></tr></tbody></table></div></div></li><li class="listitem">
							Click Create Flavor.
						</li></ol></div></section><section class="section" id="update_general_attributes"><div class="titlepage"><div><div><h3 class="title">7.3.3. Update General Attributes</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							As an admin user in the dashboard, select <span class="strong strong"><strong>Admin &gt; System &gt; Flavors</strong></span>.
						</li><li class="listitem">
							Click the flavor’s <span class="strong strong"><strong>Edit Flavor</strong></span> button.
						</li><li class="listitem">
							Update the values, and click <span class="strong strong"><strong>Save</strong></span>.
						</li></ol></div></section><section class="section" id="section-update-flavor-metadata"><div class="titlepage"><div><div><h3 class="title">7.3.4. Update Flavor Metadata</h3></div></div></div><p>
					In addition to editing general attributes, you can add metadata to a flavor (<code class="literal">extra_specs</code>), which can help fine-tune instance usage. For example, you might want to set the maximum-allowed bandwidth or disk writes.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Pre-defined keys determine hardware support or quotas. Pre-defined keys are limited by the hypervisor you are using (for libvirt, see <a class="xref" href="index.html#table-libvirt-metadata" title="Table 7.5. Libvirt Metadata">Table 7.5, “Libvirt Metadata”</a>).
						</li><li class="listitem">
							Both pre-defined and user-defined keys can determine instance scheduling. For example, you might specify <code class="literal">SpecialComp=True</code>; any instance with this flavor can then only run in a host aggregate with the same key-value combination in its metadata.
						</li></ul></div><section class="section" id="view_metadata"><div class="titlepage"><div><div><h4 class="title">7.3.4.1. View Metadata</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								As an admin user in the dashboard, select <span class="strong strong"><strong>Admin &gt; System &gt; Flavors</strong></span>.
							</li><li class="listitem">
								Click the flavor’s <span class="strong strong"><strong>Metadata</strong></span> link (<code class="literal">Yes</code> or <code class="literal">No</code>). All current values are listed on the right-hand side under <span class="strong strong"><strong>Existing Metadata</strong></span>.
							</li></ol></div></section><section class="section" id="section-add-metadata"><div class="titlepage"><div><div><h4 class="title">7.3.4.2. Add Metadata</h4></div></div></div><p>
						You specify a flavor’s metadata using a <code class="literal">key/value</code> pair.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								As an admin user in the dashboard, select <span class="strong strong"><strong>Admin &gt; System &gt; Flavors</strong></span>.
							</li><li class="listitem">
								Click the flavor’s <span class="strong strong"><strong>Metadata</strong></span> link (<code class="literal">Yes</code> or <code class="literal">No</code>). All current values are listed on the right-hand side under <span class="strong strong"><strong>Existing Metadata</strong></span>.
							</li><li class="listitem">
								Under <span class="strong strong"><strong>Available Metadata</strong></span>, click on the <span class="strong strong"><strong>Other</strong></span> field, and specify the key you want to add (see <a class="xref" href="index.html#table-libvirt-metadata" title="Table 7.5. Libvirt Metadata">Table 7.5, “Libvirt Metadata”</a>).
							</li><li class="listitem">
								Click the + button; you can now view the new key under <span class="strong strong"><strong>Existing Metadata</strong></span>.
							</li><li class="listitem"><p class="simpara">
								Fill in the key’s value in its right-hand field.
							</p><p class="simpara">
								<span class="inlinemediaobject"><img src="flavor-metadata.png" alt="flavor metadata"/></span>

							</p></li><li class="listitem">
								When finished with adding key-value pairs, click <span class="strong strong"><strong>Save</strong></span>.
							</li></ol></div><div class="table" id="table-libvirt-metadata"><p class="title"><strong>Table 7.5. Libvirt Metadata</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 28%; " class="col_1"><!--Empty--></col><col style="width: 72%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747273611488" scope="col">Key</th><th align="left" valign="top" id="idm139747273610400" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw:action</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										Action that configures support limits per instance. Valid actions are:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">cpu_max_sockets</code> - Maximum supported CPU sockets.
											</li><li class="listitem">
												<code class="literal">cpu_max_cores</code> - Maximum supported CPU cores.
											</li><li class="listitem">
												<code class="literal">cpu_max_threads</code> - Maximum supported CPU threads.
											</li><li class="listitem">
												<code class="literal">cpu_sockets</code> - Preferred number of CPU sockets.
											</li><li class="listitem">
												<code class="literal">cpu_cores</code> - Preferred number of CPU cores.
											</li><li class="listitem">
												<code class="literal">cpu_threads</code> - Preferred number of CPU threads.
											</li><li class="listitem">
												<code class="literal">serial_port_count</code> - Maximum serial ports per instance.
											</li></ul></div>
									 <p>
										Example: <code class="literal">hw:cpu_max_sockets=2</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw:NUMA_def</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										Definition of NUMA topology for the instance. For flavors whose RAM and vCPU allocations are larger than the size of NUMA nodes in the compute hosts, defining NUMA topology enables hosts to better utilize NUMA and improve performance of the guest OS. NUMA definitions defined through the flavor override image definitions. Valid definitions are:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">numa_nodes</code> - Number of NUMA nodes to expose to the instance. Specify <span class="emphasis"><em>1</em></span> to ensure image NUMA settings are overridden.
											</li><li class="listitem">
												<code class="literal">numa_cpus.0</code> - Mapping of vCPUs N-M to NUMA node 0 (comma-separated list).
											</li><li class="listitem">
												<code class="literal">numa_cpus.1</code> - Mapping of vCPUs N-M to NUMA node 1 (comma-separated list).
											</li><li class="listitem">
												<code class="literal">numa_mem.0</code> - Mapping N MB of RAM to NUMA node 0.
											</li><li class="listitem">
												<code class="literal">numa_mem.1</code> - Mapping N MB of RAM to NUMA node 1.
											</li><li class="listitem">
												<code class="literal">numa_cpu.N</code> and <code class="literal">numa_mem.N</code> are only valid if <code class="literal">numa_nodes</code> is set. Additionally, they are only required if the instance’s NUMA nodes have an asymetrical allocation of CPUs and RAM (important for some NFV workloads).
											</li></ul></div>
									 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
											If the values of <code class="literal">numa_cpu</code> or <code class="literal">numa_mem.N</code> specify more than that available, an exception is raised.
										</p></div></div>
									 <p>
										Example when the instance has 8 vCPUs and 4GB RAM:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">hw:numa_nodes=2</code>
											</li><li class="listitem">
												<code class="literal">hw:numa_cpus.0=0,1,2,3,4,5</code>
											</li><li class="listitem">
												<code class="literal">hw:numa_cpus.1=6,7</code>
											</li><li class="listitem">
												<code class="literal">hw:numa_mem.0=3072</code>
											</li><li class="listitem">
												<code class="literal">hw:numa_mem.1=1024</code>
											</li></ul></div>
									 <p>
										The scheduler looks for a host with 2 NUMA nodes with the ability to run 6 CPUs + 3072 MB, or 3 GB, of RAM on one node, and 2 CPUS + 1024 MB, or 1 GB, of RAM on another node. If a host has a single NUMA node with capability to run 8 CPUs and 4 GB of RAM, it will not be considered a valid match.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw:watchdog_action</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										An instance watchdog device can be used to trigger an action if the instance somehow fails (or hangs). Valid actions are:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">disabled</code> - The device is not attached (default value).
											</li><li class="listitem">
												<code class="literal">pause</code> - Pause the instance.
											</li><li class="listitem">
												<code class="literal">poweroff</code> - Forcefully shut down the instance.
											</li><li class="listitem">
												<code class="literal">reset</code> - Forcefully reset the instance.
											</li><li class="listitem">
												<code class="literal">none</code> - Enable the watchdog, but do nothing if the instance fails.
											</li></ul></div>
									 <p>
										Example: <code class="literal">hw:watchdog_action=poweroff</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw:pci_numa_affinity_policy</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										You can use this parameter to specify the NUMA affinity policy for PCI passthrough devices and SR-IOV interfaces. Set to one of the following valid values:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">required</code>: The Compute service only creates an instance that requests a PCI device when at least one of the NUMA nodes of the instance has affinity with the PCI device. This option provides the best performance.
											</li><li class="listitem">
												<code class="literal">preferred</code>: The Compute service attempts a best effort selection of PCI devices based on NUMA affinity. If this is not possible, then the Compute service schedules the instance on a NUMA node that has no affinity with the PCI device.
											</li><li class="listitem"><p class="simpara">
												<code class="literal">legacy</code>: (Default) The Compute service creates instances that request a PCI device when either:
											</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
														The PCI device has affinity with at least one of the NUMA nodes; or
													</li><li class="listitem">
														The PCI devices do not provide information on their NUMA affinities.
													</li></ul></div></li></ul></div>
									 <p>
										Example: <code class="literal">hw:pci_numa_affinity_policy=required</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw_rng:action</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										A random-number generator device can be added to an instance using its image properties (see <code class="literal">hw_rng_model</code> in the "Command-Line Interface Reference" in Red Hat OpenStack Platform documentation).
									</p>
									 <p>
										If the device has been added, valid actions are:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">allowed</code> - If <code class="literal">True</code>, the device is enabled; if <code class="literal">False</code>, disabled. By default, the device is disabled.
											</li><li class="listitem">
												<code class="literal">rate_bytes</code> - Maximum number of bytes the instance’s kernel can read from the host to fill its entropy pool every rate_period (integer).
											</li><li class="listitem">
												<code class="literal">rate_period</code> - Duration of the read period in seconds (integer).
											</li></ul></div>
									 <p>
										Example: <code class="literal">hw_rng:allowed=True</code>.
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">hw_video:ram_max_mb</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										Maximum permitted RAM to be allowed for video devices (in MB).
									</p>
									 <p>
										Example: <code class="literal">hw:ram_max_mb=64</code>
									</p>
									 </td></tr><tr><td align="left" valign="top" headers="idm139747273611488"> <p>
										<code class="literal">quota:option</code>
									</p>
									 </td><td align="left" valign="top" headers="idm139747273610400"> <p>
										Enforcing limit for the instance. Valid options are:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">cpu_period</code> - Time period for enforcing <code class="literal">cpu_quota</code> (in microseconds). Within the specified <code class="literal">cpu_period</code>, each vCPU cannot consume more than <code class="literal">cpu_quota</code> of runtime. The value must be in range [1000, 1000000]; <span class="emphasis"><em>0</em></span> means <span class="emphasis"><em>no value</em></span>.
											</li><li class="listitem">
												<code class="literal">cpu_quota - Maximum allowed bandwidth (in microseconds) for the vCPU in each `cpu_period</code>. The value must be in range [1000, 18446744073709551]. <span class="emphasis"><em>0</em></span> means <span class="emphasis"><em>no value</em></span>; a negative value means that the vCPU is not controlled. <code class="literal">cpu_quota</code> and <code class="literal">cpu_period</code> can be used to ensure that all vCPUs run at the same speed.
											</li><li class="listitem">
												<code class="literal">cpu_shares</code> - Share of CPU time for the domain. The value only has meaning when weighted against other machine values in the same domain. That is, an instance with a flavor with <span class="emphasis"><em>200</em></span> will get twice as much machine time as an instance with <span class="emphasis"><em>100</em></span>.
											</li><li class="listitem">
												<code class="literal">disk_read_bytes_sec</code> - Maximum disk reads in bytes per second.
											</li><li class="listitem">
												<code class="literal">disk_read_iops_sec</code> - Maximum read I/O operations per second.
											</li><li class="listitem">
												<code class="literal">disk_write_bytes_sec</code> - Maximum disk writes in bytes per second.
											</li><li class="listitem">
												<code class="literal">disk_write_iops_sec</code> - Maximum write I/O operations per second.
											</li><li class="listitem">
												<code class="literal">disk_total_bytes_sec</code> - Maximum total throughput limit in bytes per second.
											</li><li class="listitem">
												<code class="literal">disk_total_iops_sec</code> - Maximum total I/O operations per second.
											</li><li class="listitem">
												<code class="literal">vif_inbound_average</code> - Desired average of incoming traffic.
											</li><li class="listitem">
												<code class="literal">vif_inbound_burst</code> - Maximum amount of traffic that can be received at <code class="literal">vif_inbound_peak</code> speed.
											</li><li class="listitem">
												<code class="literal">vif_inbound_peak</code> - Maximum rate at which incoming traffic can be received.
											</li><li class="listitem">
												<code class="literal">vif_outbound_average</code> - Desired average of outgoing traffic.
											</li><li class="listitem">
												<code class="literal">vif_outbound_burst</code> - Maximum amount of traffic that can be sent at <code class="literal">vif_outbound_peak</code> speed.
											</li><li class="listitem">
												<code class="literal">vif_outbound_peak</code> - Maximum rate at which outgoing traffic can be sent.
											</li></ul></div>
									 <p>
										Example: <code class="literal">quota:vif_inbound_average=10240</code>
									</p>
									 <p>
										In addition, the VMware driver supports the following quota options, which control upper and lower limits for CPUs, RAM, disks, and networks, as well as <span class="emphasis"><em>shares</em></span>, which can be used to control relative allocation of available resources among tenants:
									</p>
									 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
												<code class="literal">cpu_limit</code> - Maximum CPU frequency available to a virtual machine (in MHz).
											</li><li class="listitem">
												<code class="literal">cpu_reservation</code> - Guaranteed minimum amount of CPU resources available to a virtual machine (in MHz).
											</li><li class="listitem">
												<code class="literal">cpu_shares_level</code> - CPU allocation level (shares) in the case of contention. Possible values are <code class="literal">high</code>, <code class="literal">normal</code>, <code class="literal">low</code>, and <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">cpu_shares_share</code> - The number of allocated CPU shares. Applicable when <code class="literal">cpu_shares_level</code> is set to <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">memory_limit</code> - Maximum amount of RAM available to a virtual machine (in MB).
											</li><li class="listitem">
												<code class="literal">memory_reservation</code> - Guaranteed minimum amount of RAM available to a virtual machine (in MB).
											</li><li class="listitem">
												<code class="literal">memory_shares_level</code> - RAM allocation level (shares) in the case of contention. Possible values are <code class="literal">high</code>, <code class="literal">normal</code>, <code class="literal">low</code>, and <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">memory_shares_share</code> - The number of allocated RAM shares. Applicable when <code class="literal">memory_shares_level</code> is set to <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">disk_io_limit</code> - Maximum I/O utilization by a virtual machine (in I/O operations per second).
											</li><li class="listitem">
												<code class="literal">disk_io_reservation</code> - Guaranteed minimum amount of disk resources available to a virtual machine (in I/O operations per second).
											</li><li class="listitem">
												<code class="literal">disk_io_shares_level</code> - I/O allocation level (shares) in the case of contention. Possible values are <code class="literal">high</code>, <code class="literal">normal</code>, <code class="literal">low</code>, and <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">disk_io_shares_share</code> - The number of allocated I/O shares. Applicable when <code class="literal">disk_io_shares_level</code> is set to <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">vif_limit</code> - Maximum network bandwidth available to a virtual network adapter (in Mbps).
											</li><li class="listitem">
												<code class="literal">vif_reservation</code> - Guaranteed minimum network bandwidth available to a virtual network adapter (in Mbps).
											</li><li class="listitem">
												<code class="literal">vif_shares_level</code> - Network bandwidth allocation level (shares) in the case of contention. Possible values are <code class="literal">high</code>, <code class="literal">normal</code>, <code class="literal">low</code>, and <code class="literal">custom</code>.
											</li><li class="listitem">
												<code class="literal">vif_shares_share</code> - The number of allocated network bandwidth shares. Applicable when <code class="literal">vif_shares_level</code> is set to <code class="literal">custom</code>.
											</li></ul></div>
									 </td></tr></tbody></table></div></div></section></section></section><section class="section" id="section-scheduler"><div class="titlepage"><div><div><h2 class="title">7.4. Schedule Hosts</h2></div></div></div><p>
				The Compute scheduling service determines on which host, or host aggregate, to place an instance. As an administrator, you can influence where the scheduler places an instance. For example, you might want to limit scheduling to hosts in a certain group or with the right RAM.
			</p><p>
				You can configure the following components:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Filters - Determine the initial set of hosts on which an instance might be placed (see <a class="xref" href="index.html#section-schedule-filters" title="7.4.1. Configure Scheduling Filters">Section 7.4.1, “Configure Scheduling Filters”</a>).
					</li><li class="listitem">
						Weights - When filtering is complete, the resulting set of hosts are prioritized using the weighting system. The highest weight has the highest priority (see <a class="xref" href="index.html#section-schedule-weights" title="7.4.2. Configure Scheduling Weights">Section 7.4.2, “Configure Scheduling Weights”</a>).
					</li><li class="listitem">
						Scheduler service - There are a number of configuration options in the <code class="literal">/var/lib/config-data/puppet-generated/&lt;nova_container&gt;/etc/nova/nova.conf</code> file (on the scheduler host), which determine how the scheduler executes its tasks, and handles weights and filters.
					</li><li class="listitem">
						Placement service - Specify the traits an instance requires a host to have, such as the type of storage disk, or the Intel CPU instruction set extension (see <a class="xref" href="index.html#section-placement-service" title="7.4.3. Configure Placement Service Traits">Section 7.4.3, “Configure Placement Service Traits”</a>).
					</li></ul></div><p>
				In the following diagram, both host 1 and 3 are eligible after filtering. Host 1 has the highest weight and therefore has the highest priority for scheduling.
			</p><p>
				<span class="inlinemediaobject"><img src="scheduling.png" alt="Scheduling Hosts"/></span>

			</p><section class="section" id="section-schedule-filters"><div class="titlepage"><div><div><h3 class="title">7.4.1. Configure Scheduling Filters</h3></div></div></div><p>
					You define the filters you want the scheduler to use using the <code class="literal">NovaSchedulerDefaultFilters</code> parameter in your Compute environment file. Filters can be added or removed.
				</p><p>
					The default configuration runs the following filters in the scheduler:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							RetryFilter
						</li><li class="listitem">
							AvailabilityZoneFilter
						</li><li class="listitem">
							ComputeFilter
						</li><li class="listitem">
							ComputeCapabilitiesFilter
						</li><li class="listitem">
							ImagePropertiesFilter
						</li><li class="listitem">
							ServerGroupAntiAffinityFilter
						</li><li class="listitem">
							ServerGroupAffinityFilter
						</li></ul></div><p>
					Some filters use information in parameters passed to the instance in:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							The <code class="literal">nova boot</code> command.
						</li><li class="listitem">
							The instance’s flavor (see <a class="xref" href="index.html#section-update-flavor-metadata" title="7.3.4. Update Flavor Metadata">Section 7.3.4, “Update Flavor Metadata”</a>)
						</li><li class="listitem">
							The instance’s image (see <a class="xref" href="index.html#appx-image-config-parameters" title="Appendix A. Image Configuration Parameters">Appendix A, <em>Image Configuration Parameters</em></a>).
						</li></ul></div><p>
					All available filters are listed in the following table.
				</p><div class="table" id="scheduling_filters_table"><p class="title"><strong>Table 7.6. Scheduling Filters</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 28%; " class="col_1"><!--Empty--></col><col style="width: 72%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747272799648" scope="col">Filter</th><th align="left" valign="top" id="idm139747272798560" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">AggregateImagePropertiesIsolation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Only passes hosts in host aggregates whose metadata matches the instance’s image metadata; only valid if a host aggregate is specified for the instance. For more information, see <a class="xref" href="index.html#section-create-images" title="1.2.1. Creating an Image">Section 1.2.1, “Creating an Image”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">AggregateInstanceExtraSpecsFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Metadata in the host aggregate must match the host’s flavor metadata. For more information, see <a class="xref" href="index.html#section-update-flavor-metadata" title="7.3.4. Update Flavor Metadata">Section 7.3.4, “Update Flavor Metadata”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									To use this filter in the same <code class="literal">NovaSchedulerDefaultFilters</code> parameter as <code class="literal">ComputeCapabilitiesFilter</code>, you must scope your flavor <code class="literal">extra_specs</code> keys by prefixing them with the correct namespace:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">ComputeCapabilitiesFilter</code> namespace = <code class="literal">"capabilities:"</code>
										</li><li class="listitem">
											<code class="literal">AggregateInstanceExtraSpecsFilter</code> namespace = <code class="literal">"aggregate_instance_extra_specs:"</code>
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">AggregateMultiTenancyIsolation</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									A host with the specified <code class="literal">filter_tenant_id</code> can only contain instances from that tenant (project).
								</p>
								 <div class="admonition note"><div class="admonition_header">Note</div><div><p>
										The tenant can still place instances on other hosts.
									</p></div></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">AllHostsFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Passes all available hosts (however, does not disable other filters).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">AvailabilityZoneFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Filters using the instance’s specified availability zone.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">ComputeCapabilitiesFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Ensures Compute metadata is read correctly. Anything before the <code class="literal">:</code> is read as a namespace. For example, <code class="literal">quota:cpu_period</code> uses <code class="literal">quota</code> as the namespace and <code class="literal">cpu_period</code> as the key.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">ComputeFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Passes only hosts that are operational and enabled.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">DifferentHostFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Enables an instance to build on a host that is different from one or more specified hosts. Specify <code class="literal">different</code> hosts using the <code class="literal">nova boot</code> option <code class="literal">--different_host</code> option.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">ImagePropertiesFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Only passes hosts that match the instance’s image properties. For more information, see <a class="xref" href="index.html#section-create-images" title="1.2.1. Creating an Image">Section 1.2.1, “Creating an Image”</a>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">IsolatedHostsFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Passes only isolated hosts running isolated images that are specified using <code class="literal">isolated_hosts</code> and <code class="literal">isolated_images</code> (comma-separated values).
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">JsonFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Recognises and uses an instance’s custom JSON filters:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Valid operators are: =, &lt;, &gt;, in, ⇐, &gt;=, not, or, and
										</li><li class="listitem">
											Recognised variables are: $free_ram_mb, $free_disk_mb, $total_usable_ram_mb, $vcpus_total, $vcpus_used
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									The filter is specified as a query hint in the <code class="literal">nova boot</code> command. For example:
								</p>
								 <p>
									<code class="literal">--hint query='['&gt;=', '$free_disk_mb', 200 * 1024]'</code>
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">MetricsFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Use this filter to limit scheduling to Compute nodes that report the metrics configured by using <code class="literal">metrics/weight_setting</code>.
								</p>
								 <div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
									By default, the Compute scheduling service updates the metrics every 60 seconds. To ensure the metrics are up-to-date, you can increase the frequency at which the metrics data is refreshed using the <code class="literal">update_resources_interval</code> configuration option. For example, use the following configuration to refresh the metrics data every 2 seconds:
								</p><pre class="screen">parameter_defaults:
  ComputeExtraConfig:
    nova::config::nova_config:
      DEFAULT/update_resources_interval:
        value: '2'</pre></div></div></td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">NUMATopologyFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Filters out hosts based on its NUMA topology. If the instance has no topology defined, any host can be used. The filter tries to match the exact NUMA topology of the instance to those of the host (it does not attempt to pack the instance onto the host). The filter also looks at the standard over-subscription limits for each NUMA node, and provides limits to the compute host accordingly.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">RetryFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Filters out hosts that have failed a scheduling attempt; valid if <code class="literal">scheduler_max_attempts</code> is greater than zero (defaults to "3").
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">SameHostFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Passes one or more specified hosts; specify hosts for the instance using the <code class="literal">--hint same_host</code> option for <code class="literal">nova boot</code>.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">ServerGroupAffinityFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Only passes hosts for a specific server group:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Give the server group the affinity policy (<code class="literal">nova server-group-create --policy affinity groupName</code>).
										</li><li class="listitem">
											Build the instance with that group (<code class="literal">nova boot</code> option <code class="literal">--hint group=UUID</code>)
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">ServerGroupAntiAffinityFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Only passes hosts in a server group that do not already host an instance:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Give the server group the anti-affinity policy (<code class="literal">nova server-group-create --policy anti-affinity groupName</code>).
										</li><li class="listitem">
											Build the instance with that group (<code class="literal">nova boot</code> option <code class="literal">--hint group=UUID</code>).
										</li></ul></div>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747272799648"> <p>
									<code class="literal">SimpleCIDRAffinityFilter</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747272798560"> <p>
									Only passes hosts on the specified IP subnet range specified by the instance’s cidr and <code class="literal">build_new_host_ip</code> hints. Example:
								</p>
								 <p>
									<code class="literal">--hint build_near_host_ip=192.0.2.0 --hint cidr=/24</code>
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="section-schedule-weights"><div class="titlepage"><div><div><h3 class="title">7.4.2. Configure Scheduling Weights</h3></div></div></div><p>
					Hosts can be weighted for scheduling; the host with the largest weight (after filtering) is selected. All weighers are given a multiplier that is applied after normalising the node’s weight. A node’s weight is calculated as:
				</p><pre class="screen">w1_multiplier * norm(w1) + w2_multiplier * norm(w2) + ...</pre><p>
					You can configure weight options in the Compute node configuration file.
				</p><div class="table" id="idm139747301135024"><p class="title"><strong>Table 7.7. Configuration options for Scheduling service weights</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 44%; " class="col_1"><!--Empty--></col><col style="width: 56%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747277752704" scope="col">Configuration option</th><th align="left" valign="top" id="idm139747277751616" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/weight_classes</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to configure which of the following attributes to use for calculating the weight of each host:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">nova.scheduler.weights.ram.RAMWeigher</code> - Weighs the available RAM on the Compute node.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.cpu.CPUWeigher</code> - Weighs the available CPUs on the Compute node.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.disk.DiskWeigher</code> - Weighs the available disks on the Compute node.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.metrics.MetricsWeigher</code> - Weighs the metrics of the Compute node.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.affinity.ServerGroupSoftAffinityWeigher</code> - Weighs the proximity of the Compute node to other nodes in the given instance group.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.affinity.ServerGroupSoftAntiAffinityWeigher</code> - Weighs the proximity of the Compute node to other nodes in the given instance group.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.compute.BuildFailureWeigher</code> - Weighs Compute nodes by the number of recent failed boot attempts.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.io_ops.IoOpsWeigher</code> - Weighs Compute nodes by their workload.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.pci.PCIWeigher</code> - Weighs Compute nodes by their PCI availability.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.cross_cell.CrossCellWeigher</code> - Weighs Compute nodes based on which cell they are in, giving preference to Compute nodes in the source cell when moving an instance.
										</li><li class="listitem">
											<code class="literal">nova.scheduler.weights.all_weighers</code> - (Default) Uses all the above weighers.
										</li></ul></div>
								 <p>
									Type: String
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/ram_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on the available RAM.
								</p>
								 <p>
									Set to a positive value to prefer hosts with more available RAM, which spreads instances across many hosts.
								</p>
								 <p>
									Set to a negative value to prefer hosts with less available RAM, which fills up (stacks) hosts as much as possible before scheduling to a less-used host.
								</p>
								 <p>
									The absolute value, whether positive or negative, controls how strong the RAM weigher is relative to other weighers.
								</p>
								 <p>
									By default, the scheduler spreads instances across all hosts evenly (<code class="literal">ram_weight_multiplier=1.0</code>).
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/disk_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on the available disk space.
								</p>
								 <p>
									Set to a positive value to prefer hosts with more available disk space, which spreads instances across many hosts.
								</p>
								 <p>
									Set to a negative value to prefer hosts with less available disk space, which fills up (stacks) hosts as much as possible before scheduling to a less-used host.
								</p>
								 <p>
									The absolute value, whether positive or negative, controls how strong the disk weigher is relative to other weighers.
								</p>
								 <p>
									By default, the scheduler spreads instances across all hosts evenly (<code class="literal">disk_weight_multiplier=1.0</code>).
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/cpu_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on the available vCPUs.
								</p>
								 <p>
									Set to a positive value to prefer hosts with more available vCPUs, which spreads instances across many hosts.
								</p>
								 <p>
									Set to a negative value to prefer hosts with less available vCPUs, which fills up (stacks) hosts as much as possible before scheduling to a less-used host.
								</p>
								 <p>
									The absolute value, whether positive or negative, controls how strong the vCPU weigher is relative to other weighers.
								</p>
								 <p>
									By default, the scheduler spreads instances across all hosts evenly (<code class="literal">cpu_weight_multiplier=1.0</code>).
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/io_ops_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on the host workload.
								</p>
								 <p>
									Set to a negative value to prefer hosts with lighter workloads, which distributes the workload across more hosts.
								</p>
								 <p>
									Set to a positive value to prefer hosts with heavier workloads, which schedules instances onto hosts that are already busy.
								</p>
								 <p>
									The absolute value, whether positive or negative, controls how strong the I/O operations weigher is relative to other weighers.
								</p>
								 <p>
									By default, the scheduler distributes the workload across more hosts (<code class="literal">io_ops_weight_multiplier=-1.0</code>).
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/build_failure_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on recent build failures.
								</p>
								 <p>
									Set to a positive value to increase the significance of build failures recently reported by the host. Hosts with recent build failures are then less likely to be chosen.
								</p>
								 <p>
									Set to <code class="literal">0</code> to disable weighing compute hosts by the number of recent failures.
								</p>
								 <p>
									Default: 1000000.0
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/cross_cell_move_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts during a cross-cell move. This option determines how much weight is placed on a host which is within the same source cell when moving an instance. By default, the scheduler prefers hosts within the same source cell when migrating an instance.
								</p>
								 <p>
									Set to a positive value to prefer hosts within the same cell the instance is currently running. Set to a negative value to prefer hosts located in a different cell from that where the instance is currently running.
								</p>
								 <p>
									Default: 1000000.0
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/pci_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts based on the number of PCI devices on the host and the number of PCI devices requested by an instance. If an instance requests PCI devices, then the more PCI devices a Compute node has the higher the weight allocated to the Compute node.
								</p>
								 <p>
									For example, if there are three hosts available, one with a single PCI device, one with multiple PCI devices and one without any PCI devices, then the Compute scheduler prioritizes these hosts based on the demands of the instance. The first host should be preferred if the instance requests one PCI device, the second host if the instance requires multiple PCI devices and the third host if the instance does not request a PCI device.
								</p>
								 <p>
									Configure this option to prevent non-PCI instances from occupying resources on hosts with PCI devices.
								</p>
								 <p>
									Default: 1.0
								</p>
								 <p>
									Type: Positive floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/host_subset_size</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the size of the subset of filtered hosts from which to select the host. Must be set to at least 1. A value of 1 selects the first host returned by the weighing functions. Any value less than 1 is ignored and 1 is used instead.
								</p>
								 <p>
									Set to a value greater than 1 to prevent multiple scheduler processes handling similar requests selecting the same host, creating a potential race condition. By selecting a host randomly from the N hosts that best fit the request, the chance of a conflict is reduced. However, the higher you set this value, the less optimal the chosen host may be for a given request.
								</p>
								 <p>
									Default: 1
								</p>
								 <p>
									Type: Integer
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/soft_affinity_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts for group soft-affinity.
								</p>
								 <p>
									Default: 1.0
								</p>
								 <p>
									Type: Positive floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">filter_scheduler/soft_anti_affinity_weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use to weigh hosts for group soft-anti-affinity.
								</p>
								 <p>
									Default: 1.0
								</p>
								 <p>
									Type: Positive floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">metrics/weight_multiplier</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the multiplier to use for weighting metrics. By default, <code class="literal">weight_multiplier=1.0</code>, which spreads instances across possible hosts.
								</p>
								 <p>
									Set to a number greater than 1.0 to increase the effect of the metric on the overall weight.
								</p>
								 <p>
									Set to a number between 0.0 and 1.0 to reduce the effect of the metric on the overall weight.
								</p>
								 <p>
									Set to 0.0 to ignore the metric value and return the value of the ‘weight_of_unavailable’ option.
								</p>
								 <p>
									Set to a negative number to prioritize the host with lower metrics, and stack instances in hosts.
								</p>
								 <p>
									Default: 1.0
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">metrics/weight_setting</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the metrics to use for weighting, and the ratio to use to calculate the weight of each metric. Valid metric names:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">cpu.frequency</code> - CPU frequency
										</li><li class="listitem">
											<code class="literal">cpu.user.time</code> - CPU user mode time
										</li><li class="listitem">
											<code class="literal">cpu.kernel.time</code> - CPU kernel time
										</li><li class="listitem">
											<code class="literal">cpu.idle.time</code> - CPU idle time
										</li><li class="listitem">
											<code class="literal">cpu.iowait.time</code> - CPU I/O wait time
										</li><li class="listitem">
											<code class="literal">cpu.user.percent</code> - CPU user mode percentage
										</li><li class="listitem">
											<code class="literal">cpu.kernel.percent</code> - CPU kernel percentage
										</li><li class="listitem">
											<code class="literal">cpu.idle.percent</code> - CPU idle percentage
										</li><li class="listitem">
											<code class="literal">cpu.iowait.percent</code> - CPU I/O wait percentage
										</li><li class="listitem">
											<code class="literal">cpu.percent</code> - Generic CPU utilization
										</li></ul></div>
								 <p>
									Example: <code class="literal">weight_setting=cpu.user.time=1.0</code>
								</p>
								 <p>
									Type: Comma-separated list of <code class="literal">metric=ratio</code> pairs.
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">metrics/required</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify how to handle configured <code class="literal">metrics/weight_setting</code> metrics that are unavailable:
								</p>
								 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											<code class="literal">True</code> - Metrics are required. If the metric is unavailable, an exception is raised. To avoid the exception, use the <code class="literal">MetricsFilter</code> filter in <code class="literal">NovaSchedulerDefaultFilters</code>.
										</li><li class="listitem">
											<code class="literal">False</code> - The unavailable metric is treated as a negative factor in the weighing process. Set the returned value by using the <code class="literal">weight_of_unavailable</code> configuration option.
										</li></ul></div>
								 <p>
									Type: Boolean
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm139747277752704"> <p>
									<code class="literal">metrics/weight_of_unavailable</code>
								</p>
								 </td><td align="left" valign="top" headers="idm139747277751616"> <p>
									Use this parameter to specify the weight to use if any <code class="literal">metrics/weight_setting</code> metric is unavailable, and <code class="literal">metrics/required=False</code>.
								</p>
								 <p>
									Default: -10000.0
								</p>
								 <p>
									Type: Floating point
								</p>
								 </td></tr></tbody></table></div></div></section><section class="section" id="section-placement-service"><div class="titlepage"><div><div><h3 class="title">7.4.3. Configure Placement Service Traits</h3></div></div></div><p>
					The placement service tracks the inventory and usage of resource providers, which can be a compute node, a shared storage pool, or an IP allocation pool. Any service that needs to manage the selection and consumption of resources can use the placement service.
				</p><p>
					To query the placement service, install the <code class="literal">python3-osc-placement</code> package on the undercloud.
				</p><p>
					Each resource provider has a set of traits. Traits are the qualitative aspects of a resource provider, for example, the type of storage disk, or the Intel CPU instruction set extension. An instance can specify which of these traits it requires.
				</p><p>
					The Compute (nova) service interacts with the placement service when it creates instances, with the <code class="literal">nova-compute</code> and <code class="literal">nova-scheduler</code> processes.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">nova-compute</code></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Creates the resource provider record.
									</li><li class="listitem">
										Sets the inventory that describes the available quantitative resources, such as the available vCPUs.
									</li><li class="listitem">
										Sets the traits that describe qualitative aspects of the resource provider. The <code class="literal">libvirt</code> virtualization driver reports these traits to the placement service (see <a class="xref" href="index.html#section-specifying-libvirt-driver-traits" title="7.4.3.1. libvirt virtualization driver capabilities as placement service traits">Section 7.4.3.1, “<code class="literal">libvirt</code> virtualization driver capabilities as placement service traits”</a> for details).
									</li></ul></div></dd><dt><span class="term"><code class="literal">nova-scheduler</code></span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Sends a request to the placement service for a list of allocation candidates.
									</li><li class="listitem">
										Decides which destination host to build a server on, based on the traits required by the instance.
									</li></ul></div></dd></dl></div><section class="section" id="section-specifying-libvirt-driver-traits"><div class="titlepage"><div><div><h4 class="title">7.4.3.1. <code class="literal">libvirt</code> virtualization driver capabilities as placement service traits</h4></div></div></div><p>
						You can use the capabilities of <code class="literal">libvirt</code> virtualization drivers as placement service traits. The traits that you can specify are defined in the <code class="literal">os-traits</code> library, for example:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">COMPUTE_TRUSTED_CERTS</code>
							</li><li class="listitem">
								<code class="literal">COMPUTE_NET_ATTACH_INTERFACE_WITH_TAG</code>
							</li><li class="listitem">
								<code class="literal">COMPUTE_IMAGE_TYPE_RAW</code>
							</li><li class="listitem">
								<code class="literal">HW_CPU_X86_AVX</code>
							</li><li class="listitem">
								<code class="literal">HW_CPU_X86_AVX512VL</code>
							</li><li class="listitem">
								<code class="literal">HW_CPU_X86_AVX512CD</code>
							</li></ul></div><p>
						See the <a class="link" href="https://docs.openstack.org/os-traits/latest/"><code class="literal">os-traits</code> library</a> for a catalog of the standardized constants that an instance can request for a particular hardware, virtualization, storage, network, or device trait.
					</p><p>
						The following <code class="literal">libvirt</code> virtualization drivers automatically report the features that a host CPU provides, such as the type of instruction set, for example, SSE4, AVX, or AVX-512, to the placement service:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Libvirt QEMU (x86)
							</li><li class="listitem">
								Libvirt KVM (x86)
							</li><li class="listitem">
								Libvirt KVM (ppc64)
							</li></ul></div><p>
						If you are using one of these drivers, you can configure the flavor extra specs or image metadata for an instance to request a resource provider with specific CPU features.
					</p></section><section class="section" id="section-using-placement-service-traits"><div class="titlepage"><div><div><h4 class="title">7.4.3.2. Using placement service traits to specify resource provider requirements</h4></div></div></div><p>
						You can use one of the following methods to specify the required resource provider traits for an instance:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<a class="link" href="index.html#section-image-metadata" title="Procedure: Requesting a trait using image metadata">Requesting a trait using image metadata</a>
							</li><li class="listitem">
								<a class="link" href="index.html#section-flavor-extra-specs" title="Procedure: Requesting a trait using flavor extra specs">Requesting a trait using flavor extra specs</a>
							</li></ul></div><p>
						In the following example procedures, the instance requires a particular type of CPU.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The placement service package, <code class="literal">python3-osc-placement</code>, is installed on the undercloud.
							</li><li class="listitem"><p class="simpara">
								Your deployment uses one of the following <code class="literal">libvirt</code> virtualization drivers:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										Libvirt QEMU (x86)
									</li><li class="listitem">
										Libvirt KVM (x86)
									</li><li class="listitem">
										Libvirt KVM (ppc64)
									</li></ul></div></li></ul></div><div class="orderedlist" id="section-image-metadata"><p class="title"><strong>Procedure: Requesting a trait using image metadata</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new image or modify an existing one to set the required trait:
							</p><pre class="screen">$ openstack image create ... $IMAGE
$ openstack image set --property trait:HW_CPU_X86_AVX512BW=required $IMAGE</pre></li><li class="listitem"><p class="simpara">
								Boot an instance using the image:
							</p><pre class="screen">$ openstack server create --image=$IMAGE ... $SERVER_NAME</pre><p class="simpara">
								Result: The instance is created on a host that supports AVX-512.
							</p></li></ol></div><div class="orderedlist" id="section-flavor-extra-specs"><p class="title"><strong>Procedure: Requesting a trait using flavor extra specs</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a new flavor or modify an existing one to set the required trait:
							</p><pre class="screen">$ openstack flavor create ... $FLAVOR
$ openstack flavor set --property trait:HW_CPU_X86_AVX512BW=required $FLAVOR</pre></li><li class="listitem"><p class="simpara">
								Boot an instance using the flavor:
							</p><pre class="screen">$ openstack server create --flavor=$FLAVOR ... $SERVER_NAME</pre><p class="simpara">
								Result: The instance is created on a host that supports AVX-512.
							</p></li></ol></div></section></section><section class="section" id="section-guaranteed-min-bw"><div class="titlepage"><div><div><h3 class="title">7.4.4. Configuring a guaranteed minimum bandwidth QoS</h3></div></div></div><p>
					You can create instances that request a guaranteed minimum bandwidth by using a Quality of Service (QoS) policy.
				</p><p>
					QoS policies with a guaranteed minimum bandwidth rule are assigned to ports on a specific physical network. When you create an instance that uses the configured port, the Compute scheduling service selects a host for the instance that satisfies this request. The Compute scheduling service checks the Placement service for the amount of bandwidth reserved by other instances on each physical interface, before selecting a host to deploy an instance on.
				</p><div class="itemizedlist"><p class="title"><strong>Limitations/Restrictions</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You can only assign a guaranteed minimum bandwidth QoS policy when creating a new instance. You cannot assign a guaranteed minimum bandwidth QoS policy to instances that are already running, as the Compute service only updates resource usage for an instance in placement during creation or move operations, which means the minimum bandwidth available to the instance cannot be guaranteed.
						</li><li class="listitem"><p class="simpara">
							You cannot live migrate an instance that uses a port that has resource requests, such as a guaranteed minimum bandwidth QoS policy. Run the following command to check if a port has resource requests:
						</p><pre class="screen">$ openstack port show &lt;port_name/port_id&gt;</pre></li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							A QoS policy is available that has a minimum bandwidth rule. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/networking_guide/sec-qos">Configuring Quality of Service (QoS) policies</a>
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							List the available QoS policies:
						</p><pre class="screen">(overcloud) $ openstack network qos policy list</pre><pre class="screen"><code class="literal">--------------------------------------</code>---------<code class="literal">--------</code>---------+
| ID                                   | Name    | Shared | Default | Project                          |
<code class="literal">--------------------------------------</code>---------<code class="literal">--------</code>---------+
| 6d771447-3cf4-4ef1-b613-945e990fa59f | policy2 | True   | False   | ba4de51bf7694228a350dd22b7a3dc24 |
| 78a24462-e3c1-4e66-a042-71131a7daed5 | policy1 | True   | False   | ba4de51bf7694228a350dd22b7a3dc24 |
| b80acc64-4fc2-41f2-a346-520d7cfe0e2b | policy0 | True   | False   | ba4de51bf7694228a350dd22b7a3dc24 |
<code class="literal">--------------------------------------</code>---------<code class="literal">--------</code>---------+</pre></li><li class="listitem"><p class="simpara">
							Check the rules of each of the available policies to determine which has the required minimum bandwidth:
						</p><pre class="screen">(overcloud) $ openstack network qos policy show policy0</pre><pre class="screen"><code class="literal">-------------</code>---------------------------------------------------------------------------------------+
| Field       | Value                                                                                 |
<code class="literal">-------------</code>---------------------------------------------------------------------------------------+
| description |                                                                                                                                                                                                                                                                                                                                                                     |
| id          | b80acc64-4fc2-41f2-a346-520d7cfe0e2b                                                                                                                                                                                                                                                                                                                                |
| is_default  | False                                                                                                                                                                                                                                                                                                                                                               |
| location    | cloud=<span class="emphasis"><em>', project.domain_id=, project.domain_name='Default</em></span>, project.id=<span class="emphasis"><em>ba4de51bf7694228a350dd22b7a3dc24</em></span>, project.name=<span class="emphasis"><em>admin</em></span>, region_name=<span class="emphasis"><em>regionOne</em></span>, zone=                                                                                                                                                                                                    |
| name        | policy0                                                                                                                                                                                                                                                                                                                                                             |
| project_id  | ba4de51bf7694228a350dd22b7a3dc24                                                                                                                                                                                                                                                                                                                                    |
| rules       | [{<span class="emphasis"><em>min_kbps</em></span>: 100000, <span class="emphasis"><em>direction</em></span>: <span class="emphasis"><em>egress</em></span>, <span class="emphasis"><em>id</em></span>: <span class="emphasis"><em>d46218fe-9218-4e96-952b-9f45a5cb3b3c</em></span>, <span class="emphasis"><em>qos_policy_id</em></span>: <span class="emphasis"><em>b80acc64-4fc2-41f2-a346-520d7cfe0e2b</em></span>, <span class="emphasis"><em>type</em></span>: <span class="emphasis"><em>minimum_bandwidth</em></span>}, {<span class="emphasis"><em>min_kbps</em></span>: 100000, <span class="emphasis"><em>direction</em></span>: <span class="emphasis"><em>ingress</em></span>, <span class="emphasis"><em>id</em></span>: <span class="emphasis"><em>1202c4e3-a03a-464c-80d5-0bf90bb74c9d</em></span>, <span class="emphasis"><em>qos_policy_id</em></span>: <span class="emphasis"><em>b80acc64-4fc2-41f2-a346-520d7cfe0e2b</em></span>, <span class="emphasis"><em>type</em></span>: <span class="emphasis"><em>minimum_bandwidth</em></span>}] |
| shared      | True                                                                                                                                                                                                                                                                                                                                                                |
| tags        | []                                                                                                                                                                                                                                                                                                                                                                  |
<code class="literal">-------------</code>---------------------------------------------------------------------------------------+</pre></li><li class="listitem"><p class="simpara">
							Create a port from the appropriate policy:
						</p><pre class="screen">(overcloud) $ openstack port create port-normal-qos --network net0 --qos-policy policy0</pre></li><li class="listitem"><p class="simpara">
							Create an instance, specifying the NIC port to use:
						</p><pre class="screen">$ openstack server create --flavor cirros256 --image cirros-0.3.5-x86_64-disk --nic port-id=port-normal-qos --wait qos_instance</pre><p class="simpara">
							An "ACTIVE" status in the output indicates that you have successfully created the instance on a host that can provide the requested guaranteed minimum bandwidth.
						</p></li></ol></div><section class="section" id="removing_a_guaranteed_minimum_bandwidth_qos_from_an_instance"><div class="titlepage"><div><div><h4 class="title">7.4.4.1. Removing a guaranteed minimum bandwidth QoS from an instance</h4></div></div></div><p>
						If you want to lift the guaranteed minimum bandwidth QoS policy restriction from an instance, you can detach the interface.
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To detach the interface, enter the following command:
							</p><pre class="screen">$ openstack server remove port &lt;vm_name|vm_id&gt; &lt;port_name|port_id&gt;</pre></li></ol></div></section></section><section class="section" id="section-reserve-numa-nodes"><div class="titlepage"><div><div><h3 class="title">7.4.5. Reserve NUMA Nodes with PCI Devices</h3></div></div></div><p>
					Compute uses the filter scheduler to prioritize hosts with PCI devices for instances requesting PCI. The hosts are weighted using the <code class="literal">PCIWeigher</code> option, based on the number of PCI devices available on the host and the number of PCI devices requested by an instance. If an instance requests PCI devices, then the hosts with more PCI devices are allocated a higher weight than the others. If an instance is not requesting PCI devices, then prioritization does not take place.
				</p><p>
					This feature is especially useful in the following cases:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							As an operator, if you want to reserve nodes with PCI devices (typically expensive and with limited resources) for guest instances that request them.
						</li><li class="listitem">
							As a user launching instances, you want to ensure that PCI devices are available when required.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						For this value to be considered, one of the following values must be added to the <code class="literal">NovaSchedulerDefaultFilters</code> parameter in your Compute environment file: <code class="literal">PciPassthroughFilter</code> or <code class="literal">NUMATopologyFilter</code>.
					</p></div></div><p>
					The <code class="literal">pci_weight_multiplier</code> configuration option must be a positive value.
				</p></section><section class="section" id="section-schedule-emulator-threads"><div class="titlepage"><div><div><h3 class="title">7.4.6. Configure Emulator Threads to run on Dedicated Physical CPU</h3></div></div></div><p>
					The Compute scheduler determines the CPU resource utilization and places instances based on the number of virtual CPUs (vCPUs) in the flavor. There are a number of hypervisor operations that are performed on the host, on behalf of the guest instance, for example, with QEMU, there are threads used for the QEMU main event loop, asynchronous I/O operations and so on and these operations need to be accounted and scheduled separately.
				</p><p>
					The <code class="literal">libvirt</code> driver implements a generic placement policy for KVM which allows QEMU emulator threads to float across the same physical CPUs (pCPUs) that the vCPUs are running on. This leads to the emulator threads using time borrowed from the vCPUs operations. When you need a guest to have dedicated vCPU allocation, it is necessary to allocate one or more pCPUs for emulator threads. It is therefore necessary to describe to the scheduler any other CPU usage that might be associated with a guest and account for that during placement.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						In an NFV deployment, to avoid packet loss, you have to make sure that the vCPUs are never preempted.
					</p></div></div><p>
					Before you enable the emulator threads placement policy on a flavor, check that the following heat parameters are defined as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">NovaComputeCpuSharedSet</code>: Set this parameter to a list of CPUs defined to run emulator threads.
						</li><li class="listitem">
							<code class="literal">NovaSchedulerDefaultFilters</code>: Include <code class="literal">NUMATopologyFilter</code> in the list of defined filters.
						</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can define or change heat parameter values on an active cluster, and then redeploy for those changes to take effect.
					</p></div></div><p>
					To isolate emulator threads, you must use a flavor configured as follows:
				</p><pre class="screen"># openstack flavor set FLAVOR-NAME \
--property hw:cpu_policy=dedicated \
--property hw:emulator_threads_policy=share</pre></section></section><section class="section" id="section-instance-snapshots"><div class="titlepage"><div><div><h2 class="title">7.5. Manage Instance Snapshots</h2></div></div></div><p>
				An instance snapshot allows you to create a new image from an instance. This is very convenient for upgrading base images or for taking a published image and customizing it for local use.
			</p><p>
				The difference between an image that you upload directly to the Image Service and an image that you create by snapshot is that an image created by snapshot has additional properties in the Image Service database. These properties are found in the <code class="literal">image_properties</code> table and include the following parameters:
			</p><div class="table" id="idm139747273671072"><p class="title"><strong>Table 7.8. Snapshot Options</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 28%; " class="col_1"><!--Empty--></col><col style="width: 72%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747273455088" scope="col">Name</th><th align="left" valign="top" id="idm139747273454112" scope="col">Value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747273455088"> <p>
								image_type
							</p>
							 </td><td align="left" valign="top" headers="idm139747273454112"> <p>
								snapshot
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747273455088"> <p>
								instance_uuid
							</p>
							 </td><td align="left" valign="top" headers="idm139747273454112"> <p>
								&lt;uuid of instance that was snapshotted&gt;
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747273455088"> <p>
								base_image_ref
							</p>
							 </td><td align="left" valign="top" headers="idm139747273454112"> <p>
								&lt;uuid of original image of instance that was snapshotted&gt;
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm139747273455088"> <p>
								image_location
							</p>
							 </td><td align="left" valign="top" headers="idm139747273454112"> <p>
								snapshot
							</p>
							 </td></tr></tbody></table></div></div><p>
				Snapshots allow you to create new instances based on that snapshot, and potentially restore an instance to that state. Moreover, this can be performed while the instance is running.
			</p><p>
				By default, a snapshot is accessible to the users and projects that were selected while launching an instance that the snapshot is based on.
			</p><section class="section" id="section-create-instance-snapshot"><div class="titlepage"><div><div><h3 class="title">7.5.1. Create an Instance Snapshot</h3></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						If you intend to use an instance snapshot as a template to create new instances, you must ensure that the disk state is consistent. Before you create a snapshot, set the snapshot image metadata property <code class="literal">os_require_quiesce=yes</code>. For example,
					</p><pre class="screen">$ glance image-update IMAGE_ID --property os_require_quiesce=yes</pre><p>
						For this to work, the guest should have the <code class="literal">qemu-guest-agent</code> package installed, and the image should be created with the metadata property parameter <code class="literal">hw_qemu_guest_agent=yes</code> set. For example,
					</p><pre class="screen">$ glance image-create --name NAME \
--disk-format raw \
--container-format bare \
--file FILE_NAME \
--is-public True \
--property hw_qemu_guest_agent=yes \
--progress</pre><p>
						If you unconditionally enable the <code class="literal">hw_qemu_guest_agent=yes</code> parameter, then you are adding another device to the guest. This consumes a PCI slot, and will limit the number of other devices you can allocate to the guest. It also causes Windows guests to display a warning message about an unknown hardware device.
					</p><p>
						For these reasons, setting the <code class="literal">hw_qemu_guest_agent=yes</code> parameter is optional, and the parameter should be used for only those images that require the QEMU guest agent.
					</p></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Instances</strong></span>.
						</li><li class="listitem">
							Select the instance from which you want to create a snapshot.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Actions</strong></span> column, click <span class="strong strong"><strong>Create Snapshot</strong></span>.
						</li><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Create Snapshot</strong></span> dialog, enter a name for the snapshot and click <span class="strong strong"><strong>Create Snapshot</strong></span>.
						</p><p class="simpara">
							The <span class="strong strong"><strong>Images</strong></span> category now shows the instance snapshot.
						</p></li></ol></div><p>
					To launch an instance from a snapshot, select the snapshot and click <span class="strong strong"><strong>Launch</strong></span>.
				</p></section><section class="section" id="section-manage-snapshot"><div class="titlepage"><div><div><h3 class="title">7.5.2. Manage a Snapshot</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Images</strong></span>.
						</li><li class="listitem">
							All snapshots you created, appear under the <span class="strong strong"><strong>Project</strong></span> option.
						</li><li class="listitem"><p class="simpara">
							For every snapshot you create, you can perform the following functions, using the dropdown list:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Use the <span class="strong strong"><strong>Create Volume</strong></span> option to create a volume and entering the values for volume name, description, image source, volume type, size and availability zone. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/storage_guide/#section-create-volume">Create a Volume</a> in the <span class="emphasis"><em>Storage Guide</em></span>.
								</li><li class="listitem">
									Use the <span class="strong strong"><strong>Edit Image</strong></span> option to update the snapshot image by updating the values for name, description, Kernel ID, Ramdisk ID, Architecture, Format, Minimum Disk (GB), Minimum RAM (MB), public or private. For more information, see <a class="xref" href="index.html#section-update-image" title="1.2.3. Update an image">Section 1.2.3, “Update an image”</a>.
								</li><li class="listitem">
									Use the <span class="strong strong"><strong>Delete Image</strong></span> option to delete the snapshot.
								</li></ol></div></li></ol></div></section><section class="section" id="section-rebuild-instance-using-snapshot"><div class="titlepage"><div><div><h3 class="title">7.5.3. Rebuild an Instance to a State in a Snapshot</h3></div></div></div><p>
					In an event that you delete an instance on which a snapshot is based, the snapshot still stores the instance ID. You can check this information using the <span class="strong strong"><strong>nova image-list</strong></span> command and use the snapshot to restore the instance.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							In the dashboard, select <span class="strong strong"><strong>Project &gt; Compute &gt; Images</strong></span>.
						</li><li class="listitem">
							Select the snapshot from which you want to restore the instance.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Actions</strong></span> column, click <span class="strong strong"><strong>Launch Instance</strong></span>.
						</li><li class="listitem">
							In the <span class="strong strong"><strong>Launch Instance</strong></span> dialog, enter a name and the other details for the instance and click <span class="strong strong"><strong>Launch</strong></span>.
						</li></ol></div><p>
					For more information on launching an instance, see <a class="xref" href="index.html#section-launch-instance" title="7.1.2. Launch an Instance">Section 7.1.2, “Launch an Instance”</a>.
				</p></section><section class="section" id="section-create-consistent-snapshots"><div class="titlepage"><div><div><h3 class="title">7.5.4. Consistent Snapshots</h3></div></div></div><p>
					Previously, file systems had to be quiesced manually (fsfreeze) before taking a snapshot of active instances for consistent backups.
				</p><p>
					Compute’s <code class="literal">libvirt</code> driver automatically requests the <span class="emphasis"><em>QEMU Guest Agent</em></span> to freeze the file systems (and applications if <code class="literal">fsfreeze-hook</code> is installed) during an image snapshot. Support for quiescing file systems enables scheduled, automatic snapshots at the block device level.
				</p><p>
					This feature is only valid if the QEMU Guest Agent is installed (<code class="literal">qemu-ga</code>) and the image metadata enables the agent (<code class="literal">hw_qemu_guest_agent=yes</code>)
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						Snapshots should not be considered a substitute for an actual system backup.
					</p></div></div></section></section><section class="section" id="section-instance-rescue"><div class="titlepage"><div><div><h2 class="title">7.6. Use Rescue Mode for Instances</h2></div></div></div><p>
				Compute has a method to reboot a virtual machine in rescue mode. Rescue mode provides a mechanism for access when the virtual machine image renders the instance inaccessible. A rescue virtual machine allows a user to fix their virtual machine by accessing the instance with a new root password. This feature is useful if an instance’s filesystem is corrupted. By default, rescue mode starts an instance from the initial image attaching the current boot disk as a secondary one.
			</p><section class="section" id="preparing_an_image_for_a_rescue_mode_instance"><div class="titlepage"><div><div><h3 class="title">7.6.1. Preparing an Image for a Rescue Mode Instance</h3></div></div></div><p>
					Due to the fact that both the boot disk and the disk for rescue mode have same UUID, sometimes the virtual machine can be booted from the boot disk instead of the disk for rescue mode.
				</p><p>
					To avoid this issue, you should create a new image as rescue image based on the procedure in <a class="xref" href="index.html#section-create-images" title="1.2.1. Creating an Image">Section 1.2.1, “Creating an Image”</a>:
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">rescue</code> image is stored in <code class="literal">glance</code> and configured in the <code class="literal">nova.conf</code> as a default, or you can select when you do the rescue.
					</p></div></div><section class="section" id="rescue_image_if_using_emphasis_ext4_emphasis_filesystem"><div class="titlepage"><div><div><h4 class="title">7.6.1.1. Rescue Image if Using <span class="emphasis"><em>ext4</em></span> Filesystem</h4></div></div></div><p>
						When the base image uses <code class="literal">ext4</code> filesystem, you can create a rescue image from it using the following procedure:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Change the <span class="emphasis"><em>UUID</em></span> to a random value using the <code class="literal">tune2fs</code> command:
							</p><pre class="screen"># tune2fs -U random /dev/DEVICE_NODE</pre><p class="simpara">
								Here <span class="emphasis"><em>DEVICE_NODE</em></span> is the root device node (for example, <code class="literal">sda</code>, <code class="literal">vda</code>, and so on).
							</p></li><li class="listitem"><p class="simpara">
								Verify the details of the filesystem, including the new <span class="emphasis"><em>UUID</em></span>:
							</p><pre class="screen"># tune2fs -l</pre></li><li class="listitem">
								Update the <code class="literal">/etc/fstab</code> to use the new <span class="emphasis"><em>UUID</em></span>. You may need to repeat this for any additional partitions you have, that are mounted in the <code class="literal">fstab</code> by <span class="emphasis"><em>UUID</em></span>.
							</li><li class="listitem">
								Update the <code class="literal">/boot/grub2/grub.conf</code> file and update the <span class="emphasis"><em>UUID</em></span> parameter with the new <span class="emphasis"><em>UUID</em></span> of the root disk.
							</li><li class="listitem">
								Shut down and use this image as your rescue image. This will cause the rescue image to have a new random <span class="emphasis"><em>UUID</em></span> that will not conflict with the instance that you are rescuing.
							</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The XFS filesystem cannot change the UUID of the root device on the running virtual machine. Reboot the virtual machine until the virtual machine is launched from the disk for rescue mode.
						</p></div></div></section></section><section class="section" id="adding_the_rescue_image_to_the_openstack_image_service"><div class="titlepage"><div><div><h3 class="title">7.6.2. Adding the Rescue Image to the OpenStack Image Service</h3></div></div></div><p>
					When you have completed modifying the UUID of your image, use the following commands to add the generated rescue image to the OpenStack Image service:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add the rescue image to the Image service:
						</p><pre class="screen"># glance image-create --name IMAGE_NAME --disk-format qcow2 \
  --container-format bare --is-public True --file IMAGE_PATH</pre><p class="simpara">
							Here <span class="emphasis"><em>IMAGE_NAME</em></span> is the name of the image, <span class="emphasis"><em>IMAGE_PATH</em></span> is the location of the image.
						</p></li><li class="listitem"><p class="simpara">
							Use the <code class="literal">image-list</code> command to obtain the <span class="emphasis"><em>IMAGE_ID</em></span> required for launching an instace in the rescue mode.
						</p><pre class="screen"># glance image-list</pre></li></ol></div><p>
					You can also upload an image using the OpenStack Dashboard, see <a class="xref" href="index.html#section-upload-image" title="1.2.2. Upload an image">Section 1.2.2, “Upload an image”</a>.
				</p></section><section class="section" id="launching_an_instance_in_rescue_mode"><div class="titlepage"><div><div><h3 class="title">7.6.3. Launching an Instance in Rescue Mode</h3></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Since you need to rescue an instance with a specific image, rather than the default one, use the <code class="literal">--image</code> parameter:
						</p><pre class="screen"># nova rescue --image IMAGE_ID VIRTUAL_MACHINE_ID</pre><p class="simpara">
							Here <span class="emphasis"><em>IMAGE_ID</em></span> is the ID of the image you want to use and <span class="emphasis"><em>VIRTUAL_MACHINE_ID</em></span> is ID of a virtual machine that you want to rescue.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The <code class="literal">nova rescue</code> command allows an instance to perform a soft shut down. This allows the guest operating system to perform a controlled shutdown before the instance is powered off. The shut down behavior is configured using <code class="literal">shutdown_timeout</code> in your Compute configuration file. The value stands for the overall period (in seconds) a guest operation system is allowed to complete the shutdown. The default timeout is 60 seconds.
							</p><p>
								The timeout value can be overridden on a per image basis by means of <code class="literal">os_shutdown_timeout</code> that is an image metadata setting allowing different types of operating systems to specify how much time they need to shut down cleanly.
							</p></div></div></li><li class="listitem">
							Reboot the virtual machine.
						</li><li class="listitem">
							Confirm the status of the virtual machine is <span class="emphasis"><em>RESCUE</em></span> on the controller node by using <code class="literal">nova list</code> command or by using dashboard.
						</li><li class="listitem">
							Log in to the new virtual machine dashboard by using the password for rescue mode.
						</li></ol></div><p>
					You can now make the necessary changes to your instance to fix any issues.
				</p></section><section class="section" id="unrescuing_an_instance"><div class="titlepage"><div><div><h3 class="title">7.6.4. Unrescuing an Instance</h3></div></div></div><p>
					You can <code class="literal">unrescue</code> the fixed instance to restart it from the boot disk.
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Execute the following commands on the controller node.
						</p><pre class="screen"># nova unrescue VIRTUAL_MACHINE_ID</pre><p class="simpara">
							Here <span class="emphasis"><em>VIRTUAL_MACHINE_ID</em></span> is ID of a virtual machine that you want to unrescue.
						</p></li></ol></div><p>
					The status of your instance returns to <span class="emphasis"><em>ACTIVE</em></span> once the unrescue operation has completed successfully.
				</p></section></section><section class="section" id="ch-configdrive"><div class="titlepage"><div><div><h2 class="title">7.7. Set a Configuration Drive for Instances</h2></div></div></div><p>
				You can use the <code class="literal">config-drive</code> parameter to present a read-only drive to your instances. This drive can contain selected files that are then accessible to the instance. The configuration drive is attached to the instance at boot, and is presented to the instance as a partition. Configuration drives are useful when combined with <span class="emphasis"><em>cloud-init</em></span> (for server bootstrapping), and when you want to pass large files to your instances.
			</p><section class="section" id="configuration_drive_options"><div class="titlepage"><div><div><h3 class="title">7.7.1. Configuration Drive Options</h3></div></div></div><p>
					Use your Compute environment file to set the following configuration drive parameters:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							<code class="literal">config_drive_format</code> - sets the format of the drive, and accepts the options <code class="literal">iso9660</code> and <code class="literal">vfat</code>. By default, it uses <code class="literal">iso9660</code>.
						</li><li class="listitem">
							<code class="literal">force_config_drive</code> - this forces the configuration drive to be presented to all instances. Set to "True".
						</li><li class="listitem">
							<code class="literal">mkisofs_cmd</code> - specifies the command to use for ISO file creation. This value must not be changed, as only <span class="emphasis"><em>genisoimage</em></span> is supported.
						</li></ul></div></section><section class="section" id="use_a_configuration_drive"><div class="titlepage"><div><div><h3 class="title">7.7.2. Use a Configuration Drive</h3></div></div></div><p>
					An instance attaches its configuration drive at boot time. This is enabled by the <code class="literal">--config-drive</code> option. For example, this command creates a new instance named <span class="emphasis"><em>test-instance01</em></span> and attaches a drive containing a file named <span class="emphasis"><em>/root/user-data.txt</em></span>:
				</p><pre class="screen"># nova boot --flavor m1.tiny --config-drive true --file /root/user-data.txt=/root/user-data.txt --image cirros test-instance01</pre><p>
					Once the instance has booted, you can log in to it and see a file named <span class="emphasis"><em>/root/user-data.txt</em></span>.
				</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can use the configuration drive as a source for <span class="emphasis"><em>cloud-init</em></span> information. During the initial instance boot, <span class="emphasis"><em>cloud-init</em></span> can automatically mount the configuration drive and run the setup scripts.
					</p></div></div></section></section></section><section class="chapter" id="ch-compute-performance"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Configuring Compute nodes for performance</h1></div></div></div><p>
			You can configure the scheduling and placement of instances for optimal performance by creating customized flavors to target specialized workloads, including NFV and High Performance Computing (HPC).
		</p><p>
			Use the following features to tune your instances for optimal performance:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					CPU pinning: Pin virtual CPUs to physical CPUs.
				</li><li class="listitem">
					Emulator threads: Pin emulator threads associated with the instance to physical CPUs.
				</li><li class="listitem">
					Huge pages: Tune instance memory allocation policies both for normal memory (4k pages) and huge pages (2 MB or 1 GB pages).
				</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Configuring any of these features creates an implicit NUMA topology on the instance if there is no NUMA topology already present.
			</p></div></div><section class="section" id="ch-cpu_pinning"><div class="titlepage"><div><div><h2 class="title">8.1. Configuring CPU pinning on the Compute node</h2></div></div></div><p>
				You can configure instances to run on dedicated host CPUs. Enabling CPU pinning implicitly configures a guest NUMA topology. Each NUMA node of this NUMA topology maps to a separate host NUMA node. For more information about NUMA, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/network_functions_virtualization_product_guide/ch-nfv_tuning_for_performance#c_cpu-numa">CPUs and NUMA nodes</a> in the <span class="emphasis"><em>Network Functions Virtualization Product Guide</em></span>.
			</p><p>
				Configure CPU pinning on your Compute node based on the NUMA topology of your host system. Reserve some CPU cores across all the NUMA nodes for the host processes for efficiency. Assign the remaining CPU cores to managing your instances.
			</p><p>
				The following example illustrates eight CPU cores spread across two NUMA nodes.
			</p><div class="table" id="idm139747270317392"><p class="title"><strong>Table 8.1. Example of NUMA Topology</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><tbody><tr><td colspan="2" align="left" valign="top"> <p>
								NUMA Node 0
							</p>
							 </td><td colspan="2" align="left" valign="top"> <p>
								NUMA Node 1
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								Core 0
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 1
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 2
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 3
							</p>
							 </td></tr><tr><td align="left" valign="top"> <p>
								Core 4
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 5
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 6
							</p>
							 </td><td align="left" valign="top"> <p>
								Core 7
							</p>
							 </td></tr></tbody></table></div></div><p>
				You can schedule dedicated (pinned) and shared (unpinned) instances on the same Compute node. The following procedure reserves cores 0 and 4 for host processes, cores 1, 3, 5 and 7 for instances that require CPU pinning, and cores 2 and 6 for floating instances that do not require CPU pinning.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If the host supports simultaneous multithreading (SMT), group thread siblings together in either the dedicated or the shared set. Thread siblings share some common hardware which means it is possible for a process running on one thread sibling to impact the performance of the other thread sibling.
				</p><p>
					For example, the host identifies four CPUs in a dual core CPU with SMT: 0, 1, 2, and 3. Of these four, there are two pairs of thread siblings:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Thread sibling 1: CPUs 0 and 2
						</li><li class="listitem">
							Thread sibling 2: CPUs 1 and 3
						</li></ul></div><p>
					In this scenario, you should not assign CPUs 0 and 1 as dedicated and 2 and 3 as shared. Instead, you should assign 0 and 2 as dedicated and 1 and 3 as shared.
				</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisite</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You know the NUMA topology of your Compute node. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/network_functions_virtualization_planning_and_configuration_guide/ch-hardware-requirements#proc_finding-numa-topology">Discovering your NUMA node topology</a> in the <span class="emphasis"><em>Network Functions Virtualization Planning and Configuration Guide</em></span>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Reserve physical CPU cores for the dedicated instances by setting the <code class="literal">NovaComputeCpuDedicatedSet</code> configuration in the Compute environment file for each Compute node:
					</p><pre class="screen">NovaComputeCpuDedicatedSet: 1,3,5,7</pre></li><li class="listitem"><p class="simpara">
						Reserve physical CPU cores for the shared instances by setting the <code class="literal">NovaComputeCpuSharedSet</code> configuration in the Compute environment file for each Compute node:
					</p><pre class="screen">NovaComputeCpuSharedSet: 2,6</pre></li><li class="listitem"><p class="simpara">
						Set the <code class="literal">NovaReservedHostMemory</code> option in the same files to the amount of RAM to reserve for host processes. For example, if you want to reserve 512 MB, use:
					</p><pre class="screen">NovaReservedHostMemory: 512</pre></li><li class="listitem"><p class="simpara">
						To ensure that host processes do not run on the CPU cores reserved for instances, set the parameter <code class="literal">IsolCpusList</code> in each Compute environment file to the CPU cores you have reserved for instances. Specify the value of the <code class="literal">IsolCpusList</code> parameter using a list, or ranges, of CPU indices separated by a whitespace.
					</p><pre class="screen">IsolCpusList: 1 2 3 5 6 7</pre></li><li class="listitem">
						To filter out hosts based on its NUMA topology, add <code class="literal">NUMATopologyFilter</code> to the <code class="literal">NovaSchedulerDefaultFilters</code> parameter in each Compute environment file.
					</li><li class="listitem"><p class="simpara">
						To apply this configuration, add the environment file(s) to your deployment command and deploy the overcloud:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files]
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml</pre></li></ol></div><section class="section" id="upgrading_cpu_pinning_configuration"><div class="titlepage"><div><div><h3 class="title">8.1.1. Upgrading CPU pinning configuration</h3></div></div></div><p>
					From Red Hat OpenStack Platform (RHOSP) 16+ it is not necessary to use host aggregates to ensure dedicated (pinned) and shared (unpinned) instance types run on separate hosts. Also, the <code class="literal">[DEFAULT] reserved_host_cpus</code> config option is no longer necessary and can be unset.
				</p><p>
					To upgrade your CPU pinning configuration from earlier versions of RHOSP:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Migrate the value of <code class="literal">NovaVcpuPinSet</code> to <code class="literal">NovaComputeCpuDedicatedSet</code> for hosts that were previously used for pinned instances.
						</li><li class="listitem">
							Migrate the value of <code class="literal">NovaVcpuPinSet</code> to <code class="literal">NovaComputeCpuSharedSet</code> for hosts that were previously used for unpinned instances.
						</li><li class="listitem">
							If there is no value set for <code class="literal">NovaVcpuPinSet</code>, then all host cores should be assigned to either <code class="literal">NovaComputeCpuDedicatedSet</code> or <code class="literal">NovaComputeCpuSharedSet</code>, depending on the type of instance running there.
						</li></ul></div><p>
					Once the upgrade is complete, it is possible to start setting both options on the same host. However, to do this, all the instances should be migrated from the host, as the Compute service cannot start when cores for an unpinned instance are not listed in <code class="literal">NovaComputeCpuSharedSet</code>, or when cores for a pinned instance are not listed in <code class="literal">NovaComputeCpuDedicatedSet</code>.
				</p></section><section class="section" id="launching_an_instance_with_cpu_pinning"><div class="titlepage"><div><div><h3 class="title">8.1.2. Launching an instance with CPU pinning</h3></div></div></div><p>
					You can launch an instance that uses CPU pinning by specifying a flavor with a dedicated CPU policy.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							Simultaneous multithreading (SMT) is enabled on the host.
						</li><li class="listitem">
							The Compute node is configured to allow CPU pinning. For more information, see <a class="link" href="index.html#ch-cpu_pinning" title="8.1. Configuring CPU pinning on the Compute node">Configuring CPU pinning on the Compute node</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a flavor for instances that require CPU pinning:
						</p><pre class="screen">(overcloud) $ openstack flavor create --ram &lt;size-mb&gt; --disk &lt;size-gb&gt; --vcpus &lt;no_reserved_vcpus&gt; pinned_cpus</pre></li><li class="listitem"><p class="simpara">
							To request pinned CPUs, set the <code class="literal">hw:cpu_policy</code> property of the flavor to <code class="literal">dedicated</code>:
						</p><pre class="screen">(overcloud) $ openstack flavor set --property hw:cpu_policy=dedicated pinned_cpus</pre></li><li class="listitem"><p class="simpara">
							To place each vCPU on thread siblings, set the <code class="literal">hw:cpu_thread_policy</code> property of the flavor to <code class="literal">require</code>:
						</p><pre class="screen">(overcloud) $ openstack flavor set --property hw:cpu_thread_policy=require pinned_cpus</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										If the host does not have an SMT architecture or enough CPU cores with available thread siblings, scheduling will fail. To prevent this, set <code class="literal">hw:cpu_thread_policy</code> to <code class="literal">prefer</code> instead of <code class="literal">require</code>. The (default) <code class="literal">prefer</code> policy ensures that thread siblings are used when available.
									</li><li class="listitem">
										If you use <code class="literal">cpu_thread_policy=isolate</code>, you must have SMT disabled or use a platform that does not support SMT.
									</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
							Create an instance using the new flavor:
						</p><pre class="screen">(overcloud) $ openstack server create --flavor pinned_cpus --image &lt;image&gt; pinned_cpu_instance</pre></li><li class="listitem"><p class="simpara">
							To verify correct placement of the new instance, run the following command and check for <code class="literal">OS-EXT-SRV-ATTR:hypervisor_hostname</code> in the output:
						</p><pre class="screen">(overcloud) $ openstack server show pinned_cpu_instance</pre></li></ol></div></section><section class="section" id="launching_a_floating_instance"><div class="titlepage"><div><div><h3 class="title">8.1.3. Launching a floating instance</h3></div></div></div><p>
					You can launch an instance that is placed on a floating CPU by specifying a flavor with a shared CPU policy.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Compute node is configured to reserve physical CPU cores for the floating instances. For more information, see <a class="link" href="index.html#ch-cpu_pinning" title="8.1. Configuring CPU pinning on the Compute node">Configuring CPU pinning on the Compute node</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a flavor for instances that do not require CPU pinning:
						</p><pre class="screen">(overcloud) $ openstack flavor create --ram &lt;size-mb&gt; --disk &lt;size-gb&gt; --vcpus &lt;no_reserved_vcpus&gt; floating_cpus</pre></li><li class="listitem"><p class="simpara">
							To request floating CPUs, set the <code class="literal">hw:cpu_policy</code> property of the flavor to <code class="literal">shared</code>:
						</p><pre class="screen">(overcloud) $ openstack flavor set --property hw:cpu_policy=shared floating_cpus</pre></li><li class="listitem"><p class="simpara">
							Create an instance using the new flavor:
						</p><pre class="screen">(overcloud) $ openstack server create --flavor floating_cpus --image &lt;image&gt; floating_cpu_instance</pre></li><li class="listitem"><p class="simpara">
							To verify correct placement of the new instance, run the following command and check for <code class="literal">OS-EXT-SRV-ATTR:hypervisor_hostname</code> in the output:
						</p><pre class="screen">(overcloud) $ openstack server show floating_cpu_instance</pre></li></ol></div></section></section><section class="section" id="ch-huge-pages"><div class="titlepage"><div><div><h2 class="title">8.2. Configuring huge pages on the Compute node</h2></div></div></div><p>
				Configure the Compute node to enable instances to request huge pages.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Configure the amount of huge page memory to reserve on each NUMA node for processes that are not instances:
					</p><pre class="screen">parameter_defaults:
  NovaReservedHugePages: ["node:0,size:2048,count:64","node:1,size:1GB,count:1"]</pre><p class="simpara">
						Where:
					</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 30%; " class="col_1"><!--Empty--></col><col style="width: 70%; " class="col_2"><!--Empty--></col></colgroup><tbody><tr><td align="left" valign="top"> <p>
										Attribute
									</p>
									 </td><td align="left" valign="top"> <p>
										Description
									</p>
									 </td></tr><tr><td align="left" valign="top"> <p>
										size
									</p>
									 </td><td align="left" valign="top"> <p>
										The size of the allocated huge page. Valid values: * 2048 (for 2MB) * 1GB
									</p>
									 </td></tr><tr><td align="left" valign="top"> <p>
										count
									</p>
									 </td><td align="left" valign="top"> <p>
										The number of huge pages used by OVS per NUMA node. For example, for 4096 of socket memory used by Open vSwitch, set this to 2.
									</p>
									 </td></tr></tbody></table></div></li><li class="listitem"><p class="simpara">
						(Optional) To allow instances to allocate 1GB huge pages, configure the CPU feature flags, <code class="literal">cpu_model_extra_flags</code>, to include "pdpe1gb":
					</p><pre class="screen">parameter_defaults:
   ComputeExtraConfig:
     nova::compute::libvirt::libvirt_cpu_mode: 'custom'
     nova::compute::libvirt::libvirt_cpu_model: 'Haswell-noTSX'
     nova::compute::libvirt::libvirt_cpu_model_extra_flags: 'vmx, pdpe1gb'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									CPU feature flags do not need to be configured to allow instances to only request 2 MB huge pages.
								</li><li class="listitem">
									You can only allocate 1G huge pages to an instance if the host supports 1G huge page allocation.
								</li><li class="listitem">
									You only need to set <code class="literal">cpu_model_extra_flags</code> to <code class="literal">pdpe1gb</code> when <code class="literal">cpu_mode</code> is set to <code class="literal">host-model</code> or <code class="literal">custom</code>.
								</li><li class="listitem">
									If the host supports <code class="literal">pdpe1gb</code>, and <code class="literal">host-passthrough</code> is used as the <code class="literal">cpu_mode</code>, then you do not need to set <code class="literal">pdpe1gb</code> as a <code class="literal">cpu_model_extra_flags</code>. The <code class="literal">pdpe1gb</code> flag is only included in Opteron_G4 and Opteron_G5 CPU models, it is not included in any of the Intel CPU models supported by QEMU.
								</li><li class="listitem">
									To mitigate for CPU hardware issues, such as Microarchitectural Data Sampling (MDS), you might need to configure other CPU flags. For more information, see <a class="link" href="https://access.redhat.com/solutions/4161561">RHOS Mitigation for MDS ("Microarchitectural Data Sampling") Security Flaws</a>.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						To avoid loss of performance after applying Meltdown protection, configure the CPU feature flags, <code class="literal">cpu_model_extra_flags</code>, to include "+pcid":
					</p><pre class="screen">parameter_defaults:
   ComputeExtraConfig:
     nova::compute::libvirt::libvirt_cpu_mode: 'custom'
     nova::compute::libvirt::libvirt_cpu_model: 'Haswell-noTSX'
     nova::compute::libvirt::libvirt_cpu_model_extra_flags: 'vmx, pdpe1gb, +pcid'</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						For more information, see <a class="link" href="https://access.redhat.com/solutions/3370461">Reducing the performance impact of <span class="emphasis"><em>Meltdown</em></span> CVE fixes for OpenStack guests with "PCID" CPU feature flag</a>.
					</p></div></div></li><li class="listitem">
						Add <code class="literal">NUMATopologyFilter</code> to the <code class="literal">NovaSchedulerDefaultFilters</code> parameter in each Compute environment file, if not already present.
					</li><li class="listitem"><p class="simpara">
						Apply this huge page configuration by adding the environment file(s) to your deployment command and deploying the overcloud:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files]
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml</pre></li></ol></div><section class="section" id="allocating_huge_pages_to_instances"><div class="titlepage"><div><div><h3 class="title">8.2.1. Allocating huge pages to instances</h3></div></div></div><p>
					Create a flavor with the <code class="literal">hw:mem_page_size</code> extra specification key to specify that the instance should use huge pages.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							The Compute node is configured for huge pages. For more information, see <a class="link" href="index.html#ch-huge-pages" title="8.2. Configuring huge pages on the Compute node">Configuring huge pages on the Compute node</a>.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a flavor for instances that require huge pages:
						</p><pre class="screen">$ openstack flavor create --ram &lt;size-mb&gt; --disk &lt;size-gb&gt; --vcpus &lt;no_reserved_vcpus&gt; huge_pages</pre></li><li class="listitem"><p class="simpara">
							Set the flavor for huge pages:
						</p><pre class="screen">$ openstack flavor set huge_pages --property hw:mem_page_size=1GB</pre><p class="simpara">
							Valid values for <code class="literal">hw:mem_page_size</code>:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">large</code> - Selects the largest page size supported on the host, which may be 2 MB or 1 GB on x86_64 systems.
								</li><li class="listitem">
									<code class="literal">small</code> - (Default) Selects the smallest page size supported on the host. On x86_64 systems this is 4 kB (normal pages).
								</li><li class="listitem">
									<code class="literal">any</code> - Selects the largest available huge page size, as determined by the libvirt driver.
								</li><li class="listitem">
									&lt;pagesize&gt;: (string) Set an explicit page size if the workload has specific requirements. Use an integer value for the page size in KB, or any standard suffix. For example: 4KB, 2MB, 2048, 1GB.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Create an instance using the new flavor:
						</p><pre class="screen">$ openstack server create --flavor huge_pages --image &lt;image&gt; huge_pages_instance</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Validation</strong></p><p>
						The scheduler identifies a host with enough free huge pages of the required size to back the memory of the instance. If the scheduler is unable to find a host and NUMA node with enough pages, then the request will fail with a NoValidHost error.
					</p></div></section></section><section class="section" id="configure-file-backed-memory"><div class="titlepage"><div><div><h2 class="title">8.3. Configuring Compute nodes to use file-backed memory for instances</h2></div></div></div><p>
				You can use file-backed memory to expand your Compute node memory capacity, by allocating files within the libvirt memory backing directory as instance memory. You can configure the amount of host disk that is available for instance memory, and the location on the disk of the instance memory files.
			</p><p>
				The OpenStack Compute service reports the capacity configured for file-backed memory to the Placement service as the total system memory capacity. This allows the Compute node to run more instances than would normally fit within the system memory.
			</p><p>
				To use file-backed memory for instances, you must enable file-backed memory on the Compute node.
			</p><div class="itemizedlist"><p class="title"><strong>Limitations</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You cannot live migrate instances between Compute nodes that have file-backed memory enabled and Compute nodes that do not have file-backed memory enabled.
					</li><li class="listitem">
						File-backed memory is not compatible with huge pages. Instances that use huge pages cannot start on a Compute node with file-backed memory enabled. Use host aggregates to ensure that instances that use huge pages are not placed on Compute nodes with file-backed memory enabled.
					</li><li class="listitem">
						File-backed memory is not compatible with memory overcommit.
					</li><li class="listitem">
						You cannot reserve memory for host processes using <code class="literal">NovaReservedHostMemory</code>. When file-backed memory is in use, reserved memory corresponds to disk space not set aside for file-backed memory. File-backed memory is reported to the Placement service as the total system memory, with RAM used as cache memory.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">NovaRAMAllocationRatio</code> must be set to "1.0" on the node and any host aggregate the node is added to.
					</li><li class="listitem">
						<code class="literal">NovaReservedHostMemory</code> must be set to "0".
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Configure the amount of host disk space, in MiB, to make available for instance RAM, by adding the following parameter to your Compute environment file:
					</p><pre class="screen">parameter_defaults:
  NovaLibvirtFileBackedMemory: 102400</pre></li><li class="listitem"><p class="simpara">
						Optional: To configure the directory to store the memory backing files, set the <code class="literal">QemuMemoryBackingDir</code> parameter in your Compute environment file. If not set, the memory backing directory defaults to <code class="literal">/var/lib/libvirt/qemu/ram/</code>.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You must locate your backing store in a directory at or above the default directory location, <code class="literal">/var/lib/libvirt/qemu/ram/</code>.
						</p></div></div><p class="simpara">
						You can also change the host disk for the backing store. For more information, see <a class="xref" href="index.html#change-host-disk" title="8.3.1. Changing the memory backing directory host disk">Section 8.3.1, “Changing the memory backing directory host disk”</a>.
					</p></li><li class="listitem">
						Save the updates to your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						To apply this configuration, add your Compute environment file to the stack with your other environment files and deploy the overcloud:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files] \
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml</pre></li></ol></div><section class="section" id="change-host-disk"><div class="titlepage"><div><div><h3 class="title">8.3.1. Changing the memory backing directory host disk</h3></div></div></div><p>
					You can move the memory backing directory from the default primary disk location to an alternative disk.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a file system on the alternative backing device. For example, enter the following command to create an <code class="literal">ext4</code> filesystem on <code class="literal">/dev/sdb</code>:
						</p><pre class="screen"># mkfs.ext4 /dev/sdb</pre></li><li class="listitem"><p class="simpara">
							Mount the backing device. For example, enter the following command to mount <code class="literal">/dev/sdb</code> on the default libvirt memory backing directory:
						</p><pre class="screen"># mount /dev/sdb /var/lib/libvirt/qemu/ram</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								The mount point must match the value of the <code class="literal">QemuMemoryBackingDir</code> parameter.
							</p></div></div></li></ol></div></section></section></section><section class="chapter" id="memory-encryption-for-instances"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Configuring SEV-capable Compute nodes to provide memory encryption for instances</h1></div></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
				This feature is available in this release as a <span class="emphasis"><em>Technology Preview</em></span>, and therefore is not fully supported by Red Hat. It should only be used for testing, and should not be deployed in a production environment. For more information about Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/production/scope_moredetail">Scope of Coverage Details</a>.
			</p></div></div><p>
			As a cloud administrator, you can provide cloud users the ability to create instances that run on SEV-capable Compute nodes with memory encryption enabled.
		</p><p>
			To enable your cloud users to create instances that use memory encryption, you must complete the following procedures:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Configure the Compute nodes that have the SEV-capable hardware.
				</li><li class="listitem">
					Create a SEV-enabled flavor or image for launching instances.
				</li></ul></div><section class="section" id="sev"><div class="titlepage"><div><div><h2 class="title">9.1. Secure Encrypted Virtualization (SEV)</h2></div></div></div><p>
				Secure Encrypted Virtualization (SEV), provided by AMD, protects the data in DRAM that a running virtual machine instance is using. SEV encrypts the memory of each instance with a unique key.
			</p><p>
				SEV increases security when you use non-volatile memory technology (NVDIMM), because an NVDIMM chip can be physically removed from a system with the data intact, similar to a hard drive. Without encryption, any stored information such as sensitive data, passwords, or secret keys can be compromised.
			</p><p>
				For more information, see the <a class="link" href="https://developer.amd.com/sev/"><span class="emphasis"><em>AMD Secure Encrypted Virtualization (SEV)</em></span></a> documentation.
			</p><div class="itemizedlist"><p class="title"><strong>Limitations of SEV-encrypted instances</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You cannot live migrate, or suspend and resume SEV-encrypted instances.
					</li><li class="listitem">
						You cannot use PCI passthrough on SEV-encrypted instances to directly access devices.
					</li><li class="listitem"><p class="simpara">
						You cannot use virtio-blk as the boot disk of SEV-encrypted instances.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You can use virtio-scsi or SATA as the boot disk, or virtio-blk for non-boot disks.
						</p></div></div></li><li class="listitem">
						The operating system running in an encrypted instance must contain SEV support.
					</li><li class="listitem">
						Machines that support SEV have a limited number of slots in their memory controller for storing encryption keys. Each running instance with encrypted memory consumes one of these slots. Therefore, the number of SEV instances that can run concurrently is limited to the number of slots in the memory controller. For example, on AMD EPYC Zen 1 the limit is 16, and on AMD EPYC Zen 2, the limit is 255.
					</li><li class="listitem">
						Memory-encrypted instances pin pages in RAM. The Compute service cannot swap these pages, therefore you cannot safely overcommit a Compute node that hosts memory-encrypted instances.
					</li></ul></div></section><section class="section" id="configure-node-for-SEV"><div class="titlepage"><div><div><h2 class="title">9.2. Configuring a SEV-capable Compute node</h2></div></div></div><p>
				To enable your cloud users to create instances that use memory encryption, you must configure the Compute nodes that have the SEV-capable hardware.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
						Your deployment must include a Compute node that runs on AMD hardware capable of supporting SEV, such as an AMD EPYC CPU. You can use the following command to determine if your deployment is SEV-capable:
					</p><pre class="screen">$ lscpu | grep sev</pre></li><li class="listitem">
						Your deployment must include libvirt 4.5 or later, which includes support for SEV.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Optional: Add the following configuration to your Compute environment file to specify the maximum number of memory-encrypted instances the SEV-capable Compute node can host concurrently:
					</p><pre class="screen">parameter_defaults:
    ComputeExtraConfig:
        nova::config::nova_config:
            libvirt/num_memory_encrypted_guests:
                value: 15</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If not set, <code class="literal">libvirt/num_memory_encrypted_guests</code> defaults to <code class="literal">none</code>, which means the SEV-capable Compute node does not impose a limit on the number of memory-encrypted instances that can be hosted concurrently. Instead, the hardware determines the maximum number of memory-encrypted instances the SEV-capable Compute node can host concurrently, which might cause some memory-encrypted instances to fail to launch.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Optional: To specify that all x86_64 images use the q35 machine type by default, add the <code class="literal">NovaHWMachineType</code> parameter to the Compute environment file, and set it to <code class="literal">x86_64=q35</code>.
					</p><p class="simpara">
						This configuration removes the need to set the <code class="literal">hw_machine_type</code> property to <code class="literal">q35</code> on every SEV-enabled instance image.
					</p></li><li class="listitem">
						To prevent memory overcommit, set the <code class="literal">NovaRAMAllocationRatio</code> parameter to <code class="literal">1.0</code> in the Compute environment file.
					</li><li class="listitem">
						To ensure that the SEV-capable Compute nodes reserve enough memory for host-level services to function, add 16MB for each potential SEV instance (the maximum number of concurrent SEV instances), to your value for <code class="literal">NovaReservedHostMemory</code> in the Compute environment file.
					</li><li class="listitem"><p class="simpara">
						Add the following configuration to your Compute environment file to schedule memory-encrypted instances on a SEV-capable Compute node aggregate:
					</p><pre class="screen">parameter_defaults:
    ControllerExtraConfig:
        nova::config::nova_config:
            scheduler/enable_isolated_aggregate_filtering:
                value: 'True'</pre></li><li class="listitem">
						Save the updates to your Compute environment file.
					</li><li class="listitem"><p class="simpara">
						To apply this configuration, add your Compute environment file to the stack with your other environment files and deploy the overcloud:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files]
  -e /home/stack/templates/&lt;compute_environment_file&gt;.yaml</pre></li><li class="listitem"><p class="simpara">
						Create a host aggregate for SEV Compute nodes to ensure that instances that do not request memory encryption are not created on SEV-capable hosts:
					</p><pre class="screen">(undercloud) $ source ~/overcloudrc
(overcloud) $ openstack aggregate create sev_agg
(overcloud) $ openstack aggregate add host sev_agg hostA
(overcloud) $ openstack aggregate add host sev_agg hostB
(overcloud) $ openstack --os-compute-api-version 2.53 aggregate set --property trait:HW_CPU_X86_AMD_SEV=required sev_agg</pre></li></ol></div></section><section class="section" id="configure-flavor-and-image-for-SEV"><div class="titlepage"><div><div><h2 class="title">9.3. Creating the image and flavor for memory encryption</h2></div></div></div><p>
				To enable your cloud users to create instances that use memory encryption, you can define a SEV-enabled flavor, and you can create a SEV-enabled image.
			</p><section class="section" id="configure-image-for-SEV"><div class="titlepage"><div><div><h3 class="title">9.3.1. Creating a SEV-enabled image for instances</h3></div></div></div><p>
					When the overcloud contains SEV-capable Compute nodes, you can create a SEV-enabled instance image that your cloud users can use to launch instances that have memory encryption.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a new image for SEV:
						</p><pre class="screen">(overcloud) $ openstack image create ... --property hw_firmware_type=uefi sev-image</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								If you use an existing image, the image must have the <code class="literal">hw_firmware_type</code> property set to <code class="literal">uefi</code>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: Add the property <code class="literal">hw_mem_encryption=True</code> to the image to enable SEV memory encryption on the image:
						</p><pre class="screen">(overcloud) $ openstack image set --property hw_mem_encryption=True sev-image</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can enable SEV memory encryption on the flavor. For more information, see <a class="link" href="index.html#configure-flavor-for-SEV" title="9.3.2. Creating a SEV-enabled flavor for instances">Creating a SEV-enabled flavor for instances</a>.
						</p></div></div></li><li class="listitem"><p class="simpara">
							Optional: Set the machine type to <code class="literal">q35</code>, if not already set in the Compute node configuration:
						</p><pre class="screen">(overcloud) $ openstack image set --property hw_machine_type=q35 sev-image</pre></li><li class="listitem"><p class="simpara">
							Optional: To schedule memory-encrypted instances on a SEV-capable host aggregate, add the following trait to the image extra specs:
						</p><pre class="screen">(overcloud) $ openstack image set --property trait:HW_CPU_X86_AMD_SEV=required sev-image</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
							You can also specify this trait on the flavor. For more information, see <a class="link" href="index.html#configure-flavor-for-SEV" title="9.3.2. Creating a SEV-enabled flavor for instances">Creating a SEV-enabled flavor for instances</a>.
						</p></div></div></li></ol></div></section><section class="section" id="configure-flavor-for-SEV"><div class="titlepage"><div><div><h3 class="title">9.3.2. Creating a SEV-enabled flavor for instances</h3></div></div></div><p>
					When the overcloud contains SEV-capable Compute nodes, you can create one or more SEV-enabled flavors that your cloud users can use to launch instances that have memory encryption.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create a flavor for SEV:
						</p><pre class="screen">(overcloud) $ openstack flavor create --vcpus 1 --ram 512 --disk 2 --property hw:mem_encryption=True m1.small-sev</pre></li><li class="listitem"><p class="simpara">
							To schedule memory-encrypted instances on a SEV-capable host aggregate, add the following trait to the flavor extra specs:
						</p><pre class="screen">(overcloud) $ openstack flavor set --property trait:HW_CPU_X86_AMD_SEV=required m1.small-sev</pre></li></ol></div></section></section></section><section class="chapter" id="migrating-virtual-machines-between-compute-nodes"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Migrating virtual machine instances between Compute nodes</h1></div></div></div><p>
			You sometimes need to migrate instances from one Compute node to another Compute node in the overcloud, to perform maintenance, rebalance the workload, or replace a failed or failing node.
		</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Compute node maintenance</span></dt><dd>
						If you need to temporarily take a Compute node out of service, for instance, to perform hardware maintenance or repair, kernel upgrades and software updates, you can migrate instances running on the Compute node to another Compute node.
					</dd><dt><span class="term">Failing Compute node</span></dt><dd>
						If a Compute node is about to fail and you need to service it or replace it, you can migrate instances from the failing Compute node to a healthy Compute node.
					</dd><dt><span class="term">Failed Compute nodes</span></dt><dd>
						If a Compute node has already failed, you can evacuate the instances. You can rebuild instances from the original image on another Compute node, using the same name, UUID, network addresses, and any other allocated resources the instance had before the Compute node failed.
					</dd><dt><span class="term">Workload rebalancing</span></dt><dd>
						You can migrate one or more instances to another Compute node to rebalance the workload. For example, you can consolidate instances on a Compute node to conserve power, migrate instances to a Compute node that is physically closer to other networked resources to reduce latency, or distribute instances across Compute nodes to avoid hot spots and increase resiliency.
					</dd></dl></div><p>
			Director configures all Compute nodes to provide secure migration. All Compute nodes also require a shared SSH key to provide the users of each host with access to other Compute nodes during the migration process. Director creates this key using the <code class="literal">OS::TripleO::Services::NovaCompute</code> composable service. This composable service is one of the main services included on all Compute roles by default. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/chap-roles">Composable Services and Custom Roles</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				If you have a functioning Compute node, and you want to make a copy of an instance for backup purposes, or to copy the instance to a different environment, follow the procedure in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#importing-virtual-machines-into-the-overcloud">Importing virtual machines into the overcloud</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
			</p></div></div><section class="section" id="migration-types"><div class="titlepage"><div><div><h2 class="title">10.1. Migration types</h2></div></div></div><p>
				Red Hat OpenStack Platform (RHOSP) supports the following types of migration.
			</p><div class="formalpara"><p class="title"><strong>Cold migration</strong></p><p>
					Cold migration, or non-live migration, involves shutting down a running instance before migrating it from the source Compute node to the destination Compute node.
				</p></div><div class="informalfigure"><div class="mediaobject"><img src="OpenStack_libvirt_Migration_11_0419_cold.png" alt="Cold Migration"/></div></div><p>
				Cold migration involves some downtime for the instance. The migrated instance maintains access to the same volumes and IP addresses.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Cold migration requires that both the source and destination Compute nodes are running.
				</p></div></div><div class="formalpara"><p class="title"><strong>Live migration</strong></p><p>
					Live migration involves moving the instance from the source Compute node to the destination Compute node without shutting it down, and while maintaining state consistency.
				</p></div><div class="informalfigure"><div class="mediaobject"><img src="OpenStack_libvirt_Migration_11_0419_live.png" alt="Live Migration"/></div></div><p>
				Live migrating an instance involves little or no perceptible downtime. However, live migration does impact performance for the duration of the migration operation. Therefore, instances should be taken out of the critical path while being migrated.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Live migration requires that both the source and destination Compute nodes are running.
				</p></div></div><p>
				In some cases, instances cannot use live migration. For more information, see <a class="link" href="index.html#migration-constraints" title="10.2. Migration constraints">Migration Constraints</a>.
			</p><div class="formalpara"><p class="title"><strong>Evacuation</strong></p><p>
					If you need to migrate instances because the source Compute node has already failed, you can evacuate the instances.
				</p></div></section><section class="section" id="migration-constraints"><div class="titlepage"><div><div><h2 class="title">10.2. Migration constraints</h2></div></div></div><p>
				Migration constraints typically arise with block migration, configuration disks, or when one or more instances access physical hardware on the Compute node.
			</p><div class="formalpara"><p class="title"><strong>CPU constraints</strong></p><p>
					The source and destination Compute nodes must have the same CPU architecture. For example, Red Hat does not support migrating an instance from an <code class="literal">x86_64</code> CPU to a <code class="literal">ppc64le</code> CPU. In some cases, the CPU of the source and destination Compute node must match exactly, such as instances that use CPU host passthrough. In all cases, the CPU features of the destination node must be a superset of the CPU features on the source node.
				</p></div><div class="formalpara"><p class="title"><strong>Memory constraints</strong></p><p>
					The destination Compute node must have sufficient available RAM. Memory oversubscription can cause migration to fail.
				</p></div><div class="formalpara"><p class="title"><strong>Block migration constraints</strong></p><p>
					Migrating instances that use disks that are stored locally on a Compute node takes significantly longer than migrating volume-backed instances that use shared storage, such as Red Hat Ceph Storage. This latency arises because OpenStack Compute (nova) migrates local disks block-by-block between the Compute nodes over the control plane network by default. By contrast, volume-backed instances that use shared storage, such as Red Hat Ceph Storage, do not have to migrate the volumes, because each Compute node already has access to the shared storage.
				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Network congestion in the control plane network caused by migrating local disks or instances that consume large amounts of RAM might impact the performance of other systems that use the control plane network, such as RabbitMQ.
				</p></div></div><div class="formalpara"><p class="title"><strong>Read-only drive migration constraints</strong></p><p>
					Migrating a drive is supported only if the drive has both read and write capabilities. For example, OpenStack Compute (nova) cannot migrate a CD-ROM drive or a read-only config drive. However, OpenStack Compute (nova) can migrate a drive with both read and write capabilities, including a config drive with a drive format such as <code class="literal">vfat</code>.
				</p></div><div class="formalpara"><p class="title"><strong>Live migration constraints</strong></p><p>
					In some cases, live migrating instances involves additional constraints.
				</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">No new operations during migration</span></dt><dd>
							To achieve state consistency between the copies of the instance on the source and destination nodes, RHOSP must prevent new operations during live migration. Otherwise, live migration might take a long time or potentially never end if writes to memory occur faster than live migration can replicate the state of the memory.
						</dd><dt><span class="term">CPU pinning with NUMA</span></dt><dd>
							<code class="literal">NovaSchedulerDefaultFilters</code> parameter in the Compute configuration must include the values <code class="literal">AggregateInstanceExtraSpecsFilter</code> and <code class="literal">NUMATopologyFilter</code>.
						</dd><dt><span class="term">Multi-cell clouds</span></dt><dd>
							In a multi-cell cloud, instances can be live migrated to a different host in the same cell, but not across cells.
						</dd><dt><span class="term">Floating instances</span></dt><dd>
							When live migrating floating instances, if the configuration of <code class="literal">NovaComputeCpuSharedSet</code> on the destination Compute node is different from the configuration of <code class="literal">NovaComputeCpuSharedSet</code> on the source Compute node, the instances will not be allocated to the CPUs configured for shared (unpinned) instances on the destination Compute node. Therefore, if you need to live migrate floating instances, you must configure all the Compute nodes with the same CPU mappings for dedicated (pinned) and shared (unpinned) instances, or use a host aggregate for the shared instances.
						</dd><dt><span class="term">Destination Compute node capacity</span></dt><dd>
							The destination Compute node must have sufficient capacity to host the instance that you want to migrate.
						</dd><dt><span class="term">SR-IOV live migration</span></dt><dd>
							Instances with SR-IOV-based network interfaces can be live migrated. Live migrating instances with direct mode SR-IOV network interfaces attached incurs network downtime while the direct mode interfaces are being detached and re-attached.
						</dd></dl></div><div class="formalpara"><p class="title"><strong>Constraints that preclude live migration</strong></p><p>
					You cannot live migrate an instance that uses the following features.
				</p></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">PCI passthrough</span></dt><dd>
							QEMU/KVM hypervisors support attaching PCI devices on the Compute node to an instance. Use PCI passthrough to give an instance exclusive access to PCI devices, which appear and behave as if they are physically attached to the operating system of the instance. However, because PCI passthrough involves physical addresses, OpenStack Compute does not support live migration of instances using PCI passthrough.
						</dd><dt><span class="term">Port resource requests</span></dt><dd><p class="simpara">
							You cannot live migrate an instance that uses a port that has resource requests, such as a guaranteed minimum bandwidth QoS policy. Use the following command to check if a port has resource requests:
						</p><pre class="screen">$ openstack port show &lt;port_name/port_id&gt;</pre></dd></dl></div></section><section class="section" id="pre-migration-procedures"><div class="titlepage"><div><div><h2 class="title">10.3. Preparing to migrate</h2></div></div></div><p>
				Before you migrate one or more instances, you need to determine the Compute node names and the IDs of the instances to migrate.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Identify the source Compute node host name and the destination Compute node host name:
					</p><pre class="screen">(undercloud) $ source ~/overcloudrc
(overcloud) $ openstack compute service list</pre></li><li class="listitem"><p class="simpara">
						List the instances on the source Compute node and locate the ID of the instance or instances that you want to migrate:
					</p><pre class="screen">(overcloud) $ openstack server list --host &lt;source&gt; --all-projects</pre><p class="simpara">
						Replace <code class="literal">&lt;source&gt;</code> with the name or ID of the source Compute node.
					</p></li><li class="listitem"><p class="simpara">
						Optional: If you are migrating instances from a source Compute node to perform maintenance on the node, you must disable the node to prevent the scheduler from assigning new instances to the node during maintenance:
					</p><pre class="screen">(overcloud) $ source ~/stackrc
(undercloud) $ openstack compute service set &lt;source&gt; nova-compute --disable</pre><p class="simpara">
						Replace <code class="literal">&lt;source&gt;</code> with the name or ID of the source Compute node.
					</p></li></ol></div><p>
				You are now ready to perform the migration. Follow the required procedure detailed in <a class="link" href="index.html#cold-migrate-a-vm" title="10.4. Cold migrating an instance">Cold migrating an instance</a> or <a class="link" href="index.html#live-migrate-a-vm" title="10.5. Live migrating an instance">Live migrating an instance</a>.
			</p></section><section class="section" id="cold-migrate-a-vm"><div class="titlepage"><div><div><h2 class="title">10.4. Cold migrating an instance</h2></div></div></div><p>
				Cold migrating an instance involves stopping the instance and moving it to another Compute node. Cold migration facilitates migration scenarios that live migrating cannot facilitate, such as migrating instances that use PCI passthrough. The scheduler automatically selects the destination Compute node. For more information, see <a class="link" href="index.html#migration-constraints" title="10.2. Migration constraints">Migration Constraints</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To cold migrate an instance, enter the following command to power off and move the instance:
					</p><pre class="screen">(overcloud) $ openstack server migrate &lt;vm&gt; --wait</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance to migrate.
							</li><li class="listitem">
								Specify the <code class="literal">--block-migration</code> flag if migrating a locally stored volume.
							</li></ul></div></li><li class="listitem">
						Wait for migration to complete. While you wait for the instance migration to complete, you can check the migration status. For more information, see <a class="link" href="index.html#check-migration-status" title="10.6. Checking migration status">Checking migration status</a>.
					</li><li class="listitem"><p class="simpara">
						Check the status of the instance:
					</p><pre class="screen">(overcloud) $ openstack server list --all-projects</pre><p class="simpara">
						A status of "VERIFY_RESIZE" indicates you need to confirm or revert the migration:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								If the migration worked as expected, confirm it:
							</p><pre class="screen">(overcloud) $ openstack server resize --confirm &lt;vm&gt;`</pre><p class="simpara">
								Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance to migrate. A status of "ACTIVE" indicates that the instance is ready to use.
							</p></li><li class="listitem"><p class="simpara">
								If the migration did not work as expected, revert it:
							</p><pre class="screen">(overcloud) $ openstack server resize --revert &lt;vm&gt;`</pre><p class="simpara">
								Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Restart the instance:
					</p><pre class="screen">(overcloud) $ openstack server start &lt;vm&gt;</pre><p class="simpara">
						Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
					</p></li><li class="listitem"><p class="simpara">
						Optional: If you disabled the source Compute node for maintenance, you must re-enable the node so that new instances can be assigned to it:
					</p><pre class="screen">(overcloud) $ source ~/stackrc
(undercloud) $ openstack compute service set &lt;source&gt; nova-compute --enable</pre><p class="simpara">
						Replace <code class="literal">&lt;source&gt;</code> with the host name of the source Compute node.
					</p></li></ol></div></section><section class="section" id="live-migrate-a-vm"><div class="titlepage"><div><div><h2 class="title">10.5. Live migrating an instance</h2></div></div></div><p>
				Live migration moves an instance from a source Compute node to a destination Compute node with a minimal amount of downtime. Live migration might not be appropriate for all instances. For more information, see <a class="link" href="index.html#migration-constraints" title="10.2. Migration constraints">Migration Constraints</a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To live migrate an instance, specify the instance and the destination Compute node:
					</p><pre class="screen">(overcloud) $ openstack server migrate &lt;vm&gt; --live-migration [--host &lt;dest&gt;] --wait</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
							</li><li class="listitem"><p class="simpara">
								Replace <code class="literal">&lt;dest&gt;</code> with the name or ID of the destination Compute node.
							</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
									The <code class="literal">openstack server migrate</code> command covers migrating instances with shared storage, which is the default. Specify the <code class="literal">--block-migration</code> flag to migrate a locally stored volume:
								</p><pre class="screen">(overcloud) $ openstack server migrate &lt;vm&gt; --live-migration [--host &lt;dest&gt;] --wait --block-migration</pre></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Confirm that the instance is migrating:
					</p><pre class="screen">(overloud) $ openstack server show &lt;vm&gt;

+----------------------+--------------------------------------+
| Field                | Value                                |
+----------------------+--------------------------------------+
| ...                  | ...                                  |
| status               | MIGRATING                            |
| ...                  | ...                                  |
+----------------------+--------------------------------------+</pre></li><li class="listitem">
						Wait for migration to complete. While you wait for the instance migration to complete, you can check the migration status. For more information, see <a class="link" href="index.html#check-migration-status" title="10.6. Checking migration status">Checking migration status</a>.
					</li><li class="listitem"><p class="simpara">
						Check the status of the instance to confirm if the migration was successful:
					</p><pre class="screen">(overcloud) $ openstack server list --host &lt;dest&gt; --all-projects</pre><p class="simpara">
						Replace <code class="literal">&lt;dest&gt;</code> with the name or ID of the destination Compute node.
					</p></li><li class="listitem"><p class="simpara">
						Optional: If you disabled the source Compute node for maintenance, you must re-enable the node so that new instances can be assigned to it:
					</p><pre class="screen">(overcloud) $ source ~/stackrc
(undercloud) $ openstack compute service set &lt;source&gt; nova-compute --enable</pre><p class="simpara">
						Replace <code class="literal">&lt;source&gt;</code> with the host name of the source Compute node.
					</p></li></ol></div></section><section class="section" id="check-migration-status"><div class="titlepage"><div><div><h2 class="title">10.6. Checking migration status</h2></div></div></div><p>
				Migration involves several state transitions before migration is complete. During a healthy migration, the migration state typically transitions as follows:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						<span class="strong strong"><strong>Queued:</strong></span> The Compute service has accepted the request to migrate an instance, and migration is pending.
					</li><li class="listitem">
						<span class="strong strong"><strong>Preparing:</strong></span> The Compute service is preparing to migrate the instance.
					</li><li class="listitem">
						<span class="strong strong"><strong>Running:</strong></span> The Compute service is migrating the instance.
					</li><li class="listitem">
						<span class="strong strong"><strong>Post-migrating:</strong></span> The Compute service has built the instance on the destination Compute node and is releasing resources on the source Compute node.
					</li><li class="listitem">
						<span class="strong strong"><strong>Completed:</strong></span> The Compute service has completed migrating the instance and finished releasing resources on the source Compute node.
					</li></ol></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Retrieve the list of migration IDs for the instance:
					</p><pre class="screen">$ nova server-migration-list &lt;vm&gt;

+----+-------------+-----------  (...)
| Id | Source Node | Dest Node | (...)
+----+-------------+-----------+ (...)
| 2  | -           | -         | (...)
+----+-------------+-----------+ (...)</pre><p class="simpara">
						Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
					</p></li><li class="listitem"><p class="simpara">
						Show the status of the migration:
					</p><pre class="screen">$  &lt;vm&gt; &lt;migration-id&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
							</li><li class="listitem"><p class="simpara">
								Replace <code class="literal">&lt;migration-id&gt;</code> with the ID of the migration.
							</p><p class="simpara">
								Running the <code class="literal">nova server-migration-show</code> command returns the following example output:
							</p><pre class="screen">+------------------------+--------------------------------------+
| Property               | Value                                |
+------------------------+--------------------------------------+
| created_at             | 2017-03-08T02:53:06.000000           |
| dest_compute           | controller                           |
| dest_host              | -                                    |
| dest_node              | -                                    |
| disk_processed_bytes   | 0                                    |
| disk_remaining_bytes   | 0                                    |
| disk_total_bytes       | 0                                    |
| id                     | 2                                    |
| memory_processed_bytes | 65502513                             |
| memory_remaining_bytes | 786427904                            |
| memory_total_bytes     | 1091379200                           |
| server_uuid            | d1df1b5a-70c4-4fed-98b7-423362f2c47c |
| source_compute         | compute2                             |
| source_node            | -                                    |
| status                 | running                              |
| updated_at             | 2017-03-08T02:53:47.000000           |
+------------------------+--------------------------------------+</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
								The OpenStack Compute service measures progress of the migration by the number of remaining memory bytes to copy. If this number does not decrease over time, the migration might be unable to complete, and the Compute service might abort it.
							</p></div></div></li></ul></div></li></ol></div><p>
				Sometimes instance migration can take a long time or encounter errors. For more information, see <a class="link" href="index.html#troubleshooting-migration" title="10.8. Troubleshooting migration">Troubleshooting migration</a>.
			</p></section><section class="section" id="evacuate-a-vm"><div class="titlepage"><div><div><h2 class="title">10.7. Evacuating an instance</h2></div></div></div><p>
				If you want to move an instance from a dead or shut-down Compute node to a new host in the same environment, you can evacuate it.
			</p><p>
				The evacuate process destroys the original instance and rebuilds it on another Compute node using the original image, instance name, UUID, network addresses, and any other resources the original instance had allocated to it.
			</p><p>
				If the instance uses shared storage, the instance root disk is not rebuilt during the evacuate process, as the disk remains accessible by the destination Compute node. If the instance does not use shared storage, then the instance root disk is also rebuilt on the destination Compute node.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You can only perform an evacuation when the Compute node is fenced, and the API reports that the state of the Compute node is "down" or "forced-down". If the Compute node is not reported as "down" or "forced-down", the <code class="literal">evacuate</code> command fails.
						</li><li class="listitem">
							To perform an evacuation, you must be a cloud administrator.
						</li></ul></div></div></div><section class="section" id="evacuate-one-vm"><div class="titlepage"><div><div><h3 class="title">10.7.1. Evacuating one instance</h3></div></div></div><p>
					You can evacuate instances one at a time.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log onto the failed Compute node as an administrator.
						</li><li class="listitem"><p class="simpara">
							Disable the Compute node:
						</p><pre class="screen">(overcloud) [stack@director ~]$ openstack compute service set \
&lt;host&gt; &lt;service&gt; --disable</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;host&gt;</code> with the name of the Compute node to evacuate the instance from.
								</li><li class="listitem">
									Replace <code class="literal">&lt;service&gt;</code> with the name of the service to disable, for example <code class="literal">nova-compute</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							To evacuate an instance, enter the following command:
						</p><pre class="screen">(overcloud) [stack@director ~]$ nova evacuate [--password &lt;pass&gt;] &lt;vm&gt; [&lt;dest&gt;]</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;pass&gt;</code> with the admin password to set for the evacuated instance. If a password is not specified, a random password is generated and output when the evacuation is complete.
								</li><li class="listitem">
									Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance to evacuate.
								</li><li class="listitem"><p class="simpara">
									Replace <code class="literal">&lt;dest&gt;</code> with the name of the Compute node to evacuate the instance to. If you do not specify the destination Compute node, the Compute scheduler selects one for you. You can find possible Compute nodes by using the following command:
								</p><pre class="screen">(overcloud) [stack@director ~]$ openstack hypervisor list</pre></li></ul></div></li></ol></div></section><section class="section" id="evacuate-all-vms"><div class="titlepage"><div><div><h3 class="title">10.7.2. Evacuating all instances on a host</h3></div></div></div><p>
					You can evacuate all instances on a specified Compute node.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Log onto the failed Compute node as an administrator.
						</li><li class="listitem"><p class="simpara">
							Disable the Compute node:
						</p><pre class="screen">(overcloud) [stack@director ~]$ openstack compute service set \
&lt;host&gt; &lt;service&gt; --disable</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;host&gt;</code> with the name of the Compute node to evacuate the instances from.
								</li><li class="listitem">
									Replace <code class="literal">&lt;service&gt;</code> with the name of the service to disable, for example <code class="literal">nova-compute</code>.
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Evacuate all instances on a specified Compute node:
						</p><pre class="screen">(overcloud) [stack@director ~]$ nova host-evacuate [--target_host &lt;dest&gt;] [--force] &lt;host&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
									Replace <code class="literal">&lt;dest&gt;</code> with the name of the destination Compute node to evacuate the instances to. If you do not specify the destination, the Compute scheduler selects one for you. You can find possible Compute nodes by using the following command:
								</p><pre class="screen">(overcloud) [stack@director ~]$ openstack hypervisor list</pre></li><li class="listitem">
									Replace <code class="literal">&lt;host&gt;</code> with the name of the Compute node to evacuate the instances from.
								</li></ul></div></li></ol></div></section><section class="section" id="cfg-shared-storage"><div class="titlepage"><div><div><h3 class="title">10.7.3. Configuring shared storage</h3></div></div></div><p>
					If you are using shared storage, export the instance directory for the Compute service to the two nodes, and ensure that the nodes have access. The directory path is set in the <code class="literal">state_path</code> and <code class="literal">instances_path</code> parameters in your Compute environment file. This procedure uses the default value, which is <code class="literal">/var/lib/nova/instances</code>. Only users with root access can set up shared storage. The Compute service user in the following procedure must be the same across Controller and Compute nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Perform the following steps on the Controller node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Ensure that the <code class="literal">/var/lib/nova/instances</code> directory has read-write access by the Compute service user, as shown in the following example:
								</p><pre class="screen">drwxr-xr-x.  9 nova nova 4096 Nov  5 20:37 instances</pre></li><li class="listitem"><p class="simpara">
									Add the following lines to the <code class="literal">/etc/exports</code> file:
								</p><pre class="screen">/var/lib/nova/instances node1_IP(rw,sync,fsid=0,no_root_squash)
/var/lib/nova/instances node2_IP(rw,sync,fsid=0,no_root_squash)</pre><p class="simpara">
									Replace <code class="literal">node1_IP</code> and <code class="literal">node2_IP</code> for the IP addresses of the two Compute nodes, for example:
								</p><pre class="screen">/var/lib/nova/instances 192.168.24.9(rw,sync,fsid=0,no_root_squash)
/var/lib/nova/instances 192.168.24.21(rw,sync,fsid=0,no_root_squash)</pre></li><li class="listitem"><p class="simpara">
									Export the <code class="literal">/var/lib/nova/instances</code> directory to the Compute nodes:
								</p><pre class="screen"># exportfs -avr</pre></li><li class="listitem"><p class="simpara">
									Restart the NFS server:
								</p><pre class="screen"># systemctl restart nfs-server</pre></li></ol></div></li><li class="listitem"><p class="simpara">
							Perform the following steps on each Compute node:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
									Ensure that the <code class="literal">/var/lib/nova/instances</code> directory exists locally.
								</li><li class="listitem"><p class="simpara">
									Add the following line to the <code class="literal">/etc/fstab</code> file:
								</p><pre class="screen">NFS_SHARE_PATH:/var/lib/nova/instances /var/lib/nova/instances nfs4 defaults 0 0</pre></li><li class="listitem"><p class="simpara">
									Mount the controller’s instance directory to mount all the devices listed in <code class="literal">/etc/fstab</code>:
								</p><pre class="screen"># mount -a -v</pre></li><li class="listitem"><p class="simpara">
									Ensure that QEMU can access the directory’s images:
								</p><pre class="screen"># ls -ld /var/lib/nova/instances
drwxr-xr-x. 9 nova nova 4096 Nov  5 20:37 /var/lib/nova/instances</pre></li><li class="listitem"><p class="simpara">
									Ensure that the node can see the instances directory with:
								</p><pre class="screen">drwxr-xr-x. 9 nova nova 4096 Nov  5 20:37 /var/lib/nova/instances</pre></li></ol></div></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can also run the following to view all mounted devices:
					</p><pre class="screen"># df -k</pre></div></div></section></section><section class="section" id="troubleshooting-migration"><div class="titlepage"><div><div><h2 class="title">10.8. Troubleshooting migration</h2></div></div></div><p>
				The following issues can arise during instance migration:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The migration process encounters errors.
					</li><li class="listitem">
						The migration process never ends.
					</li><li class="listitem">
						Performance of the instance degrades after migration.
					</li></ul></div><section class="section" id="errors_during_migration"><div class="titlepage"><div><div><h3 class="title">10.8.1. Errors during migration</h3></div></div></div><p>
					The following issues can send the migration operation into an <code class="literal">error</code> state:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Running a cluster with different versions of Red Hat OpenStack Platform (RHOSP).
						</li><li class="listitem">
							Specifying an instance ID that cannot be found.
						</li><li class="listitem">
							The instance you are trying to migrate is in an <code class="literal">error</code> state.
						</li><li class="listitem">
							The Compute service is shutting down.
						</li><li class="listitem">
							A race condition occurs.
						</li><li class="listitem">
							Live migration enters a <code class="literal">failed</code> state.
						</li></ul></div><p>
					When live migration enters a <code class="literal">failed</code> state, it is typically followed by an <code class="literal">error</code> state. The following common issues can cause a <code class="literal">failed</code> state:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							A destination Compute host is not available.
						</li><li class="listitem">
							A scheduler exception occurs.
						</li><li class="listitem">
							The rebuild process fails due to insufficient computing resources.
						</li><li class="listitem">
							A server group check fails.
						</li><li class="listitem">
							The instance on the source Compute node gets deleted before migration to the destination Compute node is complete.
						</li></ul></div></section><section class="section" id="never_ending_live_migration"><div class="titlepage"><div><div><h3 class="title">10.8.2. Never-ending live migration</h3></div></div></div><p>
					Live migration can fail to complete, which leaves migration in a perpetual <code class="literal">running</code> state. A common reason for a live migration that never completes is that client requests to the instance running on the source Compute node create changes that occur faster than the Compute service can replicate them to the destination Compute node.
				</p><p>
					Use one of the following methods to address this situation:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Abort the live migration.
						</li><li class="listitem">
							Force the live migration to complete.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Aborting live migration</strong></p><p>
						If the instance state changes faster than the migration procedure can copy it to the destination node, and you do not want to temporarily suspend the instance operations, you can abort the live migration.
					</p></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the list of migrations for the instance:
						</p><pre class="screen">$ nova server-migration-list &lt;vm&gt;</pre><p class="simpara">
							Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
						</p></li><li class="listitem"><p class="simpara">
							Abort the live migration:
						</p><pre class="screen">$ nova live-migration-abort &lt;vm&gt; &lt;migration-id&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
								</li><li class="listitem">
									Replace <code class="literal">&lt;migration-id&gt;</code> with the ID of the migration.
								</li></ul></div></li></ol></div><div class="formalpara"><p class="title"><strong>Forcing live migration to complete</strong></p><p>
						If the instance state changes faster than the migration procedure can copy it to the destination node, and you want to temporarily suspend the instance operations to force migration to complete, you can force the live migration procedure to complete.
					</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
						Forcing live migration to complete might lead to perceptible downtime.
					</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the list of migrations for the instance:
						</p><pre class="screen">$ nova server-migration-list &lt;vm&gt;</pre><p class="simpara">
							Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
						</p></li><li class="listitem"><p class="simpara">
							Force the live migration to complete:
						</p><pre class="screen">$ nova live-migration-force-complete &lt;vm&gt; &lt;migration-id&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;vm&gt;</code> with the name or ID of the instance.
								</li><li class="listitem">
									Replace <code class="literal">&lt;migration-id&gt;</code> with the ID of the migration.
								</li></ul></div></li></ol></div></section><section class="section" id="instance_performance_degrades_after_migration"><div class="titlepage"><div><div><h3 class="title">10.8.3. Instance performance degrades after migration</h3></div></div></div><p>
					For instances that use a NUMA topology, the source and destination Compute nodes must have the same NUMA topology and configuration. The NUMA topology of the destination Compute node must have sufficient resources available. If the NUMA configuration between the source and destination Compute nodes is not the same, it is possible that live migration succeeds while the instance performance degrades. For example, if the source Compute node maps NIC 1 to NUMA node 0, but the destination Compute node maps NIC 1 to NUMA node 5, after migration the instance might route network traffic from a first CPU across the bus to a second CPU with NUMA node 5 to route traffic to NIC 1. This can result in expected behavior, but degraded performance. Similarly, if NUMA node 0 on the source Compute node has sufficient available CPU and RAM, but NUMA node 0 on the destination Compute node already has instances using some of the resources, the instance might run correctly but suffer performance degradation. For more information, see <a class="link" href="index.html#migration-constraints" title="10.2. Migration constraints">Migration constraints</a>.
				</p></section></section></section><section class="chapter" id="ch-virtual_gpu"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Configuring virtual GPUs for instances</h1></div></div></div><p>
			To support GPU-based rendering on your instances, you can define and manage virtual GPU (vGPU) resources according to your available physical GPU devices and your hypervisor type. You can use this configuration to divide the rendering workloads between all your physical GPU devices more effectively, and to have more control over scheduling your vGPU-enabled instances.
		</p><p>
			To enable vGPU in OpenStack Compute, create flavors that your cloud users can use to create Red Hat Enterprise Linux (RHEL) instances with vGPU devices. Each instance can then support GPU workloads with virtual GPU devices that correspond to the physical GPU devices.
		</p><p>
			The OpenStack Compute service tracks the number of vGPU devices that are available for each GPU profile you define on each host. The Compute service schedules instances to these hosts based on the flavor, attaches the devices, and monitors usage on an ongoing basis. When an instance is deleted, the Compute service adds the vGPU devices back to the available pool.
		</p><section class="section" id="vgpu-supported-config-limitations"><div class="titlepage"><div><div><h2 class="title">11.1. Supported configurations and limitations</h2></div></div></div><div class="formalpara"><p class="title"><strong>Supported GPU cards</strong></p><p>
					For a list of supported NVIDIA GPU cards, see <a class="link" href="https://docs.nvidia.com/grid/latest/product-support-matrix/index.html">Virtual GPU Software Supported Products</a> on the NVIDIA website.
				</p></div><div class="itemizedlist"><p class="title"><strong>Limitations when using vGPU devices</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You can enable only one vGPU type on each Compute node.
					</li><li class="listitem">
						Each instance can use only one vGPU resource.
					</li><li class="listitem">
						Live migration of vGPU between hosts is not supported.
					</li><li class="listitem">
						Suspend operations on a vGPU-enabled instance is not supported due to a libvirt limitation. Instead, you can snapshot or shelve the instance.
					</li><li class="listitem">
						Resize and cold migration operations on an instance with a vGPU flavor does not automatically re-allocate the vGPU resources to the instance. After you resize or migrate the instance, you must rebuild it manually to re-allocate the vGPU resources.
					</li><li class="listitem">
						By default, vGPU types on Compute hosts are not exposed to API users. To grant access, add the hosts to a host aggregate. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/creating-and-managing-host-aggregates">Creating and managing host aggregates</a>.
					</li><li class="listitem">
						If you use NVIDIA accelerator hardware, you must comply with the NVIDIA licensing requirements. For example, NVIDIA vGPU GRID requires a licensing server. For more information about the NVIDIA licensing requirements, see <a class="link" href="https://docs.nvidia.com/grid/latest/grid-license-server-release-notes/index.html">NVIDIA License Server Release Notes</a> on the NVIDIA website.
					</li></ul></div></section><section class="section" id="vgpu-nvidia-overview"><div class="titlepage"><div><div><h2 class="title">11.2. Configuring vGPU on the Compute nodes</h2></div></div></div><p>
				To enable your cloud users to create instances that use a virtual GPU (vGPU), you must configure the Compute nodes that have the physical GPUs:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Build a custom GPU-enabled overcloud image.
					</li><li class="listitem">
						Prepare the GPU role, profile, and flavor for designating Compute nodes for vGPU.
					</li><li class="listitem">
						Configure the Compute node for vGPU.
					</li><li class="listitem">
						Deploy the overcloud.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					To use an NVIDIA GRID vGPU, you must comply with the NVIDIA GRID licensing requirements and you must have the URL of your self-hosted license server. For more information, see the <a class="link" href="https://docs.nvidia.com/grid/latest/grid-license-server-release-notes/index.html">NVIDIA License Server Release Notes</a> web page.
				</p></div></div><section class="section" id="nvidia-build-overcloud-image"><div class="titlepage"><div><div><h3 class="title">11.2.1. Building a custom GPU overcloud image</h3></div></div></div><p>
					Perform the following steps on the director node to install the NVIDIA GRID host driver on an overcloud Compute image and upload the image to the OpenStack Image Service (glance).
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Copy the overcloud image and add the <code class="literal">gpu</code> suffix to the copied image.
						</p><pre class="screen">$ cp overcloud-full.qcow2 overcloud-full-gpu.qcow2</pre></li><li class="listitem"><p class="simpara">
							Install an ISO image generator tool from YUM.
						</p><pre class="screen">$ sudo yum install genisoimage -y</pre></li><li class="listitem"><p class="simpara">
							Download the NVIDIA GRID host driver RPM package that corresponds to your GPU device from the NVIDIA website. To determine which driver you need, see the <a class="link" href="https://www.nvidia.com/Download/index.aspx?lang=en-us">NVIDIA Driver Downloads Portal</a>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								You must be a registered NVIDIA customer to download the drivers from the portal.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create an ISO image from the driver RPM package and save the image in the <code class="literal">nvidia-host</code> directory.
						</p><pre class="screen">$ genisoimage -o nvidia-host.iso -R -J -V NVIDIA nvidia-host/
I: -input-charset not specified, using utf-8 (detected in locale settings)
  9.06% done, estimate finish Wed Oct 31 11:24:46 2018
 18.08% done, estimate finish Wed Oct 31 11:24:46 2018
 27.14% done, estimate finish Wed Oct 31 11:24:46 2018
 36.17% done, estimate finish Wed Oct 31 11:24:46 2018
 45.22% done, estimate finish Wed Oct 31 11:24:46 2018
 54.25% done, estimate finish Wed Oct 31 11:24:46 2018
 63.31% done, estimate finish Wed Oct 31 11:24:46 2018
 72.34% done, estimate finish Wed Oct 31 11:24:46 2018
 81.39% done, estimate finish Wed Oct 31 11:24:46 2018
 90.42% done, estimate finish Wed Oct 31 11:24:46 2018
 99.48% done, estimate finish Wed Oct 31 11:24:46 2018
Total translation table size: 0
Total rockridge attributes bytes: 358
Total directory bytes: 0
Path table size(bytes): 10
Max brk space used 0
55297 extents written (108 MB)</pre></li><li class="listitem"><p class="simpara">
							Create a driver installation script for your Compute nodes. This script installs the NVIDIA GRID host driver on each Compute node that you run it on. The following example creates a script named <code class="literal">install_nvidia.sh</code>:
						</p><pre class="screen">#/bin/bash

# NVIDIA GRID package
mkdir /tmp/mount
mount LABEL=NVIDIA /tmp/mount
rpm -ivh /tmp/mount/NVIDIA-vGPU-rhel-8.1-430.27.x86_64.rpm</pre></li><li class="listitem"><p class="simpara">
							Customize the overcloud image by attaching the ISO image that you generated in Step 4, and running the driver installation script that you created in Step 5:
						</p><pre class="screen">$ virt-customize --attach nvidia-packages.iso -a overcloud-full-gpu.qcow2  -v --run install_nvidia.sh
[   0.0] Examining the guest ...
libguestfs: launch: program=virt-customize
libguestfs: launch: version=1.36.10rhel=8,release=6.el8_5.2,libvirt
libguestfs: launch: backend registered: unix
libguestfs: launch: backend registered: uml
libguestfs: launch: backend registered: libvirt</pre></li><li class="listitem"><p class="simpara">
							Relabel the customized image with SELinux:
						</p><pre class="screen">$ virt-customize -a overcloud-full-gpu.qcow2 --selinux-relabel
[   0.0] Examining the guest ...
[   2.2] Setting a random seed
[   2.2] SELinux relabelling
[  27.4] Finishing off</pre></li><li class="listitem"><p class="simpara">
							Prepare the custom image files for upload to the OpenStack Image Service:
						</p><pre class="screen">$ mkdir /var/image/x86_64/image
$ guestmount -a overcloud-full-gpu.qcow2 -i --ro image
$ cp image/boot/vmlinuz-3.10.0-862.14.4.el8.x86_64 ./overcloud-full-gpu.vmlinuz
$ cp image/boot/initramfs-3.10.0-862.14.4.el8.x86_64.img ./overcloud-full-gpu.initrd</pre></li><li class="listitem"><p class="simpara">
							From the undercloud, upload the custom image to the OpenStack Image Service:
						</p><pre class="screen">(undercloud) $ openstack overcloud image upload --update-existing --os-image-name overcloud-full-gpu.qcow2</pre></li></ol></div></section><section class="section" id="nvidia-config-role"><div class="titlepage"><div><div><h3 class="title">11.2.2. Designating Compute nodes for vGPU</h3></div></div></div><p>
					To designate Compute nodes for vGPU workloads, you must create a new role file to configure the vGPU role, and configure a new flavor to use to tag the GPU-enabled Compute nodes.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							To create the new <code class="literal">ComputeGPU</code> role file, copy the file <code class="literal">/usr/share/openstack-tripleo-heat-templates/roles/Compute.yaml</code> to <code class="literal">/usr/share/openstack-tripleo-heat-templates/roles/ComputeGPU.yaml</code> and edit the following file sections:
						</p><div class="table" id="idm139747269421152"><p class="title"><strong>Table 11.1. ComputeGPU role file edits</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747269414640" scope="col">Section/Parameter</th><th align="left" valign="top" id="idm139747269413552" scope="col">Current value</th><th align="left" valign="top" id="idm139747269412464" scope="col">New value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											Role comment
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">Role: Compute</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">Role: ComputeGpu</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											Role name
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">name: Compute</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">name: ComputeGpu</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											<code class="literal">description</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">Basic Compute Node role</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">GPU Compute Node role</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											<code class="literal">ImageDefault</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">overcloud-full</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">overcloud-full-gpu</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											<code class="literal">HostnameFormatDefault</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">-compute-</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">-computegpu-</code>
										</p>
										 </td></tr><tr><td align="left" valign="top" headers="idm139747269414640"> <p>
											<code class="literal">deprecated_nic_config_name</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269413552"> <p>
											<code class="literal">compute.yaml</code>
										</p>
										 </td><td align="left" valign="top" headers="idm139747269412464"> <p>
											<code class="literal">compute-gpu.yaml</code>
										</p>
										 </td></tr></tbody></table></div></div></li><li class="listitem"><p class="simpara">
							Generate a new roles data file named <code class="literal">gpu_roles_data.yaml</code> that includes the <code class="literal">Controller</code>, <code class="literal">Compute</code>, and <code class="literal">ComputeGpu</code> roles.
						</p><pre class="screen">(undercloud) [stack@director templates]$ openstack overcloud roles generate -o /home/stack/templates/gpu_roles_data.yaml Controller Compute ComputeGpu</pre><p class="simpara">
							The following example shows the <code class="literal">ComputeGpu</code> role details:
						</p><pre class="screen">#####################################################################
# Role: ComputeGpu                                                  #
#####################################################################
- name: ComputeGpu
  description: |
    GPU Compute Node role
  CountDefault: 1
  ImageDefault: overcloud-full-gpu
  networks:
    - InternalApi
    - Tenant
    - Storage
  HostnameFormatDefault: '%stackname%-computegpu-%index%'
  RoleParametersDefault:
    TunedProfileName: "virtual-host"
  # Deprecated &amp; backward-compatible values (FIXME: Make parameters consistent)
  # Set uses_deprecated_params to True if any deprecated params are used.
  uses_deprecated_params: True
  deprecated_param_image: 'NovaImage'
  deprecated_param_extraconfig: 'NovaComputeExtraConfig'
  deprecated_param_metadata: 'NovaComputeServerMetadata'
  deprecated_param_scheduler_hints: 'NovaComputeSchedulerHints'
  deprecated_param_ips: 'NovaComputeIPs'
  deprecated_server_resource_name: 'NovaCompute'
  deprecated_nic_config_name: 'compute-gpu.yaml'
  ServicesDefault:
    - OS::TripleO::Services::Aide
    - OS::TripleO::Services::AuditD
    - OS::TripleO::Services::BootParams
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CephClient
    - OS::TripleO::Services::CephExternal
    - OS::TripleO::Services::CertmongerUser
    - OS::TripleO::Services::Collectd
    - OS::TripleO::Services::ComputeCeilometerAgent
    - OS::TripleO::Services::ComputeNeutronCorePlugin
    - OS::TripleO::Services::ComputeNeutronL3Agent
    - OS::TripleO::Services::ComputeNeutronMetadataAgent
    - OS::TripleO::Services::ComputeNeutronOvsAgent
    - OS::TripleO::Services::Docker
    - OS::TripleO::Services::Fluentd
    - OS::TripleO::Services::IpaClient
    - OS::TripleO::Services::Ipsec
    - OS::TripleO::Services::Iscsid
    - OS::TripleO::Services::Kernel
    - OS::TripleO::Services::LoginDefs
    - OS::TripleO::Services::MetricsQdr
    - OS::TripleO::Services::MySQLClient
    - OS::TripleO::Services::NeutronBgpVpnBagpipe
    - OS::TripleO::Services::NeutronLinuxbridgeAgent
    - OS::TripleO::Services::NeutronVppAgent
    - OS::TripleO::Services::NovaCompute
    - OS::TripleO::Services::NovaLibvirt
    - OS::TripleO::Services::NovaLibvirtGuests
    - OS::TripleO::Services::NovaMigrationTarget
    - OS::TripleO::Services::ContainersLogrotateCrond
    - OS::TripleO::Services::OpenDaylightOvs
    - OS::TripleO::Services::Podman
    - OS::TripleO::Services::Rhsm
    - OS::TripleO::Services::RsyslogSidecar
    - OS::TripleO::Services::Securetty
    - OS::TripleO::Services::SensuClient
    - OS::TripleO::Services::Snmp
    - OS::TripleO::Services::Sshd
    - OS::TripleO::Services::Timesync
    - OS::TripleO::Services::Timezone
    - OS::TripleO::Services::TripleoFirewall
    - OS::TripleO::Services::TripleoPackages
    - OS::TripleO::Services::Tuned
    - OS::TripleO::Services::Vpp
    - OS::TripleO::Services::OVNController
    - OS::TripleO::Services::OVNMetadataAgent</pre></li><li class="listitem">
							Register the node for the overcloud. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#sect-Registering_Nodes_for_the_Overcloud-basic">Registering nodes for the overcloud</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
						</li><li class="listitem">
							Inspect the node hardware. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#inspecting-the-hardware-of-nodes-basic">Inspecting the hardware of nodes</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
						</li><li class="listitem"><p class="simpara">
							Create the <code class="literal">compute-vgpu-nvidia</code> flavor to use to tag nodes that you want to designate for vGPU workloads:
						</p><pre class="screen">(undercloud) [stack@director templates]$ openstack flavor create --id auto --ram 6144 --disk 40 --vcpus 4 compute-vgpu-nvidia
+----------------------------+--------------------------------------+
| Field                      | Value                                |
+----------------------------+--------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                |
| OS-FLV-EXT-DATA:ephemeral  | 0                                    |
| disk                       | 40                                   |
| id                         | 9cb47954-be00-47c6-a57f-44db35be3e69 |
| name                       | compute-vgpu-nvidia                  |
| os-flavor-access:is_public | True                                 |
| properties                 |                                      |
| ram                        | 6144                                 |
| rxtx_factor                | 1.0                                  |
| swap                       |                                      |
| vcpus                      | 4                                    |
+----------------------------+--------------------------------------+</pre></li><li class="listitem"><p class="simpara">
							Tag each node that you want to designate for GPU workloads with the <code class="literal">compute-vgpu-nvidia</code> profile.
						</p><pre class="screen">(undercloud) [stack@director templates]$ openstack baremetal node set --property capabilities='profile:compute-vgpu-nvidia,boot_option:local' &lt;node&gt;</pre><p class="simpara">
							Replace <code class="literal">&lt;node&gt;</code> with the ID of the baremetal node.
						</p></li></ol></div></section><section class="section" id="nvidia-prepare-config-files-deploy"><div class="titlepage"><div><div><h3 class="title">11.2.3. Configuring the Compute node for vGPU and deploying the overcloud</h3></div></div></div><p>
					You need to retrieve and assign the vGPU type that corresponds to the physical GPU device in your environment, and prepare the environment files to configure the Compute node for vGPU.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							Install Red Hat Enterprise Linux and the NVIDIA GRID driver on a temporary Compute node and launch the node. For more information about installing the NVIDIA GRID driver, see <a class="xref" href="index.html#nvidia-build-overcloud-image" title="11.2.1. Building a custom GPU overcloud image">Section 11.2.1, “Building a custom GPU overcloud image”</a>.
						</li><li class="listitem"><p class="simpara">
							On the Compute node, locate the vGPU type of the physical GPU device that you want to enable. For libvirt, virtual GPUs are mediated devices, or <code class="literal">mdev</code> type devices. To discover the supported <code class="literal">mdev</code> devices, enter the following command:
						</p><pre class="screen">[root@overcloud-computegpu-0 ~]# ls /sys/class/mdev_bus/0000\:06\:00.0/mdev_supported_types/
nvidia-11  nvidia-12  nvidia-13  nvidia-14  nvidia-15  nvidia-16  nvidia-17  nvidia-18  nvidia-19  nvidia-20  nvidia-21  nvidia-210  nvidia-22

[root@overcloud-computegpu-0 ~]# cat /sys/class/mdev_bus/0000\:06\:00.0/mdev_supported_types/nvidia-18/description
num_heads=4, frl_config=60, framebuffer=2048M, max_resolution=4096x2160, max_instance=4</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">compute-gpu.yaml</code> file to the <code class="literal">network-environment.yaml</code> file:
						</p><pre class="screen">resource_registry:
  OS::TripleO::Compute::Net::SoftwareConfig: /home/stack/templates/nic-configs/compute.yaml
  OS::TripleO::ComputeGpu::Net::SoftwareConfig: /home/stack/templates/nic-configs/compute-gpu.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: /home/stack/templates/nic-configs/controller.yaml
  #OS::TripleO::AllNodes::Validation: OS::Heat::None</pre></li><li class="listitem"><p class="simpara">
							Add the following parameters to the <code class="literal">node-info.yaml</code> file to specify the number of GPU Compute nodes, and the flavor to use for the GPU-designated Compute nodes:
						</p><pre class="screen">parameter_defaults:
  OvercloudControllerFlavor: control
  OvercloudComputeFlavor: compute
  OvercloudComputeGpuFlavor: compute-vgpu-nvidia
  ControllerCount: 1
  ComputeCount: 0
  ComputeGpuCount: 1</pre></li><li class="listitem"><p class="simpara">
							Create a <code class="literal">gpu.yaml</code> file to specify the vGPU type of your GPU device:
						</p><pre class="screen">parameter_defaults:
  ComputeGpuExtraConfig:
    nova::compute::vgpu::enabled_vgpu_types:
      - nvidia-18</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Each physical GPU supports only one virtual GPU type. If you specify multiple vGPU types in this property, only the first type is used.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Deploy the overcloud, adding your new role and environment files to the stack along with your other environment files:
						</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -r /home/stack/templates/nvidia/gpu_roles_data.yaml
  -e /home/stack/templates/node-info.yaml
  -e /home/stack/templates/network-environment.yaml
  -e [your environment files]
  -e /home/stack/templates/gpu.yaml</pre></li></ol></div></section></section><section class="section" id="vgpu-instance-creation"><div class="titlepage"><div><div><h2 class="title">11.3. Creating the vGPU image and flavor</h2></div></div></div><p>
				To enable your cloud users to create instances that use a virtual GPU (vGPU), you can define a custom vGPU-enabled image, and you can create a vGPU flavor.
			</p><section class="section" id="nvidia-build-guest-image"><div class="titlepage"><div><div><h3 class="title">11.3.1. Creating a custom GPU instance image</h3></div></div></div><p>
					After you deploy the overcloud with GPU-enabled Compute nodes, you can create a custom vGPU-enabled instance image with the NVIDIA GRID guest driver and license file.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an instance with the hardware and software profile that your vGPU instances require:
						</p><pre class="screen">(overcloud) [stack@director ~]$ openstack server create --flavor &lt;flavor&gt; --image &lt;image&gt; temp_vgpu_instance</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Replace <code class="literal">&lt;flavor&gt;</code> with the name or ID of the flavor that has the hardware profile that your vGPU instances require. For information on default flavors, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/ch-manage_instances#section-flavors">Manage flavors</a>.
								</li><li class="listitem">
									Replace <code class="literal">&lt;image&gt;</code> with the name or ID of the image that has the software profile that your vGPU instances require. For information on downloading RHEL cloud images, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/instances_and_images_guide/ch-image-service">Image service</a>.
								</li></ul></div></li><li class="listitem">
							Log in to the instance as a cloud-user. For more information, see <a class="link" href="index.html#section-Check-instance">Log in to an Instance</a>.
						</li><li class="listitem">
							Create the <code class="literal">gridd.conf</code> NVIDIA GRID license file on the instance, following the NVIDIA guidance: <a class="link" href="https://docs.nvidia.com/grid/latest/grid-licensing-user-guide/index.html#licensing-grid-vgpu-linux-config-file">Licensing an NVIDIA vGPU on Linux by Using a Configuration File</a>.
						</li><li class="listitem"><p class="simpara">
							Install the GPU driver on the instance. For more information about installing an NVIDIA driver, see <a class="link" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#installing-vgpu-drivers-linux">Installing the NVIDIA vGPU Software Graphics Driver on Linux</a>.
						</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
								Use the <code class="literal">hw_video_model</code> image property to define the GPU driver type. You can choose <code class="literal">none</code> if you want to disable the emulated GPUs for your vGPU instances. For more information about supported drivers, see <a class="xref" href="index.html#appx-image-config-parameters" title="Appendix A. Image Configuration Parameters">Appendix A, <em>Image Configuration Parameters</em></a>.
							</p></div></div></li><li class="listitem"><p class="simpara">
							Create an image snapshot of the instance:
						</p><pre class="screen">(overcloud) [stack@director ~]$ openstack server image create --name vgpu_image temp_vgpu_instance</pre></li><li class="listitem">
							Optional: Delete the instance.
						</li></ol></div></section><section class="section" id="nvidia-create-gpu-flavor-instance"><div class="titlepage"><div><div><h3 class="title">11.3.2. Creating a vGPU flavor for instances</h3></div></div></div><p>
					After you deploy the overcloud with GPU-enabled Compute nodes, you can create a custom flavor that your cloud users can use to launch instances for GPU workloads.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an NVIDIA GPU flavor. For example:
						</p><pre class="screen">(overcloud) [stack@virtlab-director2 ~]$ openstack flavor create --vcpus 6 --ram 8192 --disk 100 m1.small-gpu
+----------------------------+--------------------------------------+
| Field                      | Value                                |
+----------------------------+--------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                |
| OS-FLV-EXT-DATA:ephemeral  | 0                                    |
| disk                       | 100                                  |
| id                         | a27b14dd-c42d-4084-9b6a-225555876f68 |
| name                       | m1.small-gpu                         |
| os-flavor-access:is_public | True                                 |
| properties                 |                                      |
| ram                        | 8192                                 |
| rxtx_factor                | 1.0                                  |
| swap                       |                                      |
| vcpus                      | 6                                    |
+----------------------------+--------------------------------------+</pre></li><li class="listitem"><p class="simpara">
							Assign a vGPU resource to the flavor that you created. You can assign only one vGPU for each instance.
						</p><pre class="screen">(overcloud) [stack@virtlab-director2 ~]$ openstack flavor set m1.small-gpu --property "resources:VGPU=1"

(overcloud) [stack@virtlab-director2 ~]$ openstack flavor show m1.small-gpu
+----------------------------+--------------------------------------+
| Field                      | Value                                |
+----------------------------+--------------------------------------+
| OS-FLV-DISABLED:disabled   | False                                |
| OS-FLV-EXT-DATA:ephemeral  | 0                                    |
| access_project_ids         | None                                 |
| disk                       | 100                                  |
| id                         | a27b14dd-c42d-4084-9b6a-225555876f68 |
| name                       | m1.small-gpu                         |
| os-flavor-access:is_public | True                                 |
| properties                 | resources:VGPU='1'                   |
| ram                        | 8192                                 |
| rxtx_factor                | 1.0                                  |
| swap                       |                                      |
| vcpus                      | 6                                    |
+----------------------------+--------------------------------------+</pre></li></ol></div></section><section class="section" id="nvidia-launch-test-instance"><div class="titlepage"><div><div><h3 class="title">11.3.3. Launching a vGPU instance</h3></div></div></div><p>
					You can create a GPU-enabled instance for GPU workloads.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Create an instance using a GPU flavor and image. For example:
						</p><pre class="screen">(overcloud) [stack@virtlab-director2 ~]$ openstack server create --flavor m1.small-gpu --image vgpu_image --security-group web --nic net-id=internal0 --key-name lambda vgpu-instance</pre></li><li class="listitem">
							Log in to the instance as a cloud-user. For more information, see <a class="link" href="index.html#section-Check-instance">Log in to an Instance</a>.
						</li><li class="listitem"><p class="simpara">
							To verify that the GPU is accessible from the instance, run the following command from the instance:
						</p><pre class="screen">$ lspci -nn | grep &lt;gpu_name&gt;</pre></li></ol></div></section></section><section class="section" id="gpu-pci-passthru"><div class="titlepage"><div><div><h2 class="title">11.4. Enabling PCI passthrough for a GPU device</h2></div></div></div><p>
				You can use PCI passthrough to attach a physical PCI device, such as a graphics card, to an instance. If you use PCI passthrough for a device, the instance reserves exclusive access to the device for performing tasks, and the device is not available to the host.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						The <code class="literal">pciutils</code> package is installed on the physical servers that have the PCI cards.
					</li><li class="listitem">
						The GPU driver is available to install on the GPU instances. For more information, see <a class="xref" href="index.html#nvidia-build-overcloud-image" title="11.2.1. Building a custom GPU overcloud image">Section 11.2.1, “Building a custom GPU overcloud image”</a>.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To determine the vendor ID and product ID for each passthrough device type, run the following command on the physical server that has the PCI cards:
					</p><pre class="screen"># lspci -nn | grep -i &lt;gpu_name&gt;</pre><p class="simpara">
						For example, to determine the vendor and product ID for an NVIDIA GPU, run the following command:
					</p><pre class="screen"># lspci -nn | grep -i nvidia
3b:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1eb8] (rev a1)
d8:00.0 3D controller [0302]: NVIDIA Corporation TU104GL [Tesla T4] [10de:1db4] (rev a1)</pre></li><li class="listitem">
						To configure the Controller node on the overcloud for PCI passthrough, create an environment file, for example, <code class="literal">pci_passthru_controller.yaml</code>.
					</li><li class="listitem"><p class="simpara">
						Add <code class="literal">PciPassthroughFilter</code> to the <code class="literal">NovaSchedulerDefaultFilters</code> parameter in <code class="literal">pci_passthru_controller.yaml</code>:
					</p><pre class="screen">parameter_defaults:
  NovaSchedulerDefaultFilters: ['RetryFilter','AvailabilityZoneFilter','ComputeFilter','ComputeCapabilitiesFilter','ImagePropertiesFilter','ServerGroupAntiAffinityFilter','ServerGroupAffinityFilter','PciPassthroughFilter','NUMATopologyFilter']</pre></li><li class="listitem"><p class="simpara">
						To specify the PCI alias for the devices on the Controller node, add the following to <code class="literal">pci_passthru_controller.yaml</code>:
					</p><pre class="screen">ControllerExtraConfig:
    nova::pci::aliases:
      -  name: "t4"
         product_id: "1eb8"
         vendor_id: "10de"
      -  name: "v100"
         product_id: "1db4"
         vendor_id: "10de"</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							If the <code class="literal">nova-api</code> service is running in a role other than the Controller, then replace <code class="literal">ControllerExtraConfig</code> with the user role, in the format <code class="literal">&lt;Role&gt;ExtraConfig</code>.
						</p></div></div></li><li class="listitem">
						To configure the Compute node on the overcloud for PCI passthrough, create an environment file, for example, <code class="literal">pci_passthru_compute.yaml</code>.
					</li><li class="listitem"><p class="simpara">
						To specify the available PCIs for the devices on the Compute node, add the following to <code class="literal">pci_passthru_compute.yaml</code>:
					</p><pre class="screen">parameter_defaults:
  NovaPCIPassthrough:
    - vendor_id: "10de"
      product_id: "1eb8"</pre></li><li class="listitem"><p class="simpara">
						To enable IOMMU in the server BIOS of the Compute nodes to support PCI passthrough, add the <code class="literal">KernelArgs</code> parameter to <code class="literal">pci_passthru_compute.yaml</code>:
					</p><pre class="screen">   parameter_defaults:
      ...
      ComputeParameters:
        KernelArgs: "intel_iommu=on iommu=pt"</pre></li><li class="listitem"><p class="simpara">
						Deploy the overcloud, adding your custom environment files to the stack along with your other environment files:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -e [your environment files]
  -e /home/stack/templates/pci_passthru_controller.yaml
  -e /home/stack/templates/pci_passthru_compute.yaml</pre></li><li class="listitem"><p class="simpara">
						Configure a flavor to request the PCI devices. The following example requests two devices, each with a vendor ID of <code class="literal">10de</code> and a product ID of <code class="literal">13f2</code>:
					</p><pre class="screen"># openstack flavor set m1.large --property "pci_passthrough:alias"="t4:2"</pre></li><li class="listitem"><p class="simpara">
						Create an instance with a PCI passthrough device:
					</p><pre class="screen"># openstack server create --flavor m1.large --image rhelgpu --wait test-pci</pre></li><li class="listitem">
						Log in to the instance as a cloud-user. For more information, see <a class="link" href="index.html#section-Check-instance">Log in to an Instance</a>.
					</li><li class="listitem"><p class="simpara">
						Install the GPU driver on the instance. For example, run the following script to install an NVIDIA driver:
					</p><pre class="screen">$ sh NVIDIA-Linux-x86_64-430.24-grid.run</pre></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To verify that the GPU is accessible from the instance, run the following command from the instance:
					</p><pre class="screen">$ lspci -nn | grep &lt;gpu_name&gt;</pre></li><li class="listitem"><p class="simpara">
						To check the NVIDIA System Management Interface status, run the following command from the instance:
					</p><pre class="screen">$ nvidia-smi</pre><p class="simpara">
						Example output:
					</p><pre class="screen"><code class="literal">-----------------------------------------------------------------------------</code>
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------<code class="literal">----------------------</code>----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================<code class="literal">======================</code>======================|
|   0  Tesla T4            Off  | 00000000:01:00.0 Off |                    0 |
| N/A   43C    P0    20W /  70W |      0MiB / 15109MiB |      0%      Default |
<code class="literal">-------------------------------</code>----------------------<code class="literal">----------------------</code>

<code class="literal">-----------------------------------------------------------------------------</code>
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
<code class="literal">-----------------------------------------------------------------------------</code></pre></li></ol></div></section></section><section class="chapter" id="realtime-compute"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Configuring Real-Time Compute</h1></div></div></div><p>
			In some use-cases, you might need instances on your Compute nodes to adhere to low-latency policies and perform real-time processing. Real-time Compute nodes include a real-time capable kernel, specific virtualization modules, and optimized deployment parameters, to facilitate real-time processing requirements and minimize latency.
		</p><p>
			The process to enable Real-time Compute includes:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					configuring the BIOS settings of the Compute nodes
				</li><li class="listitem">
					building a real-time image with real-time kernel and Real-Time KVM (RT-KVM) kernel module
				</li><li class="listitem">
					assigning the <code class="literal">ComputeRealTime</code> role to the Compute nodes
				</li></ul></div><p>
			For a use-case example of Real-time Compute deployment for NFV workloads, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/network_functions_virtualization_planning_and_configuration_guide/#assembly_config-vxlan-dpdk-odl">Example: Configuring OVS-DPDK with ODL and VXLAN tunnelling</a> section in the <span class="emphasis"><em>Network Functions Virtualization Planning and Configuration Guide</em></span>.
		</p><section class="section" id="trc-preparing"><div class="titlepage"><div><div><h2 class="title">12.1. Preparing Your Compute Nodes for Real-Time</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Real-time Compute nodes are supported only with Red Hat Enterprise Linux version 7.5 or later.
				</p></div></div><p>
				Before you can deploy Real-time Compute in your overcloud, you must enable Red Hat Enterprise Linux Real-Time KVM (RT-KVM), configure your BIOS to support real-time, and build the real-time image.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You must use Red Hat certified servers for your RT-KVM Compute nodes. See <a class="link" href="https://access.redhat.com/ecosystem/search/#/ecosystem/Red%20Hat%20Enterprise%20Linux?sort=sortTitle%20asc&amp;certifications=Red%20Hat%20Enterprise%20Linux%20for%20Real%20Time%207&amp;category=Server">Red Hat Enterprise Linux for Real Time 7 certified servers</a> for details.
					</li><li class="listitem"><p class="simpara">
						You must enable the <code class="literal">rhel-8-for-x86_64-nfv-rpms</code> repository for RT-KVM to build the real-time image.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							You need a separate subscription to <span class="emphasis"><em>Red Hat OpenStack Platform for Real Time</em></span> before you can access this repository. For details on managing repositories and subscriptions for your undercloud, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/installing-the-undercloud#registering-and-updating-your-undercloud">Registering and updating your undercloud</a> section in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
						</p></div></div><p class="simpara">
						To check which packages will be installed from the repository, run the following command:
					</p><pre class="screen">$ dnf repo-pkgs rhel-8-for-x86_64-nfv-rpms list
Loaded plugins: product-id, search-disabled-repos, subscription-manager
Available Packages
kernel-rt.x86_64                   4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-debug.x86_64             4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-debug-devel.x86_64       4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-debug-kvm.x86_64         4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-devel.x86_64             4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-doc.noarch               4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
kernel-rt-kvm.x86_64               4.18.0-80.7.1.rt9.153.el8_0               rhel-8-for-x86_64-nfv-rpms
[ output omitted…]</pre></li></ul></div><div class="formalpara"><p class="title"><strong>Building the real-time image</strong></p><p>
					To build the overcloud image for Real-time Compute nodes:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Install the <code class="literal">libguestfs-tools</code> package on the undercloud to get the <code class="literal">virt-customize</code> tool:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ sudo dnf install libguestfs-tools</pre></li><li class="listitem"><p class="simpara">
						Extract the images:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ tar -xf /usr/share/rhosp-director-images/overcloud-full.tar
(undercloud) [stack@undercloud-0 ~]$ tar -xf /usr/share/rhosp-director-images/ironic-python-agent.tar</pre></li><li class="listitem"><p class="simpara">
						Copy the default image:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ cp overcloud-full.qcow2 overcloud-realtime-compute.qcow2</pre></li><li class="listitem"><p class="simpara">
						Register the image and configure the required subscriptions:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$  virt-customize -a overcloud-realtime-compute.qcow2 --run-command 'subscription-manager register --username=[username] --password=[password]'
[  0.0] Examining the guest ...
[ 10.0] Setting a random seed
[ 10.0] Running: subscription-manager register --username=[username] --password=[password]
[ 24.0] Finishing off</pre><p class="simpara">
						Replace the <code class="literal">username</code> and <code class="literal">password</code> values with your Red Hat customer account details. For general information about building a Real-time overcloud image, see the <a class="link" href="https://access.redhat.com/articles/1556833">Modifying the Red Hat Enterprise Linux OpenStack Platform Overcloud Image with virt-customize</a> knowledgebase article.
					</p></li><li class="listitem"><p class="simpara">
						Find the SKU of the <span class="emphasis"><em>Red Hat OpenStack Platform for Real Time</em></span> subscription. The SKU might be located on a system that is already registered to the Red Hat Subscription Manager with the same account and credentials. For example:
					</p><pre class="screen">$ sudo subscription-manager list</pre></li><li class="listitem"><p class="simpara">
						Attach the <span class="emphasis"><em>Red Hat OpenStack Platform for Real Time</em></span> subscription to the image:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$  virt-customize -a overcloud-realtime-compute.qcow2 --run-command 'subscription-manager attach --pool [subscription-pool]'</pre></li><li class="listitem"><p class="simpara">
						Create a script to configure <code class="literal">rt</code> on the image:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ cat rt.sh
  #!/bin/bash

  set -eux

  subscription-manager repos --enable=[REPO_ID]
  dnf -v -y --setopt=protected_packages= erase kernel.$(uname -m)
  dnf -v -y install kernel-rt kernel-rt-kvm tuned-profiles-nfv-host

  # END OF SCRIPT</pre></li><li class="listitem"><p class="simpara">
						Run the script to configure the real-time image:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ virt-customize -a overcloud-realtime-compute.qcow2 -v --run rt.sh 2&gt;&amp;1 | tee virt-customize.log</pre></li><li class="listitem"><p class="simpara">
						Re-label SELinux:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ virt-customize -a overcloud-realtime-compute.qcow2 --selinux-relabel</pre></li><li class="listitem"><p class="simpara">
						Extract <code class="literal">vmlinuz</code> and <code class="literal">initrd</code>. For example:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ mkdir image
(undercloud) [stack@undercloud-0 ~]$ guestmount -a overcloud-realtime-compute.qcow2 -i --ro image
(undercloud) [stack@undercloud-0 ~]$ cp image/boot/vmlinuz-4.18.0-80.7.1.rt9.153.el8_0.x86_64 ./overcloud-realtime-compute.vmlinuz
(undercloud) [stack@undercloud-0 ~]$ cp image/boot/initramfs-4.18.0-80.7.1.rt9.153.el8_0.x86_64.img ./overcloud-realtime-compute.initrd
(undercloud) [stack@undercloud-0 ~]$ guestunmount image</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The software version in the <code class="literal">vmlinuz</code> and <code class="literal">initramfs</code> filenames vary with the kernel version.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Upload the image:
					</p><pre class="screen">(undercloud) [stack@undercloud-0 ~]$ openstack overcloud image upload --update-existing --os-image-name overcloud-realtime-compute.qcow2</pre></li></ol></div><p>
				You now have a real-time image you can use with the <code class="literal">ComputeRealTime</code> composable role on select Compute nodes.
			</p><div class="formalpara"><p class="title"><strong>Modifying BIOS settings on Real-time Compute nodes</strong></p><p>
					To reduce latency on your Real-time Compute nodes, you must modify the BIOS settings in the Compute nodes. You should disable all options for the following components in your Compute node BIOS settings:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Power Management
					</li><li class="listitem">
						Hyper-Threading
					</li><li class="listitem">
						CPU sleep states
					</li><li class="listitem">
						Logical processors
					</li></ul></div><p>
				See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/8/html/tuning_guide/chap-general_system_tuning#Setting_BIOS_parameters">Setting BIOS parameters</a> for descriptions of these settings and the impact of disabling them. See your hardware manufacturer documentation for complete details on how to change BIOS settings.
			</p></section><section class="section" id="rtc-deploying"><div class="titlepage"><div><div><h2 class="title">12.2. Deploying the Real-time Compute Role</h2></div></div></div><p>
				Red Hat OpenStack Platform director provides the template for the <code class="literal">ComputeRealTime</code> role, which you can use to deploy real-time Compute nodes. You must perform additional steps to designate Compute nodes for real-time.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Based on the <span class="strong strong"><strong><span class="emphasis"><em>/usr/share/openstack-tripleo-heat-templates/environments/compute-real-time-example.yaml</em></span></strong></span> file, create a <span class="strong strong"><strong><span class="emphasis"><em>compute-real-time.yaml</em></span></strong></span> environment file that sets the parameters for the <code class="literal">ComputeRealTime</code> role.
					</p><pre class="screen">cp /usr/share/openstack-tripleo-heat-templates/environments/compute-real-time-example.yaml /home/stack/templates/compute-real-time.yaml</pre><p class="simpara">
						The file must include values for the following parameters:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								<code class="literal">IsolCpusList</code> and <code class="literal">NovaComputeCpuDedicatedSet</code>: List of isolated CPU cores and virtual CPU pins to reserve for real-time workloads. This value depends on the CPU hardware of your real-time Compute nodes.
							</li><li class="listitem">
								<code class="literal">NovaComputeCpuSharedSet</code>: List of host CPUs to reserve for emulator threads.
							</li><li class="listitem">
								<code class="literal">KernelArgs</code>: Arguments to pass to the kernel of the Real-time Compute nodes. For example, you can use <code class="literal">default_hugepagesz=1G hugepagesz=1G hugepages=&lt;number_of_1G_pages_to_reserve&gt; hugepagesz=2M hugepages=&lt;number_of_2M_pages&gt;</code> to define the memory requirements of guests that have huge pages with multiple sizes. In this example, the default size is 1GB but you can also reserve 2M huge pages.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">ComputeRealTime</code> role to your roles data file and regenerate the file. For example:
					</p><pre class="screen">$ openstack overcloud roles generate -o /home/stack/templates/rt_roles_data.yaml Controller Compute ComputeRealTime</pre><p class="simpara">
						This command generates a <code class="literal">ComputeRealTime</code> role with contents similar to the following example, and also sets the <code class="literal">ImageDefault</code> option to <code class="literal">overcloud-realtime-compute</code>.
					</p><pre class="screen">- name: ComputeRealTime
  description: |
    Compute role that is optimized for real-time behaviour. When using this role
    it is mandatory that an overcloud-realtime-compute image is available and
    the role specific parameters IsolCpusList, NovaComputeCpuDedicatedSet and
    NovaComputeCpuSharedSet are set accordingly to the hardware of the real-time compute nodes.
  CountDefault: 1
  networks:
    InternalApi:
      subnet: internal_api_subnet
    Tenant:
      subnet: tenant_subnet
    Storage:
      subnet: storage_subnet
  HostnameFormatDefault: '%stackname%-computerealtime-%index%'
  ImageDefault: overcloud-realtime-compute
  RoleParametersDefault:
    TunedProfileName: "realtime-virtual-host"
    KernelArgs: ""      # these must be set in an environment file
    IsolCpusList: ""    # or similar according to the hardware
    NovaComputeCpuDedicatedSet: ""  # of real-time nodes
    NovaComputeCpuSharedSet: ""     #
    NovaLibvirtMemStatsPeriodSeconds: 0
  ServicesDefault:
    - OS::TripleO::Services::Aide
    - OS::TripleO::Services::AuditD
    - OS::TripleO::Services::BootParams
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CephClient
    - OS::TripleO::Services::CephExternal
    - OS::TripleO::Services::CertmongerUser
    - OS::TripleO::Services::Collectd
    - OS::TripleO::Services::ComputeCeilometerAgent
    - OS::TripleO::Services::ComputeNeutronCorePlugin
    - OS::TripleO::Services::ComputeNeutronL3Agent
    - OS::TripleO::Services::ComputeNeutronMetadataAgent
    - OS::TripleO::Services::ComputeNeutronOvsAgent
    - OS::TripleO::Services::Docker
    - OS::TripleO::Services::Fluentd
    - OS::TripleO::Services::IpaClient
    - OS::TripleO::Services::Ipsec
    - OS::TripleO::Services::Iscsid
    - OS::TripleO::Services::Kernel
    - OS::TripleO::Services::LoginDefs
    - OS::TripleO::Services::MetricsQdr
    - OS::TripleO::Services::MySQLClient
    - OS::TripleO::Services::NeutronBgpVpnBagpipe
    - OS::TripleO::Services::NeutronLinuxbridgeAgent
    - OS::TripleO::Services::NeutronVppAgent
    - OS::TripleO::Services::NovaCompute
    - OS::TripleO::Services::NovaLibvirt
    - OS::TripleO::Services::NovaLibvirtGuests
    - OS::TripleO::Services::NovaMigrationTarget
    - OS::TripleO::Services::ContainersLogrotateCrond
    - OS::TripleO::Services::OpenDaylightOvs
    - OS::TripleO::Services::Podman
    - OS::TripleO::Services::Rhsm
    - OS::TripleO::Services::RsyslogSidecar
    - OS::TripleO::Services::Securetty
    - OS::TripleO::Services::SensuClient
    - OS::TripleO::Services::SkydiveAgent
    - OS::TripleO::Services::Snmp
    - OS::TripleO::Services::Sshd
    - OS::TripleO::Services::Timesync
    - OS::TripleO::Services::Timezone
    - OS::TripleO::Services::TripleoFirewall
    - OS::TripleO::Services::TripleoPackages
    - OS::TripleO::Services::Vpp
    - OS::TripleO::Services::OVNController
    - OS::TripleO::Services::OVNMetadataAgent</pre><p class="simpara">
						For general information about custom roles and about the <span class="strong strong"><strong><span class="emphasis"><em>roles-data.yaml</em></span></strong></span>, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/#roles">Roles</a> section.
					</p></li><li class="listitem"><p class="simpara">
						Create the <code class="literal">compute-realtime</code> flavor to tag nodes that you want to designate for real-time workloads. For example:
					</p><pre class="screen">$ source ~/stackrc
$ openstack flavor create --id auto --ram 6144 --disk 40 --vcpus 4 compute-realtime
$ openstack flavor set --property "cpu_arch"="x86_64" --property "capabilities:boot_option"="local" --property "capabilities:profile"="compute-realtime" compute-realtime</pre></li><li class="listitem"><p class="simpara">
						Tag each node that you want to designate for real-time workloads with the <code class="literal">compute-realtime</code> profile.
					</p><pre class="screen">$ openstack baremetal node set --property capabilities='profile:compute-realtime,boot_option:local' &lt;NODE UUID&gt;</pre></li><li class="listitem"><p class="simpara">
						Map the <code class="literal">ComputeRealTime</code> role to the <code class="literal">compute-realtime</code> flavor by creating an environment file with the following content:
					</p><pre class="screen">parameter_defaults:
  OvercloudComputeRealTimeFlavor: compute-realtime</pre></li><li class="listitem"><p class="simpara">
						Run the <code class="literal">openstack overcloud deploy</code> command with the <code class="literal">-e</code> option and specify all the environment files that you created, as well as the new roles file. For example:
					</p><pre class="screen">$ openstack overcloud deploy -r /home/stack/templates/rt~/my_roles_data.yaml  -e home/stack/templates/compute-real-time.yaml &lt;FLAVOR_ENV_FILE&gt;</pre></li></ol></div></section><section class="section" id="rtc-testing"><div class="titlepage"><div><div><h2 class="title">12.3. Sample Deployment and Testing Scenario</h2></div></div></div><p>
				The following example procedure uses a simple single-node deployment to test that the environment variables and other supporting configuration is set up correctly. Actual performance results might vary, depending on the number of nodes and guests that you deploy in your cloud.
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Create the <code class="literal">compute-real-time.yaml</code> file with the following parameters:
					</p><pre class="screen">parameter_defaults:
  ComputeRealTimeParameters:
    IsolCpusList: "1"
    NovaComputeCpuDedicatedSet: "1"
    NovaComputeCpuSharedSet: "0"
    KernelArgs: "default_hugepagesz=1G hugepagesz=1G hugepages=16"</pre></li><li class="listitem"><p class="simpara">
						Create a new <code class="literal">rt_roles_data.yaml</code> file with the <code class="literal">ComputeRealTime</code> role:
					</p><pre class="screen">$ openstack overcloud roles generate -o ~/rt_roles_data.yaml Controller ComputeRealTime</pre></li><li class="listitem"><p class="simpara">
						Deploy the overcloud, adding both your new real-time roles data file and your real-time environment file to the stack along with your other environment files:
					</p><pre class="screen">(undercloud) $ openstack overcloud deploy --templates \
  -r /home/stack/rt_roles_data.yaml
  -e [your environment files]
  -e /home/stack/templates/compute-real-time.yaml</pre><p class="simpara">
						This command deploys one Controller node and one Real-time Compute node.
					</p></li><li class="listitem"><p class="simpara">
						Log into the Real-time Compute node and check the following parameters. Replace <code class="literal">&lt;...&gt;</code> with the values of the relevant parameters from the <code class="literal">compute-real-time.yaml</code>.
					</p><pre class="screen">[root@overcloud-computerealtime-0 ~]# uname -a
Linux overcloud-computerealtime-0 4.18.0-80.7.1.rt9.153.el8_0.x86_64 #1 SMP PREEMPT RT Wed Dec 13 13:37:53 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
[root@overcloud-computerealtime-0 ~]# cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-4.18.0-80.7.1.rt9.153.el8_0.x86_64 root=UUID=45ae42d0-58e7-44fe-b5b1-993fe97b760f ro console=tty0 crashkernel=auto console=ttyS0,115200 default_hugepagesz=<span class="strong strong"><strong>1G</strong></span> hugepagesz=<span class="strong strong"><strong>1G</strong></span> hugepages=<span class="strong strong"><strong>16</strong></span>
[root@overcloud-computerealtime-0 ~]# tuned-adm active
Current active profile: realtime-virtual-host
[root@overcloud-computerealtime-0 ~]# grep ^isolated_cores /etc/tuned/realtime-virtual-host-variables.conf
isolated_cores=&lt;IsolCpusList&gt;
[root@overcloud-computerealtime-0 ~]# cat /usr/lib/tuned/realtime-virtual-host/lapic_timer_adv_ns
X (X != 0)
[root@overcloud-computerealtime-0 ~]# cat /sys/module/kvm/parameters/lapic_timer_advance_ns
X (X != 0)
[root@overcloud-computerealtime-0 ~]# cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages
X (X != 0)
[root@overcloud-computerealtime-0 ~]# crudini --get /var/lib/config-data/puppet-generated/nova_libvirt/etc/nova/nova.conf compute cpu_dedicated_set
&lt;NovaComputeCpuDedicatedSet&gt;
[root@overcloud-computerealtime-0 ~]# crudini --get /var/lib/config-data/puppet-generated/nova_libvirt/etc/nova/nova.conf compute cpu_shared_set
&lt;NovaComputeCpuSharedSet&gt;</pre></li></ol></div></section><section class="section" id="rtc-instances"><div class="titlepage"><div><div><h2 class="title">12.4. Launching and Tuning Real-Time Instances</h2></div></div></div><p>
				After you deploy and configure Real-time Compute nodes, you can launch real-time instances on those nodes. You can further configure these real-time instances with CPU pinning, NUMA topology filters, and huge pages.
			</p><div class="orderedlist"><p class="title"><strong>Launching a real-time instance</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Make sure that the <code class="literal">compute-realtime</code> flavor exists on the overcloud, as described in the <span class="emphasis"><em>Deploying the Real-time Compute Role</em></span> section.
					</li><li class="listitem"><p class="simpara">
						Launch the real-time instance.
					</p><pre class="screen"># openstack server create  --image &lt;rhel&gt; --flavor r1.small --nic net-id=&lt;dpdk-net&gt; test-rt</pre></li><li class="listitem"><p class="simpara">
						Optionally, verify that the instance uses the assigned emulator threads.
					</p><pre class="screen"># virsh dumpxml &lt;instance-id&gt; | grep vcpu -A1
&lt;vcpu placement='static'&gt;4&lt;/vcpu&gt;
&lt;cputune&gt;
  &lt;vcpupin vcpu='0' cpuset='1'/&gt;
  &lt;vcpupin vcpu='1' cpuset='3'/&gt;
  &lt;vcpupin vcpu='2' cpuset='5'/&gt;
  &lt;vcpupin vcpu='3' cpuset='7'/&gt;
  &lt;emulatorpin cpuset='0-1'/&gt;
  &lt;vcpusched vcpus='2-3' scheduler='fifo'
  priority='1'/&gt;
&lt;/cputune&gt;</pre></li></ol></div><div class="formalpara"><p class="title"><strong>Pinning CPUs and setting emulator thread policy</strong></p><p>
					To ensure that there are enough CPUs on each Real-time Compute node for real-time workloads, you need to pin at least one virtual CPU (vCPU) for an instance to a physical CPU (pCPUs) on the host. The emulator threads for that vCPU then remain dedicated to that pCPU.
				</p></div><p>
				Configure your flavor to use a dedicated CPU policy. To do so, set the <code class="literal">hw:cpu_policy</code> parameter to <code class="literal">dedicated</code> on the flavor. For example:
			</p><pre class="screen"># openstack flavor set --property hw:cpu_policy=dedicated 99</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Make sure that your resources quota has enough pCPUs for the Real-time Compute nodes to consume.
				</p></div></div><div class="formalpara"><p class="title"><strong>Optimizing your network configuration</strong></p><p>
					Depending on the needs of your deployment, you might need to set parameters in the <span class="strong strong"><strong><span class="emphasis"><em>network-environment.yaml</em></span></strong></span> file to tune your network for certain real-time workloads.
				</p></div><p>
				To review an example configuration optimized for OVS-DPDK, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/network_functions_virtualization_planning_and_configuration_guide/#p-ovsdpdk-rtkvm-networkenv">Configuring the OVS-DPDK parameters</a> section of the <span class="emphasis"><em>Network Functions Virtualization Planning and Configuration Guide</em></span>.
			</p><div class="formalpara"><p class="title"><strong>Configuring huge pages</strong></p><p>
					It is recommended to set the default huge pages size to 1GB. Otherwise, TLB flushes might create jitter in the vCPU execution. For general information about using huge pages, see the <a class="link" href="https://doc.dpdk.org/guides/linux_gsg/sys_reqs.html#running-dpdk-applications">Running DPDK applications</a> web page.
				</p></div><div class="formalpara"><p class="title"><strong>Disabling Performance Monitoring Unit (PMU) emulation</strong></p><p>
					Instances can provide PMU metrics by specifying an image or flavor with a vPMU. Providing PMU metrics introduces latency.
				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The vPMU defaults to enabled when <code class="literal">cpu_mode=host-passthrough</code>.
				</p></div></div><p>
				If you do not need PMU metrics, then disable the vPMU to reduce latency by setting the PMU property to "False" in the image or flavor used to create the instance:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Image: <code class="literal">hw_pmu=False</code>
					</li><li class="listitem">
						Flavor: <code class="literal">hw:pmu=False</code>
					</li></ul></div></section></section><section class="appendix" id="appx-image-config-parameters"><div class="titlepage"><div><div><h1 class="title">Appendix A. Image Configuration Parameters</h1></div></div></div><p id="appx-image-metadata">
			The following keys can be used with the <code class="literal">property</code> option for both the <code class="literal">glance image-update</code> and <code class="literal">glance image-create</code> commands.
		</p><pre class="screen">$ glance image-update IMG-UUID --property architecture=x86_64</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Behavior set using image properties overrides behavior set using flavors. For more information, see <a class="xref" href="index.html#section-flavors" title="7.3. Manage Flavors">Section 7.3, “Manage Flavors”</a>.
			</p></div></div><div class="table" id="idm139747270442032"><p class="title"><strong>Table A.1. Property Keys</strong></p><div class="table-contents"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 18%; " class="col_1"><!--Empty--></col><col style="width: 18%; " class="col_2"><!--Empty--></col><col style="width: 18%; " class="col_3"><!--Empty--></col><col style="width: 46%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm139747270434608" scope="col">Specific to</th><th align="left" valign="top" id="idm139747270433520" scope="col">Key</th><th align="left" valign="top" id="idm139747270432432" scope="col">Description</th><th align="left" valign="top" id="idm139747270431344" scope="col">Supported values</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">architecture</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The CPU architecture that must be supported by the hypervisor. For example, <code class="literal">x86_64</code>, <code class="literal">arm</code>, or <code class="literal">ppc64</code>. Run <code class="literal">uname -m</code> to get the architecture of a machine.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">alpha</code> - DEC 64-bit RISC
								</li><li class="listitem">
									<code class="literal">armv7l</code> - ARM Cortex-A7 MPCore
								</li><li class="listitem">
									<code class="literal">cris</code>- Ethernet, Token Ring, AXis-Code Reduced Instruction Set
								</li><li class="listitem">
									<code class="literal">i686</code> - Intel sixth-generation x86 (P6 micro architecture)
								</li><li class="listitem">
									<code class="literal">ia64</code> - Itanium
								</li><li class="listitem">
									<code class="literal">lm32</code> - Lattice Micro32
								</li><li class="listitem">
									<code class="literal">m68k</code> - Motorola 68000
								</li><li class="listitem">
									<code class="literal">microblaze</code> - Xilinx 32-bit FPGA (Big Endian)
								</li><li class="listitem">
									<code class="literal">microblazeel</code> - Xilinx 32-bit FPGA (Little Endian)
								</li><li class="listitem">
									<code class="literal">mips</code> - MIPS 32-bit RISC (Big Endian)
								</li><li class="listitem">
									<code class="literal">mipsel</code> - MIPS 32-bit RISC (Little Endian)
								</li><li class="listitem">
									<code class="literal">mips64</code> - MIPS 64-bit RISC (Big Endian)
								</li><li class="listitem">
									<code class="literal">mips64el</code> - MIPS 64-bit RISC (Little Endian)
								</li><li class="listitem">
									<code class="literal">openrisc</code> - OpenCores RISC
								</li><li class="listitem">
									<code class="literal">parisc</code> - HP Precision Architecture RISC
								</li><li class="listitem">
									<code class="literal">parisc64</code> - HP Precision Architecture 64-bit RISC
								</li><li class="listitem">
									<code class="literal">ppc</code> - PowerPC 32-bit
								</li><li class="listitem">
									<code class="literal">ppc64</code> - PowerPC 64-bit
								</li><li class="listitem">
									<code class="literal">ppcemb</code> - PowerPC (Embedded 32-bit)
								</li><li class="listitem">
									<code class="literal">s390</code> - IBM Enterprise Systems Architecture/390
								</li><li class="listitem">
									<code class="literal">s390x</code> - S/390 64-bit
								</li><li class="listitem">
									<code class="literal">sh4</code> - SuperH SH-4 (Little Endian)
								</li><li class="listitem">
									<code class="literal">sh4eb</code> - SuperH SH-4 (Big Endian)
								</li><li class="listitem">
									<code class="literal">sparc</code> - Scalable Processor Architecture, 32-bit
								</li><li class="listitem">
									<code class="literal">sparc64</code> - Scalable Processor Architecture, 64-bit
								</li><li class="listitem">
									<code class="literal">unicore32</code> - Microprocessor Research and Development Center RISC Unicore32
								</li><li class="listitem">
									<code class="literal">x86_64</code> - 64-bit extension of IA-32
								</li><li class="listitem">
									<code class="literal">xtensa</code> - Tensilica Xtensa configurable microprocessor core
								</li><li class="listitem">
									<code class="literal">xtensaeb</code> - Tensilica Xtensa configurable microprocessor core (Big Endian)
								</li></ul></div>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hypervisor_type</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The hypervisor type.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">kvm</code>, <code class="literal">vmware</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">instance_uuid</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							For snapshot images, this is the UUID of the server used to create this image.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Valid server UUID
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">kernel_id</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The ID of an image stored in the Image Service that should be used as the kernel when booting an AMI-style image.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Valid image ID
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">os_distro</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The common name of the operating system distribution in lowercase.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">arch</code> - Arch Linux. Do not use <code class="literal">archlinux</code> or <code class="literal">org.archlinux</code>.
								</li><li class="listitem">
									<code class="literal">centos</code> - Community Enterprise Operating System. Do not use <code class="literal">org.centos</code> or <code class="literal">CentOS</code>.
								</li><li class="listitem">
									<code class="literal">debian</code> - Debian. Do not use <code class="literal">Debian</code> or <code class="literal">org.debian</code>.
								</li><li class="listitem">
									<code class="literal">fedora</code> - Fedora. Do not use <code class="literal">Fedora</code>, <code class="literal">org.fedora</code>, or <code class="literal">org.fedoraproject</code>.
								</li><li class="listitem">
									<code class="literal">freebsd</code> - FreeBSD. Do not use <code class="literal">org.freebsd</code>, <code class="literal">freeBSD</code>, or <code class="literal">FreeBSD</code>.
								</li><li class="listitem">
									<code class="literal">gentoo</code> - Gentoo Linux. Do not use <code class="literal">Gentoo</code> or <code class="literal">org.gentoo</code>.
								</li><li class="listitem">
									<code class="literal">mandrake</code> - Mandrakelinux (MandrakeSoft) distribution. Do not use <code class="literal">mandrakelinux</code> or <code class="literal">MandrakeLinux</code>.
								</li><li class="listitem">
									<code class="literal">mandriva</code> - Mandriva Linux. Do not use <code class="literal">mandrivalinux</code>.
								</li><li class="listitem">
									<code class="literal">mes</code> - Mandriva Enterprise Server. Do not use <code class="literal">mandrivaent</code> or <code class="literal">mandrivaES</code>.
								</li><li class="listitem">
									<code class="literal">msdos</code> - Microsoft Disc Operating System. Do not use <code class="literal">ms-dos</code>.
								</li><li class="listitem">
									<code class="literal">netbsd</code> - NetBSD. Do not use <code class="literal">NetBSD</code> or <code class="literal">org.netbsd</code>.
								</li><li class="listitem">
									<code class="literal">netware</code> - Novell NetWare. Do not use <code class="literal">novell</code> or <code class="literal">NetWare</code>.
								</li><li class="listitem">
									<code class="literal">openbsd</code> - OpenBSD. Do not use <code class="literal">OpenBSD</code> or <code class="literal">org.openbsd</code>.
								</li><li class="listitem">
									<code class="literal">opensolaris</code> - OpenSolaris. Do not use <code class="literal">OpenSolaris</code> or <code class="literal">org.opensolaris</code>.
								</li><li class="listitem">
									<code class="literal">opensuse</code> - openSUSE. Do not use <code class="literal">suse</code>, <code class="literal">SuSE</code>, or <code class="literal">org.opensuse</code>.
								</li><li class="listitem">
									<code class="literal">rhel</code> - Red Hat Enterprise Linux. Do not use <code class="literal">redhat</code>, <code class="literal">RedHat</code>, or <code class="literal">com.redhat</code>.
								</li><li class="listitem">
									<code class="literal">sled</code> - SUSE Linux Enterprise Desktop. Do not use <code class="literal">com.suse</code>.
								</li><li class="listitem">
									<code class="literal">ubuntu</code> - Ubuntu. Do not use <code class="literal">Ubuntu</code>, <code class="literal">com.ubuntu</code>, <code class="literal">org.ubuntu</code>, or <code class="literal">canonical</code>.
								</li><li class="listitem">
									<code class="literal">windows</code> - Microsoft Windows. Do not use <code class="literal">com.microsoft.server</code>.
								</li></ul></div>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">os_version</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The operating system version as specified by the distributor.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Version number (for example, "11.10")
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">ramdisk_id</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The ID of image stored in the Image Service that should be used as the ramdisk when booting an AMI-style image.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Valid image ID
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							All
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">vm_mode</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The virtual machine mode. This represents the host/guest ABI (application binary interface) used for the virtual machine.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">hvm</code>-Fully virtualized. This is the mode used by QEMU and KVM.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_disk_bus</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Specifies the type of disk controller to attach disk devices to.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">scsi</code>, <code class="literal">virtio</code>, <code class="literal">ide</code>, or <code class="literal">usb</code>. Note that if using <code class="literal">iscsi</code>, the <code class="literal">hw_scsi_model</code> needs to be set to <code class="literal">virtio-scsi</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_cdrom_bus</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Specifies the type of disk controller to attach CD-ROM devices to.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">scsi</code>, <code class="literal">virtio</code>, <code class="literal">ide</code>, or <code class="literal">usb</code>. If you specify <code class="literal">iscsi</code>, you must set the <code class="literal">hw_scsi_model</code> parameter to <code class="literal">virtio-scsi</code>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_numa_nodes</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Number of NUMA nodes to expose to the instance (does not override flavor definition).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Integer. For a detailed example of NUMA-topology definition, see the hw:NUMA_def key in <a class="link" href="index.html#section-add-metadata" title="7.3.4.2. Add Metadata">Add Metadata</a>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_numa_cpus.0</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Mapping of vCPUs N-M to NUMA node 0 (does not override flavor definition).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Comma-separated list of integers.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_numa_cpus.1</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Mapping of vCPUs N-M to NUMA node 1 (does not override flavor definition).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Comma-separated list of integers.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_numa_mem.0</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Mapping N MB of RAM to NUMA node 0 (does not override flavor definition).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Integer
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_numa_mem.1</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Mapping N MB of RAM to NUMA node 1 (does not override flavor definition).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Integer
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_qemu_guest_agent</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Guest agent support. If set to <code class="literal">yes</code>, and if <code class="literal">qemu-ga</code> is also installed, file systems can be quiesced (frozen) and snapshots created automatically.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">yes / no</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_rng_model</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Adds a random-number generator device to the image’s instances. The cloud administrator can enable and control device behavior by configuring the instance’s flavor. By default:
						</p>
						 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The generator device is disabled.
								</li><li class="listitem">
									/dev/random is used as the default entropy source. To specify a physical HW RNG device, set <code class="literal">rng_dev_path</code> to "/dev/hwrng" in your Compute environment file.
								</li></ul></div>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">virtio</code>, or other supported device.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_scsi_model</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Enables the use of VirtIO SCSI (virtio-scsi) to provide block device access for compute instances; by default, instances use VirtIO Block (virtio-blk). VirtIO SCSI is a para-virtualized SCSI controller device that provides improved scalability and performance, and supports advanced SCSI hardware.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">virtio-scsi</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_video_model</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The video device driver to use in virtual machine instances.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							List of supported drivers, in order of precedence:
						</p>
						 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									<code class="literal">virtio</code>. (Recommended) Virtual GPU with the Gallium GPU specification that uses the VIRGL renderer to render OpenGL. This GPU model is supported in all architectures, and can leverage hardware acceleration if the host has a dedicated GPU. For more information, see <a class="link" href="https://virgil3d.github.io/">https://virgil3d.github.io/</a>.
								</li><li class="listitem">
									<code class="literal">qxl</code>. High-performance driver for Spice or noVNC environments.
								</li><li class="listitem">
									<code class="literal">cirrus</code>. Legacy driver, use if the <code class="literal">QXL</code> driver is not available.
								</li><li class="listitem">
									<code class="literal">vga</code>. Use this driver for IBM Power environments.
								</li><li class="listitem">
									<code class="literal">gop</code>. Not supported for QEMU/KVM environments.
								</li><li class="listitem">
									<code class="literal">xen</code>. Not supported for KVM environments.
								</li><li class="listitem">
									<code class="literal">vmvga</code>. Legacy driver, do not use.
								</li><li class="listitem">
									<code class="literal">none</code>. Use this value to disable emulated graphics or video in virtual GPU (vGPU) instances where the driver is configured separately. For more information, see <a class="xref" href="index.html#ch-virtual_gpu" title="Chapter 11. Configuring virtual GPUs for instances">Chapter 11, <em>Configuring virtual GPUs for instances</em></a>.
								</li></ul></div>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_video_ram</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Maximum RAM for the video image. Used only if a <code class="literal">hw_video:ram_max_mb</code> value has been set in the flavor’s <code class="literal">extra_specs</code> and that value is higher than the value set in <code class="literal">hw_video_ram</code>.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							Integer in MB (for example, <span class="emphasis"><em>64</em></span>)
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_watchdog_action</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Enables a virtual hardware watchdog device that carries out the specified action if the server hangs. The watchdog uses the i6300esb device (emulating a PCI Intel 6300ESB). If <code class="literal">hw_watchdog_action</code> is not specified, the watchdog is disabled.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									disabled-The device is not attached. Allows the user to disable the watchdog for the image, even if it has been enabled using the image’s flavor. The default value for this parameter is disabled.
								</li><li class="listitem">
									reset-Forcefully reset the guest.
								</li><li class="listitem">
									poweroff-Forcefully power off the guest.
								</li><li class="listitem">
									pause-Pause the guest.
								</li><li class="listitem">
									none-Only enable the watchdog; do nothing if the server hangs.
								</li></ul></div>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">os_command_line</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The kernel command line to be used by the libvirt driver, instead of the default. For Linux Containers (LXC), the value is used as arguments for initialization. This key is valid only for Amazon kernel, ramdisk, or machine images (aki, ari, or ami).
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver and VMware API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">hw_vif_model</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Specifies the model of virtual network interface device to use.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							The valid options depend on the configured hypervisor.
						</p>
						 <div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									KVM and QEMU: e1000, ne2k_pci, pcnet, rtl8139, and virtio.
								</li><li class="listitem">
									VMware: e1000, e1000e, VirtualE1000, VirtualE1000e, VirtualPCNet32, VirtualSriovEthernetCard, and VirtualVmxnet.
								</li><li class="listitem">
									Xen: e1000, netfront, ne2k_pci, pcnet, and rtl8139.
								</li></ul></div>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							VMware API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">vmware_adaptertype</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The virtual SCSI or IDE controller used by the hypervisor.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">lsiLogic</code>, <code class="literal">busLogic</code>, or <code class="literal">ide</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							VMware API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">vmware_ostype</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							A VMware GuestID which describes the operating system installed in the image. This value is passed to the hypervisor when creating a virtual machine. If not specified, the key defaults to <code class="literal">otherGuest</code>.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							For more information, see <a class="link" href="https://docs.openstack.org/nova/train/admin/configuration/hypervisor-vmware.html#images-with-vmware-vsphere">Images with VMware vSphere</a>.
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							VMware API driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">vmware_image_version</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							Currently unused.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">1</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							XenAPI driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">auto_disk_config</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							If true, the root partition on the disk is automatically resized before the instance boots. This value is only taken into account by the Compute service when using a Xen-based hypervisor with the XenAPI driver. The Compute service will only attempt to resize if there is a single partition on the image, and only if the partition is in <code class="literal">ext3</code> or <code class="literal">ext4</code> format.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">true / false</code>
						</p>
						 </td></tr><tr><td align="left" valign="top" headers="idm139747270434608"> <p>
							libvirt API driver and XenAPI driver
						</p>
						 </td><td align="left" valign="top" headers="idm139747270433520"> <p>
							<code class="literal">os_type</code>
						</p>
						 </td><td align="left" valign="top" headers="idm139747270432432"> <p>
							The operating system installed on the image. The XenAPI driver contains logic that takes different actions depending on the value of the <code class="literal">os_type</code> parameter of the image. For example, for <code class="literal">os_type=windows</code> images, it creates a FAT32-based swap partition instead of a Linux swap partition, and it limits the injected host name to less than 16 characters.
						</p>
						 </td><td align="left" valign="top" headers="idm139747270431344"> <p>
							<code class="literal">linux</code> or <code class="literal">windows</code>
						</p>
						 </td></tr></tbody></table></div></div></section><section class="appendix" id="appx-enabling-launch-instance-wizard"><div class="titlepage"><div><div><h1 class="title">Appendix B. Enabling the Launch Instance Wizard</h1></div></div></div><p>
			There are two methods that you can use to launch instances from the dashboard:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					The Launch Instance form
				</li><li class="listitem">
					The Launch Instance wizard
				</li></ul></div><p>
			The Launch Instance form is enabled by default, but you can enable the Launch Instance wizard at any time. You can also enable both the Launch Instance form and the Launch Instance wizard at the same time. The Launch Instance wizard simplifies the steps required to create instances.
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
					Edit <code class="literal">/etc/openstack-dashboard/local_settings</code> file, and add the following values:
				</p><pre class="screen">LAUNCH_INSTANCE_LEGACY_ENABLED = False
LAUNCH_INSTANCE_NG_ENABLED = True</pre></li><li class="listitem"><p class="simpara">
					Restart the <span class="emphasis"><em>httpd</em></span> service:
				</p><pre class="screen"># systemctl restart httpd</pre></li></ol></div><p>
			The preferences for the Launch Instance form and Launch Instance wizard are updated.
		</p><p>
			If you enabled only one of these options, the <span class="strong strong"><strong>Launch Instance</strong></span> button in the dashboard opens that option by default. If you enabled both options, two <span class="strong strong"><strong>Launch Instance</strong></span> buttons are displayed in the dashboard, with the button on the left opening the Launch Instance wizard and the button on the right opening the Launch Instance form.
		</p></section><div><div xml:lang="en-US" class="legalnotice" id="idm139747270154416"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2020 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div>
  
  </div>
  </div>
</div>
<div id="comments-footer" class="book-comments">
  </div>
<meta itemscope="" itemref="md1">

    <!-- Display: Next/Previous Nav -->
          
      </div>
</article>


  

            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="index.html#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

            <div role="navigation">
                <h3>Quick Links</h3>
                <ul>
                    <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li><a class="manage-subscriptions" href="https://access.redhat.com/management/subscriptions/#active">Subscriptions</a></li>
                    <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                    <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                    <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Help</h3>
                <ul>
                    <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                    <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                    <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Site Info</h3>
                <ul>
                  <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                  <li><a class="browser-support-policy" href="https://access.redhat.com/help/browsers/">Browser Support Policy</a></li>
                  <li><a class="accessibility" href="https://access.redhat.com/help/accessibility/">Accessibility</a></li>
                  <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                  <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Related Sites</h3>
                <ul>
                    <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                    <li><a href="https://www.openshift.com" class="openshift-com">openshift.com</a></li>
                    <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                    <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>About</h3>
                <ul>
                    <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                    <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                    <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                </ul>
            </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                          <span class="status-description"></span>
                          <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2020 Red Hat, Inc.</div>

                        <div role="navigation" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://access.redhat.com/help/terms/" class="terms-of-use">Customer Portal Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                      <img src="rh-summit-red-a.svg" alt="Red Hat Summit" />
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHatSupport" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>
                        <a href="https://www.facebook.com/RedHatSupport" class="sm-icon facebook"><span class="nicon-facebook"></span><span class="offscreen">Facebook</span></a>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
  
  <div id="formatHelp" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="formatTitle" aria-hidden="true"><div class="modal-dialog"><div class="modal-content">
    <div class="modal-header">
      <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
      <h3 id="formatTitle">Formatting Tips</h3>
    </div>
    <div class="modal-body">
      <p>Here are the common uses of Markdown.</p><dl class="formatting-help">
        <dt class="codeblock">Code blocks</dt><dd class="codeblock"><pre><code>~~~
Code surrounded in tildes is easier to read
~~~</code></pre></dd>
        <dt class="urls">Links/URLs</dt><dd class="urls"><code>[Red Hat Customer Portal](https://access.redhat.com)</code></dd>
       </dl>
    </div>
    <div class="modal-footer">
      <a target="_blank" href="https://access.redhat.com/help/markdown" class="btn btn-primary">Learn more</a>
      <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
    </div>
  </div></div></div></body>
</html>

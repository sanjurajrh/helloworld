<!DOCTYPE html>
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video#" >

<head profile="http://www.w3.org/1999/xhtml/vocab">
	  <!--[if IE]><![endif]-->
<meta charset="utf-8" />
<meta name="revision" title="a1274284-ac29-47e7-970f-b757603146bd" product="7eff92b4-effc-4326-8604-6f852e3e9747" revision="c687f5d9fa69377007954f951f4a1287d6b8c85e:en-us" page="6440fead-315a-4531-9b33-d17332549456" />
<meta name="generator" content="Drupal 7 (http://drupal.org)" />
<link rel="canonical" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/index" />
<link rel="shortlink" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/index" />
<meta property="og:site_name" content="Red Hat Customer Portal" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/index" />
<meta property="og:title" content="Deploying an overcloud with containerized Red Hat Ceph Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta property="og:description" content="This guide provides information about using the Red Hat OpenStack Platform director to create an overcloud with a containerized Red Hat Ceph Storage cluster. This includes instructions for customizing your Ceph cluster through the director." />
<meta property="og:image" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:url" content="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/index" />
<meta name="twitter:title" content="Deploying an overcloud with containerized Red Hat Ceph Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal" />
<meta name="twitter:description" content="This guide provides information about using the Red Hat OpenStack Platform director to create an overcloud with a containerized Red Hat Ceph Storage cluster. This includes instructions for customizing your Ceph cluster through the director." />
<meta name="twitter:image:src" content="https://access.redhat.com/webassets/avalon/g/shadowman-200.png" />
  <title>Deploying an overcloud with containerized Red Hat Ceph Red Hat OpenStack Platform 16.1 | Red Hat Customer Portal</title>
  <meta charset="utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<!--[if IEMobile]><meta http-equiv="cleartype" content="on"><![endif]-->

<!-- metaInclude -->
<meta name="avalon-host-info" content="kcs05.web.prod.ext.phx2.redhat.com" />
<meta name="avalon-version" content="fc526cb5" />
<meta name="cp-chrome-build-date" content="2020-10-15T21:45:53.836Z" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-status-bar-style" content="#000000">
<link rel="manifest" href="https://access.redhat.com/webassets/avalon/j/manifest.json">
<!-- Open Search - Tap to search -->
<link rel="search" type="application/opensearchdescription+xml" title="Red Hat Customer Portal" href="https://access.redhat.com/webassets/avalon/j/opensearch.xml" />

 

<script type="text/javascript">
    window.portal = {
        analytics : {},
        host      : "https://access.redhat.com",
        idp_url   : "https://sso.redhat.com",
        lang      : "en",  
        version   : "fc526cb5",
        builddate : "2020-10-15T21:45:53.836Z",
        fetchdate : "2020-10-19T10:40:37-0400",
        nrid      : "14615289",
        nrlk      : "2a497fa56f"
    };
</script>
<script type="text/javascript">
    if (!/\/logout.*/.test(location.pathname) && portal.host === location.origin && document.cookie.indexOf('rh_sso_session') >= 0 && !(document.cookie.indexOf('rh_jwt') >= 0)) window.location = '/login?redirectTo=' + encodeURIComponent(window.location.href);
</script>
<!-- cssInclude -->

<link rel="shortcut icon" href="favicon.ico" />

<link media="all" rel="stylesheet" type="text/css" href="bootstrap.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="bootstrap-grid.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="main.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="components.css%3Fv=fc526cb5.css" />
<link media="all" rel="stylesheet" type="text/css" href="pages.css%3Fv=fc526cb5.css" />

<link href="chosen.css%3Fv=fc526cb5.css" rel="stylesheet" type="text/css" />

<!--[if lte IE 9]>
<link media="all" rel="stylesheet" type="text/css" href="https://access.redhat.com/chrome_themes/nimbus/css/ie.css" />
<![endif]-->

<noscript>
    <style type="text/css" media="screen"> .primary-nav { display: block; } </style>
</noscript>

<!-- /cssInclude -->
<script type="text/javascript" src="require.js%3Fv=fc526cb5" data-main="/webassets/avalon/j/"></script>

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<script src="https://access.redhat.com/chrome_themes/nimbus/js/ie8.js"></script>
<![endif]-->

  <!-- TrustArc -->
  <script src="https://static.redhat.com/libs/redhat/marketing/latest/trustarc/trustarc.js"></script>

  <link type="text/css" rel="stylesheet" href="css__c2Nkkx_5vYh8rvZbfBAGB4EMzMtH5ouFJDBlDsnhSR8__-ax0_8bam15ZDJT9j7LilCfJrDyEhGGAgc0KC8HYvJg__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__Jy3BSr8TrxptaufAzQDT1skBUlX2CnL_wm6BizzYuGw__-YZvqB8yuA4kK_iKklbk5HdZCoG2qAgz1l-8Qi2NFH4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="messages.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__m76iIFREtc70Nmw5xe1ZwHbNBlwOP2Zjc3DcacNWnFQ__STEh8aY3w8E_bKzhB4Xke2WOQ9XMDQquHIP5B8SeDIY__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="all" />
<link type="text/css" rel="stylesheet" href="css__4sM4s6XOQ2Nm0pdkrWRLvrgtwpTmHAFzR_LcdesKYj8__vJ0jjVEhoPh3KBeLZfkqg51T3AFxhQCuXg1reipa__k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.css" media="print" />
  <link rel="stylesheet" type="text/css" media="all" href="list.css" />
  <script src="js__ZyeOaiFuDejQQbhUV7yg7atYZnj4WLfH77o0scv4068__MZdWWgUEYpsEWLcU0RqkaXMsEyksbpCgnf4XwXRkqz0__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__5ysXPc5KIyMmizxqRY68ILfrEGrj0P29WBIifnPTJvQ__Cap0DACEVMsefumg1gS1APLLd8stDkdGfp6c1uswMo4__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>chrometwo_require(["analytics/attributes"], function(attributes) {
attributes.set('ResourceID',       'a1274284-ac29-47e7-970f-b757603146bd');
attributes.set('ResourceTitle',    'Deploying an overcloud with containerized Red Hat Ceph');
attributes.set('Language',         'en-us');
attributes.set('RevisionId',       'c687f5d9fa69377007954f951f4a1287d6b8c85e:en-us');
attributes.set('PublicationState', ['active','published']);
attributes.set('Product',          'Red Hat OpenStack Platform');
attributes.set('ProductVersion',   '16.1');
attributes.set('ProductId',        'Red Hat OpenStack Platform 16.1');
});</script>
<script>breadcrumbs = [ ["Products &amp; Services", "/products/"], ["Product Documentation", "/documentation/"], ["Red Hat OpenStack Platform", "/documentation/en-us/red_hat_openstack_platform/"], ["16.1", "/documentation/en-us/red_hat_openstack_platform/16.1/"], ["Deploying an overcloud with containerized Red Hat Ceph", "/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/"] ];</script>
<script>window.siteMapState = "products";</script>
<script src="js__i6ieGBO-OPmIKm_f0srsb6gM7QELIWrBpKh6ub_yj8A__wjRg_duSK4rEZCRV2uwrCIPMl80Z_LJ9ew61H5hE-ZI__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script src="js__vGeEtorfXGI3aPAp71YA74N5p8wfpYVhnoIqwaJrOiQ__8abcou7ybLtGOLy-V9E-NQlDejOMZmdStjlYfyX9W-k__7oVp6UW_380R7g-dHXPK35SfMqy4yUfhIx8G4wh5Zdk.js"></script>
<script>jQuery.extend(Drupal.settings, {"basePath":"\/","pathPrefix":"","ajaxPageState":{"theme":"kcs","theme_token":"tnqVUByd6OfVpWs7G_zn3eZ17NT2FVU1rjjc3_bTBCA","js":{"misc\/jquery-extend-3.4.0.js":1,"misc\/jquery-html-prefilter-3.5.0-backport.js":1,"misc\/jquery.once.js":1,"misc\/drupal.js":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.jquery.min.js":1,"modules\/chosen\/chosen.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/prism.js":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/js\/underscore.js":1,"sites\/all\/themes\/kcs\/js\/kcs_base.js":1,"sites\/all\/themes\/kcs\/js\/showdown.js":1,"sites\/all\/themes\/kcs\/js\/case_links.js":1},"css":{"modules\/system\/system.base.css":1,"modules\/system\/system.menus.css":1,"modules\/system\/system.messages.css":1,"modules\/book\/book.css":1,"modules\/comment\/comment.css":1,"modules\/date\/date_api\/date.css":1,"modules\/date\/date_popup\/themes\/datepicker.1.7.css":1,"modules\/field\/theme\/field.css":1,"modules\/node\/node.css":1,"modules\/search\/search.css":1,"modules\/user\/user.css":1,"sites\/all\/modules\/contrib\/user_prune\/css\/user_prune.css":1,"modules\/views\/css\/views.css":1,"sites\/all\/libraries\/chosen\/chosen\/chosen.css":1,"modules\/ctools\/css\/ctools.css":1,"sites\/all\/modules\/contrib\/panels\/css\/panels.css":1,"sites\/all\/modules\/custom\/rate\/rate.css":1,"sites\/all\/modules\/custom\/rh_doc_fetcher\/plugins\/layouts\/docs_page\/docs_page.css":1,"https:\/\/access.redhat.com\/webassets\/avalon\/s\/messages.css":1,"sites\/all\/themes\/zen\/system.base.css":1,"sites\/all\/themes\/zen\/system.menus.css":1,"sites\/all\/themes\/zen\/system.messages.css":1,"sites\/all\/themes\/zen\/system.theme.css":1,"sites\/all\/themes\/zen\/comment.css":1,"sites\/all\/themes\/zen\/node.css":1,"sites\/all\/themes\/kcs\/css\/html-reset.css":1,"sites\/all\/themes\/kcs\/css\/wireframes.css":1,"sites\/all\/themes\/kcs\/css\/layout-fixed.css":1,"sites\/all\/themes\/kcs\/css\/page-backgrounds.css":1,"sites\/all\/themes\/kcs\/css\/tabs.css":1,"sites\/all\/themes\/kcs\/css\/pages.css":1,"sites\/all\/themes\/kcs\/css\/blocks.css":1,"sites\/all\/themes\/kcs\/css\/navigation.css":1,"sites\/all\/themes\/kcs\/css\/views-styles.css":1,"sites\/all\/themes\/kcs\/css\/nodes.css":1,"sites\/all\/themes\/kcs\/css\/comments.css":1,"sites\/all\/themes\/kcs\/css\/forms.css":1,"sites\/all\/themes\/kcs\/css\/fields.css":1,"sites\/all\/themes\/kcs\/css\/kcs.css":1,"sites\/all\/themes\/kcs\/css\/print.css":1}},"chosen":{"selector":"#edit-field-kcs-component-select-und, #edit-field-kcs-sbr-select-und, #edit-field-kcs-product-select-und, #edit-field-kcs-type-select-und, #edit-language, #edit-field-kcs-a-category-select-und, #edit-field-kcs-state-select-und, #edit-field-kcs-tags-select-und, #edit-field-kcs-state-select-value, #edit-field-vid-reference-product-und, #edit-state, #edit-product, #edit-category, #edit-field-kcs-product-select-tid, #edit-field-kcs-type-select-tid, #edit-field-kcs-a-category-select-tid, #edit-sort-bef-combine, #edit-kcs-state, #edit-field-category-tid, #edit-field-product-tid, #edit-field-tags-tid, #edit-field-category-und, #edit-field-product-und, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-category, #edit-field-tags-und, #views-exposed-form-questions-list-questions-filter-block #edit-field-product, #views-exposed-form-questions-list-questions-filter-block #edit-field-tags, #edit-field-mega-menu-tab-und, #edit-field-internal-tags-tid, #edit-tags, #edit-field-kcs-tags-select-tid, #edit-subscriptions-and-choose-field-select, #edit-subscriptions-and-type, #edit-vid-13, #edit-vid-4, #edit-vid-5, #edit-vid-53, #edit-vid-1, #edit-vid-3, #edit-group-blog, #edit-subscriptions-and-choose-field-select-two, #edit-subscriptions-and-type-two, #edit-vid-13-two, #edit-vid-4-two, #edit-vid-5-two, #edit-vid-53-two, #edit-vid-1-two, #edit-vid-3-two, #edit-group-blog-two, #edit-subscriptions-and-edit-choose-field-select, #edit-subscriptions-and-type-edit, #edit-subscriptions-and-edit-type-two, #edit-vid-13-edit-two, #edit-vid-4-edit-two, #edit-vid-5-edit-two, #edit-vid-53-edit-two, #edit-vid-1-edit-two, #edit-vid-3-edit-two, #edit-subscriptions-and-edit-choose-field-select-two, #edit-vid-13-edit, #edit-vid-4-edit, #edit-vid-5-edit, #edit-vid-53-edit, #edit-vid-1-edit, #edit-vid-3-edit, #edit-group-blog-edit, #edit-field-supported-languages-tid, #edit-field-geography-und, #edit-field-supported-products-und, #edit-field-supported-languages-und, #edit-field-vendor-und, #edit-field-errata-type-text-und, #edit-field-errata-severity-text-und, #edit-field-software-partner-level-und, #edit-field-scert-product-category-und, #edit-field-certifications-und-0-field-product-und, #edit-kcs-article-type, #edit-field-eco-industry-tag-select-tid, #edit-field-eco-software-catego-select-tid, #edit-field-ecosystem-tag-select-und, #edit-field-eco-industry-tag-select-und, #edit-field-eco-software-catego-select-und, #edit-field-vendor-tsanet-member-ref-und, #edit-field-og-vendor-ref-und-0-default, #edit-field-eco-products-enabled-col-und-0-field-eco-subscription-model-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-support-level-ref-und, #edit-field-eco-products-enabled-col-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-select-und, #edit-field-eco-certifications-select-und-0-field-eco-product-archite-select-und, #edit-field-eco-certifications-select-und-0-field-eco-certificati-lvl-select-und, #edit-field-eco-certifications-select-und-0-field-eco-hypervisor-str-und, #edit-field-eco-supported-language-ref-und, #edit-field-eco-region-ref-und, #edit-field-cs-product-category-str-und, #edit-field-eco-certifications-select-und-0-field-eco-format-ref-und, #edit-field-eco-group-access-ref-und, #edit-field-hw-category-tag-ref-und, #edit-field-profile-industry-und, #edit-field-profile-tech-interests-und, #edit-field-product-page-features-ref-und, #edit-field-eco-product-select-und, #edit-field-base-product-ref-und, #edit-field-eco-product-archite-select-und, #edit-field-eco-format-ref-und, #edit-field-certification-status-ref-und, #edit-field-certification-result-ref-und, #edit-field-og-certified-product-ref-und-0-default, #edit-field-eco-subscription-model-ref-und, #edit-field-eco-support-level-ref-und, #edit-field-eco-certificati-lvl-select-und, #edit-field-eco-hypervisor-str-und, #edit-field-ccp-thirdparty-cert-select-und, .use_, #edit-field-eco-cert-product-tag-und, #edit-field-documentation-location-ref-und, #edit-field-documentation-title-und, #edit-field-accelerator-products-und","minimum":"0"},"rh_doc_fetcher":{"page_type":"single"},"section":"","kcs":{"nodeType":null,"nodeId":null}});</script>
    <!--[if lt IE 9]>
  <script src="https://access.redhat.com/sites/all/themes/kcs/js/html5shiv.js"></script>
  <![endif]-->
  
      


    <!--kcs06-->
<script type="text/javascript">
  Drupal.portal = {"version":{"redhat_portal":"package drupal7-redhat_portal is not installed"}};
  Drupal.portal.currentUser = {};
  </script>

</head>

<body class="portal-page  kcs_external" >
  
  <div id="page-wrap" class="page-wrap">
    <div class="top-page-wrap">

        <!--googleoff: all-->
        <header class="masthead" id="masthead">

            <script>
    chrometwo_require(["wc"], function(wc){    
        wc.include("@cpelements/cp-search-autocomplete/dist/cp-search-autocomplete.umd");
    }); 
</script>

<!-- Accessibility Nav & Header -->
<div class="accessibility-nav sr-only">
    <a href="https://access.redhat.com/">
        <h1>Red Hat <span>Customer </span><span>Portal</span></h1>
    </a>
    <p><a href="index.html#cp-main">Skip to main content</a></p>
    <nav aria-labelledby="accessibility-nav-heading">
        <h2 id="accessibility-nav-heading">Main Navigation</h2>
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="accessibility-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="accessibility-accountUser">
                    <div class="account-name"><strong id="accessibility-userFullName"></strong></div>
                    <div class="account-org"><span id="accessibility-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="accessibility-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="accessibility-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="accessibility-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="accessibility-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="accessibility-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="accessibility-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- Mobile Header -->
<nav class="mobile-nav-bar hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <button id="menu-btn" class="menu menu-white" type="button" aria-label="Toggle Navigation">
        <span class="lines"></span>
    </button>
    <a class="logo" href="https://access.redhat.com/">
        <span class="logo-crop">
          <!-- logo -->
          <span class="sr-only">Red Hat Customer Portal</span>
          <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
            <defs>
            <style>
              .rh-logo-type {
                fill:#fff;
              }
              .rh-logo-hat {
                fill:#e00;
              }
            </style>
            </defs>
            <g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
          </svg>
        </span>
            </a>
    <button class="btn btn-search btn-utility" data-target="#site-search">
        <span class="web-icon-search"></span>
        <span class="link-text">Search</span>
    </button>
</nav>

<!-- Mobile Menu Drawer -->
<div class="nav-drawer mobile-nav-drawer hidden-sm hidden-md hidden-lg" aria-hidden="true">
    <nav class="nav-container">
        <ul>
    <li>
        <a href="index.html#" class="has-subnav"><span>Products &amp; Services</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/products">View All Products</a></li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Infrastructure and Management</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a></li>
                    <li><a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|p|im|rhaijan2016&">Red Hat Insights</a></li>
                    <li><a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Cloud Computing</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a></li>
                    <li><a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Storage</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a></li>
                    <li><a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a></li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Runtimes</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true">
                      <a href="index.html#" class="back">Back</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                    </li>
                </ul>
            </li>
            <li>
                <a href="index.html#" class="has-subnav"><span>Integration and Automation</span></a>
                <ul class="mm-hide">
                    <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                    </li>
                    <li>
                      <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                    </li>
                    <li>
                        <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                    </li>
                </ul>
            </li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/support/">Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
            <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
            <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>

            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://access.redhat.com/documentation/">Documentation</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
            <li><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>

            <li class="heading">Services</li>
            <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
            <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
            <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>

            
            <li class="heading"><a class="cta-link cta-link-darkbg" href="https://catalog.redhat.com/">Ecosystem Catalog</a></li>
            <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
            <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
          </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Tools</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/insights/info/?intcmp=mm|t|c1|rhaidec2015&">Red Hat Insights</a></li>
            <li class="heading">Tools</li>
            <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
            <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
            <li><a href="https://access.redhat.com/errata/">Errata</a></li>
            <li class="heading">Customer Portal Labs</li>
            <li><a href="https://access.redhat.com/labs/">Explore Labs</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
            <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Security</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/security/">Product Security Center</a></li>
            <li class="heading">Security Updates</li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
            <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
            <li class="heading">Resources</li>
            <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
            <li><a href="https://access.redhat.com/blogs/product-security">Security Blog</a></li>
            <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
            <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
            <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
            <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Community</span></a>
        <ul class="mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="heading">Customer Portal Community</li>
            <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
            <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
            <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
            <li><a href="https://access.redhat.com/community/">Community Activity</a></li>
            <li class="heading">Customer Events</li>
            <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
            <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
            <li class="heading">Stories</li>
            <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
            <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
            <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
        </ul>
    </li>
    <li><a href="https://access.redhat.com/management/">Subscriptions</a></li>
    <li><a href="https://access.redhat.com/downloads/">Downloads</a></li>
    <li><a href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
    <li><a href="https://access.redhat.com/support/cases/">Support Cases</a></li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Account</span></a>
        <ul class="utility mm-hide">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://access.redhat.com/login" id="mobile-accountLogin">Log In</a></li>
            <li class="mobile-accountLinksLoggedOut unauthenticated"><a href="https://www.redhat.com/wapps/ugc/register.html">Register</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated">
                <div class="account-info" id="mobile-accountUser">
                    <div class="account-name"><strong id="mobile-userFullName"></strong></div>
                    <div class="account-org"><span id="mobile-userOrg"></span></div>
                    <div class="account-number mobile-accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                </div>
            </li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
            <!-- <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user">My Profile</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/user" id="mobile-userNotificationsLink">Notifications</a></li>
            <li class="mobile-accountLinksLoggedIn authenticated"><a href="https://access.redhat.com/help/">Help</a></li>
            <li class="logout mobile-accountLinksLoggedIn authenticated"><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout">Log Out</a></li>
        </ul>
    </li>
    <li>
        <a href="index.html#" class="has-subnav"><span>Language</span></a>
        <ul class="utility mm-hide" id="mobile-localesMenu">
            <li aria-hidden="true"><a href="index.html#" class="back">Back</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=en" id="mobile-en">English</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ko" id="mobile-ko">한국어</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=ja" id="mobile-ja">日本語</a></li>
            <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" id="mobile-zh_CN">中文 (中国)</a></li>
		</ul>
    </li>
</ul>
    </nav>
</div>

<!-- White Utility Menu for large screens -->
<div class="utility-wrap" aria-hidden="true">
    <div class="utility-container">
        <div class="utility-bar hidden-xs">
            <div role="navigation" class="top-nav">
                <ul>
                    <li id="nav-subscription" data-portal-tour-1="1"><a class="top-nav-subscriptions" href="https://access.redhat.com/management/">Subscriptions</a></li>
                    <li id="nav-downloads" data-portal-tour-1="2"><a class="top-nav-downloads" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li id="nav-containers"><a class="top-nav-containers" href="https://catalog.redhat.com/software/containers/explore/">Containers</a></li>
                    <li id="nav-support" data-portal-tour-1="3"><a class="top-nav-support-cases" href="https://access.redhat.com/support/cases/">Support Cases</a></li>
                </ul>
            </div>

            <div role="navigation" class="utility-nav">
                <ul>
                    <li id="searchABTestHide">
                        <a class="btn-search" data-target="#site-search" title="Search" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-search" aria-label="search"></span>
                            <span class="link-text">Search</span>
                        </a>
                    </li>
                    <!-- AB Test -->
                    <li id="searchABTestShow">
                        <form id="topSearchFormABTest" name="topSearchFormABTest">
                            <label for="topSearchInputABTest" class="sr-only">Search</label>
                            <input id="topSearchInputABTest" name="keyword" placeholder="Search" value="" type="text" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" class="form-control">
                            <button type="submit" class="btn btn-app btn-sm btn-link"><span class="web-icon-search" aria-label="search"></span></button>
                        </form>
                    </li>
                    <!-- End of AB Test -->
                    <li>
                        <a class="btn-profile" data-target="#account-info" data-portal-tour-1="4a" title="Account" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-user" aria-label="log in"></span>
                            <span class="link-text">Log In</span>
                            <span class="account-user" id="accountUserName"></span>
                        </a>
                    </li>
                    <li>
                        <a class="btn-language" data-target="#language" data-portal-tour-1="6" title="Language" data-toggle="tooltip" data-placement="bottom">
                            <span class="web-icon-globe" aria-label="language"></span>
                            <span class="link-text">Language</span>
                        </a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- Utility Tray -->
        <div class="utility-tray-container">
            <div class="utility-tray">
                <div id="site-search" class="utility-link site-search">
                    <div class="content">
                        <form class="ng-pristine ng-valid topSearchForm" id="topSearchForm" name="topSearchForm" action="https://access.redhat.com/search/browse/search/" method="get" enctype="application/x-www-form-urlencoded">
                            <cp-search-autocomplete class="push-bottom" path="/webassets/avalon/j/data.json"></cp-search-autocomplete>
                            <div class="input-group push-bottom">
                                <input class="form-control searchField" id="topSearchInput" name="keyword" value="" placeholder="Enter your search term" type="text">
                                <span class="input-group-btn">
                                    <button type="submit" class="btn btn-primary">Search</button>
                                </span>
                            </div>
                            <div>Or <a href="https://access.redhat.com/support/cases/#/troubleshoot">troubleshoot an issue</a>.</div>
                        </form>
                    </div>
                </div>
                <div id="account-info" class="utility-link account-info">
                    <div class="content">

                        <!-- Account Unauthenticated -->
                        <div id="accountLinksLoggedOut" class="unauthenticated">
                            <h2 class="utility-header">Log in to Your Red Hat Account</h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border">
                                    <p><a href="https://access.redhat.com/login" id="accountLogin" class="btn btn-primary">Log In</a></p>
                                    <p>Your Red Hat account gives you access to your profile, preferences, and services, depending on your status.</p>
                                </div>
                                <div class="col-sm-6 col-border col-border-left">
                                    <p><a href="https://www.redhat.com/wapps/ugc/register.html" class="btn btn-primary">Register</a></p>
                                    <p>If you are a new customer, register now for access to product evaluations and purchasing capabilities. </p>

                                    <strong>Need access to an account?</strong><p>If your company has an existing Red Hat account, your organization administrator can grant you access.</p>

                                    <p><a href="https://access.redhat.com/support/contact/customerService/">If you have any questions, please contact customer service.</a></p>
                                </div>
                            </div>
                        </div>

                        <!-- Account Authenticated -->
                        <div id="accountLinksLoggedIn" class="authenticated">
                            <h2 class="utility-header"><span id="userFirstName"></span></h2>
                            <div class="row col-border-row">
                                <div class="col-sm-6 col-border col-border-right">
                                    <div class="account-info" id="accountUser">
                                        <div class="avatar"><!-- placeholder--></div>
                                        <div class="account-name"><strong id="userFullName"></strong></div>
                                        <div class="account-org"><span id="userOrg"></span></div>
                                        <div class="account-number accountNumber"><span class="property">Red Hat Account Number:</span> <span class="value"></span></div>
                                    </div>

                                    <div class="row account-settings">
                                        <div class="col-md-6" data-portal-tour-1="4">
                                            <h3>Red Hat Account</h3>
                                            <ul class="reset">
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/personalInfo.html">Account Details</a></li>
                                                <!-- <li><a href="https://www.redhat.com/wapps/ugc/protected/prefs.html">Newsletter and Contact Preferences</a></li> -->
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/usermgt/userList.html" class="user-management-link">User Management</a></li>
                                                <li><a href="https://www.redhat.com/wapps/ugc/protected/account.html">Account Maintenance</a></li>
                                                <li><a href="https://access.redhat.com/account-team">Account Team</a></li>
                                            </ul>
                                        </div>
                                        <div class="col-md-6" data-portal-tour-1="5">
                                            <h3>Customer Portal</h3>
                                            <ul class="reset">
                                                <li><a href="https://access.redhat.com/user">My Profile</a></li>
                                                <li><a href="https://access.redhat.com/user" id="userNotificationsLink">Notifications</a></li>
                                                <li><a href="https://access.redhat.com/help/">Help</a></li>
                                            </ul>
                                        </div>
                                    </div>

                                </div>
                                <div class="col-sm-6 col-border">
                                    <p>For your security, if you’re on a public computer and have finished using your Red Hat services, please be sure to log out.</p>
                                    <p><a href="https://sso.redhat.com/auth/realms/redhat-external/logout?redirectUrl=https%3A%2F%2Faccess.redhat.com%2Flogout" id="accountLogout" class="btn btn-primary">Log Out</a></p>
                                </div>
                            </div>
                        </div>

                    </div>
                </div>
                <div id="language" class="utility-link language">
                    <div class="content">
                        <h2 class="utility-header">Select Your Language</h2>
                        <div class="row" id="localesMenu">
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=en" data-lang="en" id="en">English</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ko" data-lang="ko" id="ko">한국어</a></li>
                                </ul>
                            </div>
                            <div class="col-sm-2">
                                <ul class="reset">
                                    <li><a href="https://access.redhat.com/changeLanguage?language=ja"    data-lang="ja"    id="ja">日本語</a></li>
                                    <li><a href="https://access.redhat.com/changeLanguage?language=zh_CN" data-lang="zh_CN" id="zh_CN">中文 (中国)</a></li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<div id="scroll-anchor"></div>

<!-- Main Menu for large screens -->
<div class="header-nav visible-sm visible-md visible-lg" aria-hidden="true">
    <div id="header-nav">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">

                    <a href="https://access.redhat.com/" class="logo">
                      <span class="sr-only">Red Hat Customer Portal</span>
                      <span class="logo-crop">
                        <svg aria-hidden="true" class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 950 200">
                          <defs>
                          <style>
                            .rh-logo-type {
                              fill:#fff;
                            }
                            .rh-logo-hat {
                              fill:#e00;
                            }
                          </style>
                          </defs>
                          <title>Red Hat Customer Portal</title><g id="Two_lines" data-name="Two lines"><g id="Two_line_logo" data-name="Two line logo"><path class="rh-logo-type" d="M318.62,9.25h25.44c11.13,0,18.64,6.72,18.64,16.56,0,7.36-4.47,13-11.51,15.36l12.48,24.08h-9.29l-11.6-23H327v23h-8.4Zm8.4,7.36V35.33h16.33c6.55,0,10.87-3.76,10.87-9.36s-4.32-9.36-10.87-9.36Z"/><path class="rh-logo-type" d="M387.5,66a21,21,0,0,1-21.36-21.12c0-11.76,9-21,20.4-21,11.2,0,19.68,9.28,19.68,21.28v2.32H374.06a13.74,13.74,0,0,0,13.76,11.68,15.84,15.84,0,0,0,10.32-3.6l5.13,5A24.33,24.33,0,0,1,387.5,66ZM374.14,41.49H398.3c-1.19-6.24-6-10.88-11.92-10.88C380.22,30.61,375.34,35,374.14,41.49Z"/><path class="rh-logo-type" d="M445.18,61.41a19.23,19.23,0,0,1-12.48,4.48c-11.52,0-20.56-9.2-20.56-21a20.72,20.72,0,0,1,33-17V9.25l8-1.76V65.25h-7.92Zm-11.36-2.48a14.67,14.67,0,0,0,11.28-4.88V35.57a14.89,14.89,0,0,0-11.28-4.8,13.63,13.63,0,0,0-13.84,14A13.77,13.77,0,0,0,433.82,58.93Z"/><path class="rh-logo-type" d="M480.22,9.25h8.4v24h29.76v-24h8.4v56h-8.4V40.85H488.62v24.4h-8.4Z"/><path class="rh-logo-type" d="M534.38,53.57c0-7.68,6.24-12.4,16.48-12.4A28.75,28.75,0,0,1,562,43.41V39.09c0-5.76-3.44-8.64-9.92-8.64-3.76,0-7.6,1-12.64,3.44l-3-6c6.08-2.88,11.36-4.16,16.72-4.16,10.56,0,16.64,5.2,16.64,14.56v27H562V61.73A19.32,19.32,0,0,1,549.34,66C540.46,66,534.38,60.93,534.38,53.57Zm16.8,6.48A15.66,15.66,0,0,0,562,56.21v-7a21.15,21.15,0,0,0-10.48-2.48c-5.84,0-9.44,2.64-9.44,6.72C542.06,57.33,545.74,60.05,551.18,60.05Z"/><path class="rh-logo-type" d="M582.7,31.25h-8.64V24.53h8.64V14.13l7.92-1.92V24.53h12v6.72h-12V53.33c0,4.16,1.68,5.68,6,5.68a15.72,15.72,0,0,0,5.84-1v6.72a26.5,26.5,0,0,1-7.6,1.2c-7.92,0-12.16-3.76-12.16-10.8Z"/><path class="rh-logo-type" d="M361,132.21l7.59,7.52a30.76,30.76,0,0,1-23,10.32c-16.88,0-29.84-12.56-29.84-28.8s13-28.88,29.84-28.88c9,0,18.09,4.08,23.28,10.48l-7.83,7.76a19.52,19.52,0,0,0-15.45-7.6c-10.16,0-17.92,7.84-17.92,18.24A17.8,17.8,0,0,0,346,139.33,19.24,19.24,0,0,0,361,132.21Z"/><path class="rh-logo-type" d="M383.74,131.81c0,5.36,3.44,8.8,8.64,8.8a10.05,10.05,0,0,0,8.65-4.16V107.57h11v41.68H401v-3.36a17.79,17.79,0,0,1-11.77,4.16c-9.68,0-16.48-6.88-16.48-16.64V107.57h11Z"/><path class="rh-logo-type" d="M422.46,137c4.88,3.2,9.12,4.72,13.52,4.72,4.88,0,8.08-1.76,8.08-4.4,0-2.16-1.6-3.36-5.2-3.92l-8-1.2c-8.24-1.28-12.64-5.36-12.64-12.08,0-8.08,6.72-13.2,17.36-13.2a30.75,30.75,0,0,1,17.12,5.2l-5.28,7c-4.56-2.72-8.64-4-12.88-4-4,0-6.56,1.6-6.56,4.08,0,2.24,1.6,3.36,5.68,3.92l8,1.2c8.16,1.2,12.72,5.44,12.72,11.92,0,7.84-7.76,13.76-18.24,13.76-7.6,0-14.4-2-19.12-5.76Z"/><path class="rh-logo-type" d="M464.7,116.77h-8.56v-9.2h8.56V96.93l11-2.48v13.12H487.5v9.2H475.66v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M512.86,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S500.38,106.77,512.86,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S524.38,135.17,524.38,128.45Z"/><path class="rh-logo-type" d="M541.82,107.57h11v3.12a16.21,16.21,0,0,1,10.88-3.92A15,15,0,0,1,576.22,113,17.16,17.16,0,0,1,590,106.77c9.36,0,15.91,6.8,15.91,16.56v25.92h-11V124.93c0-5.28-3-8.72-7.83-8.72a9.37,9.37,0,0,0-8,4.16,18.89,18.89,0,0,1,.23,3v25.92h-11V124.93c0-5.28-3-8.72-7.84-8.72a9.3,9.3,0,0,0-7.76,3.76v29.28h-11Z"/><path class="rh-logo-type" d="M634.78,150.05c-12.64,0-22.4-9.44-22.4-21.6a21.28,21.28,0,0,1,21.44-21.6c11.84,0,20.64,9.6,20.64,22.4v2.88h-31a12,12,0,0,0,11.84,8.72,13.12,13.12,0,0,0,9.2-3.36l7.2,6.56A25,25,0,0,1,634.78,150.05Zm-11.44-25.76h20.4c-1.36-5-5.36-8.4-10.16-8.4C628.54,115.89,624.7,119.17,623.34,124.29Z"/><path class="rh-logo-type" d="M661.18,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M710.54,93.25h28.08c11,0,18.8,7.28,18.8,17.6,0,10-7.92,17.28-18.8,17.28H722.14v21.12h-11.6Zm11.6,10v15.28h15.2c5,0,8.32-3,8.32-7.6s-3.28-7.68-8.32-7.68Z"/><path class="rh-logo-type" d="M782.14,106.77c12.48,0,22.24,9.52,22.24,21.68s-9.76,21.6-22.24,21.6-22.24-9.44-22.24-21.6S769.66,106.77,782.14,106.77Zm11.52,21.68c0-6.88-5-12.16-11.52-12.16s-11.52,5.28-11.52,12.16c0,6.72,5,12.08,11.52,12.08S793.66,135.17,793.66,128.45Z"/><path class="rh-logo-type" d="M811.1,107.57h11v4.56a13.26,13.26,0,0,1,11.12-5.52,9.44,9.44,0,0,1,4.56,1v9.6a14,14,0,0,0-5.6-1.12,11,11,0,0,0-10.08,6.24v27h-11Z"/><path class="rh-logo-type" d="M850,116.77h-8.56v-9.2H850V96.93l11-2.48v13.12h11.84v9.2H860.94v18.48c0,3.92,1.52,5.36,5.76,5.36a16.86,16.86,0,0,0,5.84-1v9a34,34,0,0,1-8.48,1.28c-9.28,0-14.08-4.24-14.08-12.4Z"/><path class="rh-logo-type" d="M876.22,137.17c0-7.92,6.4-12.64,17.12-12.64a33.71,33.71,0,0,1,10.16,1.6v-3c0-4.8-3-7.28-8.8-7.28-3.52,0-7.44,1.12-12.72,3.44l-4-8.08a43.46,43.46,0,0,1,18.56-4.48c11.28,0,17.76,5.6,17.76,15.44v27H903.5v-2.88a19.81,19.81,0,0,1-12.08,3.6C882.46,150,876.22,144.77,876.22,137.17Zm18.08,5a15.78,15.78,0,0,0,9.2-2.64v-6.24a24.22,24.22,0,0,0-8.8-1.52c-5,0-8,2-8,5.2S889.66,142.13,894.3,142.13Z"/><path class="rh-logo-type" d="M933.58,149.25h-11v-56l11-2.4Z"/><g id="Hat_icon" data-name="Hat icon"><path id="Red_hat" data-name="Red hat" class="rh-logo-hat" d="M129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42L151.82,39c-1.72-7.12-3.23-10.35-15.74-16.6-9.7-5-30.82-13.15-37.07-13.15-5.83,0-7.55,7.54-14.45,7.54-6.68,0-11.64-5.6-17.89-5.6-6,0-9.92,4.1-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24Zm32.55-11.42c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33C23.77,60.77,2,63.79,2,83c0,31.48,74.59,70.28,133.65,70.28,45.27,0,56.7-20.48,56.7-36.65C192.35,103.88,181.35,89.44,161.52,80.82Z"/><path class="rh-logo-band" id="Black_band" data-name="Black band" d="M161.52,80.82c1.72,8.19,1.72,9.05,1.72,10.13,0,14-15.73,21.77-36.43,21.77-46.77,0-87.73-27.37-87.73-45.48a18.32,18.32,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,44,52.79C44,62,80.32,92.24,129,92.24c12.5,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/></g><path id="Dividing_line" data-name="Dividing line" class="rh-logo-type" d="M255.47,160.75a2.25,2.25,0,0,1-2.25-2.25V4.25a2.25,2.25,0,0,1,4.5,0V158.5A2.25,2.25,0,0,1,255.47,160.75Z"/></g></g>
                        </svg>
                      </span>
                                            </a>

                    <nav class="primary-nav hidden-sm">
                        <ul>
                            <li id="nav-products"><a class="products" data-link="mega" data-target="products-menu" href="https://access.redhat.com/products/" id="products-menu">Products &amp; Services</a></li>
                            <li id="nav-tools"><a class="tools" data-link="mega" data-target="tools-menu" href="https://access.redhat.com/labs/" id="tools-menu">Tools</a></li>
                            <li id="nav-security"><a class="security" data-link="mega" data-target="security-menu" href="https://access.redhat.com/security/" id="security-menu">Security</a></li>
                            <li id="nav-community"><a class="community" data-link="mega" data-target="community-menu" href="https://access.redhat.com/community/" id="community-menu">Community</a></li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Main Mega Menu for large screens -->
<div class="mega-wrap visible-sm visible-md visible-lg" aria-hidden="true">
    <nav class="mega">
        <div class="container">
            <div class="mega-menu-wrap">

                <!-- Products Menu -->
                <div aria-labelledby="products-menu" class="products-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-md-6 col-sm-8">
                            <div class="root clearfix" data-portal-tour-1="7">
                                <ul class="subnav subnav-root">
                                    <li data-target="infrastructure-menu" class="active">
                                        <h3>Infrastructure and Management</h3>
                                    </li>
                                    <li data-target="cloud-menu">
                                        <h3>Cloud Computing</h3>
                                    </li>
                                    <li data-target="storage-menu">
                                        <h3>Storage</h3>
                                    </li>
                                    <li data-target="jboss-dev-menu">
                                        <h3>Runtimes</h3>
                                    </li>
                                    <li data-target="jboss-int-menu">
                                        <h3>Integration and Automation</h3>
                                    </li>
                                </ul>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="infrastructure-menu" style="display: block;">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-enterprise-linux/">Red Hat Enterprise Linux</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-virtualization/">Red Hat Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/identity-management/">Red Hat Identity Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-directory-server/">Red Hat Directory Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-certificate-system/">Red Hat Certificate System</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-satellite/">Red Hat Satellite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-subscription-management/">Red Hat Subscription Management</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-update-infrastructure/">Red Hat Update Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-insights/">Red Hat Insights</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/ansible-tower-red-hat/">Red Hat Ansible Tower</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ansible-engine/">Red Hat Ansible Engine</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="cloud-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloudforms/">Red Hat CloudForms</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openstack-platform/">Red Hat OpenStack Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-cloud-suite/">Red Hat Cloud Suite</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-openshift-container-platform/">Red Hat OpenShift Container Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-online-red-hat/">Red Hat OpenShift Online</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openshift-dedicated-red-hat/">Red Hat OpenShift Dedicated</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/">Red Hat Advanced Cluster Management for Kubernetes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-quay/">Red Hat Quay</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-workspaces/">Red Hat CodeReady Workspaces</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="storage-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-storage/">Red Hat Gluster Storage</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/">Red Hat Hyperconverged Infrastructure</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-ceph-storage/">Red Hat Ceph Storage</a>
                                        </li>
                                        <li>
                                          <a href="https://access.redhat.com/products/red-hat-openshift-container-storage">Red Hat Openshift Container Storage</a>
                                        </li>
                                    </ul>
                                </div>
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-dev-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-runtimes/">Red Hat Runtimes</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/">Red Hat JBoss Enterprise Application Platform</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-data-grid/">Red Hat Data Grid</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-web-server/">Red Hat JBoss Web Server</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-single-sign-on/">Red Hat Single Sign On</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/spring-boot/">Red Hat support for Spring Boot</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/nodejs/">Red Hat build of Node.js</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/thorntail/">Red Hat build of Thorntail</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/eclipse-vertx/">Red Hat build of Eclipse Vert.x</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/openjdk/">Red Hat build of OpenJDK</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/open-liberty/">Open Liberty</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/quarkus/">Red Hat build of Quarkus</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-codeready-studio/">Red Hat CodeReady Studio</a>
                                        </li>
                                    </ul>
                                </div>
                                <!-- <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                        </li>
                                        <li>
                                            <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                        </li>
                                    </ul>
                                </div> -->
                                <div class="subnav subnav-child" data-eh="subnav-child" data-menu-local="jboss-int-menu">
                                    <ul class="border-bottom" id="portal-menu-border-bottom">
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-integration/">Red Hat Integration</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-fuse/">Red Hat Fuse</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-amq/">Red Hat AMQ</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-3scale/">Red Hat 3scale API Management</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-jboss-data-virtualization/">Red Hat JBoss Data Virtualization</a>
                                      </li>
                                    </ul>
                                    <ul>
                                      <li>
                                        <a href="https://access.redhat.com/products/red-hat-process-automation/">Red Hat Process Automation</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-process-automation-manager/">Red Hat Process Automation Manager</a>
                                      </li>
                                      <li>
                                          <a href="https://access.redhat.com/products/red-hat-decision-manager/">Red Hat Decision Manager</a>
                                      </li>
                                    </ul>
                                </div>
                            </div>
                            <a href="https://access.redhat.com/products" class="btn btn-primary">View All Products</a>
                        </div>

                        <div class="col-md-6 col-sm-4 pull-right" data-portal-tour-1="8">
                            <div class="row">
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/support" class="cta-link cta-link-darkbg">Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/production/">Production Support</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/developer/">Development Support</a></li>
                                        <li><a href="https://access.redhat.com/product-life-cycles/">Product Life Cycles</a></li>
                                    </ul>
                                    <h4 class="nav-title">Services</h4>
                                    <ul>
                                        <li><a href="https://www.redhat.com/en/services/consulting">Consulting</a></li>
                                        <li><a href="https://access.redhat.com/support/offerings/tam/">Technical Account Management</a></li>
                                        <li><a href="https://www.redhat.com/en/services/training-and-certification">Training &amp; Certifications</a></li>
                                    </ul>
                                </div>
                                <div class="col-md-6">
                                    <ul>
                                        <li><a href="https://access.redhat.com/documentation" class="cta-link cta-link-darkbg">Documentation</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_enterprise_linux">Red Hat Enterprise Linux</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_jboss_enterprise_application_platform">Red Hat JBoss Enterprise Application Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/red_hat_openstack_platform">Red Hat OpenStack Platform</a></li>
                                        <li><a href="https://access.redhat.com/documentation/en/openshift_container_platform">Red Hat OpenShift Container Platform</a></li>
                                    </ul>
                                    <ul>
                                        
                                        <li><a href="https://catalog.redhat.com/" class="cta-link cta-link-darkbg">Ecosystem Catalog</a></li>
                                        <li><a href="https://access.redhat.com/public-cloud">Red Hat in the Public Cloud</a></li>
                                        <li><a href="https://access.redhat.com/ecosystem/partner-resources">Partner Resources</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Tools Menu -->
                <div aria-labelledby="tools-menu" class="tools-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row" data-portal-tour-1="9">
                        <div class="col-sm-12">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h3 class="nav-title">Tools</h3>
                                    <ul>
                                        <li><a href="https://access.redhat.com/solution-engine">Solution Engine</a></li>
                                        <li><a href="https://access.redhat.com/downloads/content/package-browser">Packages</a></li>
                                        <li><a href="https://access.redhat.com/errata/">Errata</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <ul class="list-flat">
                                        <li><a href="https://access.redhat.com/labs/" class="cta-link cta-link-darkbg">Customer Portal Labs</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=config">Configuration</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=deploy">Deployment</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=security">Security</a></li>
                                        <li><a href="https://access.redhat.com/labs/#?type=troubleshoot">Troubleshooting</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <div class="card card-dark-grey">
                                        <h4 class="card-heading">Red Hat Insights</h4>
                                        <p class="text-white">Increase visibility into IT operations to detect and resolve technical issues before they impact your business.</p>
                                        <ul class="list-flat rh-l-grid rh-m-gutters rh-m-all-6-col-on-md">
                                          <li><a href="https://www.redhat.com/en/technologies/management/insights" class="cta-link cta-link-md cta-link-darkbg">Learn more</a></li>
                                          <li><a href="https://cloud.redhat.com/insights" class="cta-link cta-link-md cta-link-darkbg">Go to Insights</a></li>
                                        </ul>
                                    </div>
                                </div>

                            </div>
                        </div>

                    </div>
                </div>

                <!-- Security Menu -->
                <div aria-labelledby="security-menu" class="security-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="10">

                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Red Hat Product Security Center</h2>
                                    <p class="text-white">Engage with our Red Hat Product Security team, access security updates, and ensure your environments are not exposed to any known security vulnerabilities.</p>
                                    <p><a href="https://access.redhat.com/security/" class="btn btn-primary">Product Security Center</a></p>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Security Updates</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-advisories">Security Advisories</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/cve">Red Hat CVE Database</a></li>
                                        <li><a href="https://access.redhat.com/security/security-updates/#/security-labs">Security Labs</a></li>
                                    </ul>
                                    <p class="text-white">Keep your systems secure with Red Hat&#039;s specialized responses for high-priority security vulnerabilities.</p>
                                    <ul>
                                        <li class="more-link"><a href="https://access.redhat.com/security/vulnerability">View Responses</a></li>
                                    </ul>
                                </div>

                                <div class="col-sm-4">
                                    <h2 class="nav-title">Resources</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/security/overview/">Overview</a></li>
                                        <li><a href="https://www.redhat.com/en/blog/channel/security">Security Blog</a></li>
                                        <li><a href="https://www.redhat.com/security/data/metrics/">Security Measurement</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/classification/">Severity Ratings</a></li>
                                        <li><a href="https://access.redhat.com/security/updates/backporting/">Backporting Policies</a></li>
                                        <li><a href="https://access.redhat.com/security/team/key/">Product Signing (GPG) Keys</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Community Menu -->
                <div aria-labelledby="community-menu" class="community-menu mega-menu" data-eh="mega-menu" role="navigation">
                    <div class="row">
                        <div class="col-sm-12 basic" data-portal-tour-1="11">
                            <div class="row">
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Portal Community</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/discussions?keyword=&name=&product=All&category=All&tags=All">Discussions</a></li>
                                        <li><a href="https://access.redhat.com/blogs/">Blogs</a></li>
                                        <li><a href="https://access.redhat.com/groups/">Private Groups</a></li>
                                        <p class="push-top"><a href="https://access.redhat.com/community/" class="btn btn-primary">Community Activity</a></p>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Customer Events</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/convergence/">Red Hat Convergence</a></li>
                                        <li><a href="http://www.redhat.com/summit/">Red Hat Summit</a></li>
                                    </ul>
                                </div>
                                <div class="col-sm-4">
                                    <h2 class="nav-title">Stories</h2>
                                    <ul>
                                        <li><a href="https://access.redhat.com/subscription-value/">Red Hat Subscription Value</a></li>
                                        <li><a href="https://access.redhat.com/you-asked-we-acted/">You Asked. We Acted.</a></li>
                                        <li><a href="http://www.redhat.com/en/open-source">Open Source Communities</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <!--<a href="https://access.redhat.com/community/" class="btn btn-primary">Explore Community</a>-->
                    </div>
                </div>
            </div>
        </div>
    </nav>
</div>

            <!--[if IE 8]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    You are using an unsupported web browser. Update to a supported browser for the best experience. <a href="/announcements/2120951">Read the announcement</a>.
                </div>
            </div>
            <![endif]-->
            <!--[if IE 9]>
            <div class="portal-messages">
                <div class="alert alert-warning alert-portal alert-w-icon">
                    <span class="icon-warning alert-icon" aria-hidden="true"></span>
                    As of March 1, 2016, the Red Hat Customer Portal will no longer support Internet Explorer 9. See our new <a href="/help/browsers">browser support policy</a> for more information.
                </div>
            </div>
            <![endif]-->
            <div id="site-section"></div>
        </header>
        <!--googleon: all-->

        <main id="cp-main" class="portal-content-area">
            <div id="cp-content" class="main-content">
  <script type="text/javascript">
    chrometwo_require(['jquery-ui']);
  </script>


      <article class="rh_docs">
  <div class="container">
        <!-- Display: Book Page Content -->
    
  

<div class="row">
  <div itemscope="" itemtype="https://schema.org/TechArticle" itemref="techArticle-md1 techArticle-md2 techArticle-md3"></div>
  <div itemscope="" itemtype="https://schema.org/SoftwareApplication" itemref="softwareApplication-md1 softwareApplication-md2 softwareApplication-md3 softwareApplication-md4"></div>
  


<a class="toc-toggle toc-show" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
  <span class="sr-only">Show Table of Contents</span>
  <span class="web-icon-mobile-menu" aria-hidden="true"></span>
</a>
<nav id="toc-main" class="toc-main collapse in">
  <div class="toc-menu affix-top">
    <a class="toc-toggle toc-hide" data-toggle="collapse" data-target="#toc-main" aria-expanded="false" aria-controls="toc-main">
      <span class="sr-only">Hide Table of Contents</span>
      <span class="icon-remove" aria-hidden="true"></span>
    </a>
    <div class="doc-options">
      <div class="doc-language btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          English <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="index.html">English</a></li>
                      <li><a href="https://access.redhat.com/documentation/ja-jp/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/">日本語</a></li>
                  </ul>
      </div>
      <div class="doc-format btn-group">
        <button type="button" class="btn btn-app dropdown-toggle" data-toggle="dropdown" aria-expanded="false">
          Single-page HTML <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" role="menu">
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/deploying_an_overcloud_with_containerized_red_hat_ceph/">Multi-page HTML</a></li>
                                <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/pdf/deploying_an_overcloud_with_containerized_red_hat_ceph/Red_Hat_OpenStack_Platform-16.1-Deploying_an_overcloud_with_containerized_Red_Hat_Ceph-en-US.pdf">PDF</a></li>
                      <li><a href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/epub/deploying_an_overcloud_with_containerized_red_hat_ceph/Red_Hat_OpenStack_Platform-16.1-Deploying_an_overcloud_with_containerized_Red_Hat_Ceph-en-US.epub">ePub</a></li>
                  </ul>
      </div>
    </div>
    <ol class="menu"><li class=" leaf"><a href="index.html">Deploying an overcloud with containerized Red Hat Ceph</a></li><li class=" leaf"><a href="index.html#intro">1. Introduction</a><ol class="menu"><li class=" leaf"><a href="index.html#intro-defining">1.1. Introduction to Ceph Storage</a></li><li class=" leaf"><a href="index.html#requirements">1.2. Requirements</a><ol class="menu"><li class=" leaf"><a href="index.html#ceph-storage-node-requirements">1.2.1. Ceph Storage node requirements</a></li></ol></li><li class=" leaf"><a href="index.html#additional_resources">1.3. Additional resources</a></li></ol></li><li class=" leaf"><a href="index.html#creation">2. Preparing Ceph Storage nodes for overcloud deployment</a><ol class="menu"><li class=" leaf"><a href="index.html#Formatting_Ceph_Storage_Nodes_Disks_to_GPT">2.1. Cleaning Ceph Storage node disks</a></li><li class=" leaf"><a href="index.html#register-nodes">2.2. Registering nodes</a></li><li class=" leaf"><a href="index.html#assembly_pre-deployment-validations-for-ceph">2.3. Pre-deployment validations for Ceph Storage</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_verifying-ceph-ansible-version_assembly-pre-deployment-validations-for-ceph">2.3.1. Verifying the ceph-ansible package version</a></li><li class=" leaf"><a href="index.html#proc_verifying-packages-for-pre-provisioned-nodes_assembly-pre-deployment-validations-for-ceph">2.3.2. Verifying packages for pre-provisioned nodes</a></li></ol></li><li class=" leaf"><a href="index.html#manual-node-tag">2.4. Manually tagging nodes into profiles</a></li><li class=" leaf"><a href="index.html#defining-the-root-disk">2.5. Defining the root disk for multi-disk clusters</a></li><li class=" leaf"><a href="index.html#using-the-overcloud-minimal-image-to-avoid-using-a-Red-Hat-subscription-entitlement">2.6. Using the overcloud-minimal image to avoid using a Red Hat subscription entitlement</a></li></ol></li><li class=" leaf"><a href="index.html#dedicated-nodes">3. Deploying Ceph services on dedicated nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#custom-node-file">3.1. Creating a custom roles file</a></li><li class=" leaf"><a href="index.html#dedicated-nodes-mon">3.2. Creating a custom role and flavor for the Ceph MON service</a></li><li class=" leaf"><a href="index.html#dedicated-nodes-mds">3.3. Creating a custom role and flavor for the Ceph MDS service</a></li></ol></li><li class=" leaf"><a href="index.html#enable-ceph-overcloud">4. Customizing the Storage service</a><ol class="menu"><li class=" leaf"><a href="index.html#ceph-mds">4.1. Enabling the Ceph Metadata Server</a></li><li class=" leaf"><a href="index.html#ceph-rgw">4.2. Enabling the Ceph Object Gateway</a></li><li class=" leaf"><a href="index.html#configuring_ceph_object_store_to_use_external_ceph_object_gateway">4.3. Configuring Ceph Object Store to use external Ceph Object Gateway</a></li><li class=" leaf"><a href="index.html#cinder-backup-ceph">4.4. Configuring the Backup Service to use Ceph</a></li><li class=" leaf"><a href="index.html#multibonded-nics">4.5. Configuring multiple bonded interfaces for Ceph nodes</a><ol class="menu"><li class=" leaf"><a href="index.html#multibonded-nics-ovs-opts">4.5.1. Configuring bonding module directives</a></li></ol></li></ol></li><li class=" leaf"><a href="index.html#Configuring_Ceph_Storage_Cluster_Settings">5. Customizing the Ceph Storage cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#setting_ceph_ansible_group_variables">5.1. Setting ceph-ansible group variables</a></li><li class=" leaf"><a href="index.html#ceph-containers-for-osp-with-ceph-storage">5.2. Ceph containers for Red Hat OpenStack Platform with Ceph Storage</a></li><li class=" leaf"><a href="index.html#Mapping_the_Ceph_Storage_Node_Disk_Layout">5.3. Mapping the Ceph Storage node disk layout</a><ol class="menu"><li class=" leaf"><a href="index.html#using_bluestore">5.3.1. Using BlueStore</a></li><li class=" leaf"><a href="index.html#referring_to_devices_with_persistent_names">5.3.2. Referring to devices with persistent names</a></li></ol></li><li class=" leaf"><a href="index.html#custom-ceph-pools">5.4. Assigning custom attributes to different Ceph pools</a></li><li class=" leaf"><a href="index.html#map_disk_layout_non-homogen_ceph">5.5. Mapping the disk layout to non-homogeneous Ceph Storage nodes</a></li><li class=" leaf"><a href="index.html#increasing-restart-delay-for-large-ceph-clusters">5.6. Increasing the restart delay for large Ceph clusters</a></li></ol></li><li class=" leaf"><a href="index.html#proc_ceph-defining-performance-tiers-for-varying-workloads-with-ceph-ansible">6. Defining performance tiers for varying workloads in a Ceph Storage cluster with director</a><ol class="menu"><li class=" leaf"><a href="index.html#configuring_the_performance_tiers">6.1. Configuring the performance tiers</a></li><li class=" leaf"><a href="index.html#mapping_a_block_storage_cinder_type_to_your_new_ceph_pool">6.2. Mapping a Block Storage (cinder) type to your new Ceph pool</a></li><li class=" leaf"><a href="index.html#verifying_that_the_crush_rules_are_created_and_that_your_pools_are_set_to_the_correct_crush_rule">6.3. Verifying that the CRUSH rules are created and that your pools are set to the correct CRUSH rule</a></li></ol></li><li class=" leaf"><a href="index.html#creating_the_overcloud">7. Creating the overcloud</a><ol class="menu"><li class=" leaf"><a href="index.html#node-assignments">7.1. Assigning nodes and flavors to roles</a></li><li class=" leaf"><a href="index.html#sect-Creating_the_Overcloud">7.2. Initiating overcloud deployment</a></li></ol></li><li class=" leaf"><a href="index.html#adding-ceph-dashboard">8. Adding the Red Hat Ceph Storage Dashboard to an overcloud deployment</a><ol class="menu"><li class=" leaf"><a href="index.html#proc_including-ceph-dashboard-containers">8.1. Including the necessary containers for the Ceph Dashboard</a></li><li class=" leaf"><a href="index.html#proc_deploying-ceph-dashboard">8.2. Deploying Ceph Dashboard</a></li><li class=" leaf"><a href="index.html#proc_changing-the-default-permissions">8.3. Changing the default permissions</a></li><li class=" leaf"><a href="index.html#proc_accessing-ceph-dashboard">8.4. Accessing Ceph Dashboard</a></li></ol></li><li class=" leaf"><a href="index.html#post-deploy">9. Post-deployment</a><ol class="menu"><li class=" leaf"><a href="index.html#accessing_the_overcloud">9.1. Accessing the overcloud</a></li><li class=" leaf"><a href="index.html#monitoring_ceph_storage_nodes">9.2. Monitoring Ceph Storage nodes</a></li></ol></li><li class=" leaf"><a href="index.html#rebooting_the_environment">10. Rebooting the environment</a><ol class="menu"><li class=" leaf"><a href="index.html#rebooting_a_ceph_storage_cluster">10.1. Rebooting a Ceph Storage (OSD) cluster</a></li></ol></li><li class=" leaf"><a href="index.html#scaling_the_ceph_storage_cluster">11. Scaling the Ceph Storage cluster</a><ol class="menu"><li class=" leaf"><a href="index.html#scaling_up">11.1. Scaling up the Ceph Storage cluster</a></li><li class=" leaf"><a href="index.html#Replacing_Ceph_Storage_Nodes">11.2. Scaling down and replacing Ceph Storage nodes</a></li><li class=" leaf"><a href="index.html#adding-osd-to-ceph-storage-node">11.3. Adding an OSD to a Ceph Storage node</a></li><li class=" leaf"><a href="index.html#removing-osd-from-ceph-storage-node">11.4. Removing an OSD from a Ceph Storage node</a></li></ol></li><li class=" leaf"><a href="index.html#replacing_a_failed_disk">12. Replacing a failed disk</a><ol class="menu"><li class=" leaf"><a href="index.html#determining-if-device-name-changed">12.1. Determining if there is a device name change</a></li><li class=" leaf"><a href="index.html#ensuring-osd-down-and-destroyed">12.2. Ensuring that the OSD is down and destroyed</a></li><li class=" leaf"><a href="index.html#removing-old-disk-installing-replacement">12.3. Removing the old disk from the system and installing the replacement disk</a></li><li class=" leaf"><a href="index.html#verifying-the-disk-replacement-is-successful">12.4. Verifying that the disk replacement is successful</a></li></ol></li><li class=" leaf"><a href="index.html#envfile-createceph">A. Sample environment file: creating a Ceph Storage cluster</a></li><li class=" leaf"><a href="index.html#template-multibonded-nics">B. Sample custom interface template: multiple bonded interfaces</a></li><li class=" leaf"><a href="index.html#idm140342040708176">Legal Notice</a></li></ol>  </div>
</nav>


  <div class="doc-wrapper">
    <div class="panel-pane pane-page-title"  >
  
      
  
  <h1 class="title" itemprop="name">Deploying an overcloud with containerized Red Hat Ceph</h1>
  
  </div>
<div class="panel-pane pane-page-body"  >
  
      
  
  <div class="body"><div xml:lang="en-US" class="book" id="idm140342045566272"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat OpenStack Platform</span> <span class="productnumber">16.1</span></div><div><h2 class="subtitle">Configuring the director to deploy and use a containerized Red Hat Ceph cluster</h2></div><div><div xml:lang="en-US" class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">OpenStack</span> <span class="othername">Documentation</span> <span class="surname">Team</span></h3><code class="email"><a class="email" href="mailto:rhos-docs@redhat.com">rhos-docs@redhat.com</a></code></div></div></div><div><a href="index.html#idm140342040708176">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This guide provides information about using the Red Hat OpenStack Platform director to create an overcloud with a containerized Red Hat Ceph Storage cluster. This includes instructions for customizing your Ceph cluster through the director.
			</div></div></div></div><hr/></div><section class="chapter" id="intro"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Introduction</h1></div></div></div><p>
			Red Hat OpenStack Platform director creates a cloud environment called the <span class="emphasis"><em>overcloud</em></span>. The director provides the ability to configure extra features for an overcloud, including integration with Red Hat Ceph Storage (both Ceph Storage clusters created with the director or existing Ceph Storage clusters).
		</p><p id="intro-scenario">
			This guide contains instructions for deploying a containerized Red Hat Ceph Storage cluster with your overcloud. Director uses Ansible playbooks provided through the <code class="literal">ceph-ansible</code> package to deploy a containerized Ceph cluster. The director also manages the configuration and scaling operations of the cluster.
		</p><p>
			For more information about containerized services in Red Hat OpenStack Platform (RHOSP), see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#creating-a-basic-overcloud-with-cli-tools">Configuring a basic overcloud with the CLI tools</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
		</p><section class="section" id="intro-defining"><div class="titlepage"><div><div><h2 class="title">1.1. Introduction to Ceph Storage</h2></div></div></div><p>
				Red Hat Ceph Storage is a distributed data object store designed to provide excellent performance, reliability, and scalability. Distributed object stores are the future of storage, because they accommodate unstructured data, and because clients can use modern object interfaces and legacy interfaces simultaneously. At the core of every Ceph deployment is the Ceph Storage cluster, which consists of several types of daemons, but primarily, these two:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Ceph OSD (Object Storage Daemon)</span></dt><dd>
							Ceph OSDs store data on behalf of Ceph clients. Additionally, Ceph OSDs utilize the CPU and memory of Ceph nodes to perform data replication, rebalancing, recovery, monitoring and reporting functions.
						</dd><dt><span class="term">Ceph Monitor</span></dt><dd>
							A Ceph monitor maintains a master copy of the Ceph storage cluster map with the current state of the storage cluster.
						</dd></dl></div><p>
				For more information about Red Hat Ceph Storage, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/architecture_guide/index">Red Hat Ceph Storage Architecture Guide</a>.
			</p></section><section class="section" id="requirements"><div class="titlepage"><div><div><h2 class="title">1.2. Requirements</h2></div></div></div><p>
				This guide contains information supplementary to the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/index">Director Installation and Usage</a> guide.
			</p><p>
				Before you deploy a containerized Ceph Storage cluster with your overcloud, your environment must contain the following configuration:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						An undercloud host with the Red Hat OpenStack Platform director installed. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/installing-the-undercloud">Installing director</a>.
					</li><li class="listitem">
						Any additional hardware recommended for Red Hat Ceph Storage. For more information about recommended hardware, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/hardware_guide/index">Red Hat Ceph Storage Hardware Guide</a>.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The Ceph Monitor service installs on the overcloud Controller nodes, so you must provide adequate resources to avoid performance issues. Ensure that the Controller nodes in your environment use at least 16 GB of RAM for memory and solid-state drive (SSD) storage for the Ceph monitor data. For a medium to large Ceph installation, provide at least 500 GB of Ceph monitor data. This space is necessary to avoid levelDB growth if the cluster becomes unstable.
				</p></div></div><p>
				If you use the Red Hat OpenStack Platform director to create Ceph Storage nodes, note the following requirements.
			</p><section class="section" id="ceph-storage-node-requirements"><div class="titlepage"><div><div><h3 class="title">1.2.1. Ceph Storage node requirements</h3></div></div></div><p>
					Ceph Storage nodes are responsible for providing object storage in a Red Hat OpenStack Platform environment.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Placement Groups (PGs)</span></dt><dd>
								Ceph uses placement groups to facilitate dynamic and efficient object tracking at scale. In the case of OSD failure or cluster rebalancing, Ceph can move or replicate a placement group and its contents, which means a Ceph cluster can re-balance and recover efficiently. The default placement group count that director creates is not always optimal so it is important to calculate the correct placement group count according to your requirements. You can use the placement group calculator to calculate the correct count: <a class="link" href="https://access.redhat.com/labs/cephpgc/">Placement Groups (PGs) per Pool Calculator</a>
							</dd><dt><span class="term">Processor</span></dt><dd>
								64-bit x86 processor with support for the Intel 64 or AMD64 CPU extensions.
							</dd><dt><span class="term">Memory</span></dt><dd>
								Red Hat typically recommends a baseline of 16 GB of RAM per OSD host, with an additional 2 GB of RAM per OSD daemon.
							</dd><dt><span class="term">Disk layout</span></dt><dd><p class="simpara">
								Sizing is dependent on your storage requirements. Red Hat recommends that your Ceph Storage node configuration includes three or more disks in a layout similar to the following example:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										<code class="literal">/dev/sda</code> - The root disk. The director copies the main overcloud image to the disk. Ensure that the disk has a minimum of 40 GB of available disk space.
									</li><li class="listitem">
										<code class="literal">/dev/sdb</code> - The journal disk. This disk divides into partitions for Ceph OSD journals. For example, <code class="literal">/dev/sdb1</code>, <code class="literal">/dev/sdb2</code>, and <code class="literal">/dev/sdb3</code>. The journal disk is usually a solid state drive (SSD) to aid with system performance.
									</li><li class="listitem"><p class="simpara">
										<code class="literal">/dev/sdc</code> and onward - The OSD disks. Use as many disks as necessary for your storage requirements.
									</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
											Red Hat OpenStack Platform director uses <code class="literal">ceph-ansible</code>, which does not support installing the OSD on the root disk of Ceph Storage nodes. This means that you need at least two disks for a supported Ceph Storage node.
										</p></div></div></li></ul></div></dd><dt><span class="term">Network Interface Cards</span></dt><dd>
								A minimum of one 1 Gbps Network Interface Cards, although Red Hat recommends that you use at least two NICs in a production environment. Use additional network interface cards for bonded interfaces or to delegate tagged VLAN traffic. Red Hat recommends that you use a 10 Gbps interface for storage nodes, especially if you want to create an OpenStack Platform environment that serves a high volume of traffic.
							</dd><dt><span class="term">Power management</span></dt><dd>
								Each Controller node requires a supported power management interface, such as Intelligent Platform Management Interface (IPMI) functionality on the motherboard of the server.
							</dd></dl></div></section></section><section class="section" id="additional_resources"><div class="titlepage"><div><div><h2 class="title">1.3. Additional resources</h2></div></div></div><p>
				The <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml</code> environment file instructs the director to use playbooks derived from the <a class="link" href="https://github.com/ceph/ceph-ansible/tree/master/">ceph-ansible</a> project. These playbooks are installed in <code class="literal">/usr/share/ceph-ansible/</code> of the undercloud. In particular, the following file contains all the default settings that the playbooks apply:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">/usr/share/ceph-ansible/group_vars/all.yml.sample</code>
					</li></ul></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					While <code class="literal">ceph-ansible</code> uses playbooks to deploy containerized Ceph Storage, do not edit these files to customize your deployment. Instead, use heat environment files to override the defaults set by these playbooks. If you edit the <code class="literal">ceph-ansible</code> playbooks directly, your deployment will fail.
				</p></div></div><p>
				For more information about the playbook collection, see the documentation for this project (<a class="link" href="http://docs.ceph.com/ceph-ansible/master/">http://docs.ceph.com/ceph-ansible/master/</a>) to learn more about the playbook collection.
			</p><p>
				Alternatively, for information about the default settings applied by director for containerized Ceph Storage, see the heat templates in <code class="literal">/usr/share/openstack-tripleo-heat-templates/deployment/ceph-ansible</code>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Reading these templates requires a deeper understanding of how environment files and heat templates work in director. See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-understanding_heat_templates">Understanding Heat Templates</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-understanding_heat_templates#sect-Environment_Files">Environment Files</a> for reference.
				</p></div></div><p>
				Lastly, for more information about containerized services in OpenStack, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#creating-a-basic-overcloud-with-cli-tools">Configuring a basic overcloud with the CLI tools</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
			</p></section></section><section class="chapter" id="creation"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Preparing Ceph Storage nodes for overcloud deployment</h1></div></div></div><p>
			All nodes in this scenario are bare metal systems using IPMI for power management. These nodes do not require an operating system because the director copies a Red Hat Enterprise Linux 8 image to each node. Additionally, the Ceph Storage services on these nodes are containerized. The director communicates to each node through the Provisioning network during the introspection and provisioning processes. All nodes connect to this network through the native VLAN.
		</p><section class="section" id="Formatting_Ceph_Storage_Nodes_Disks_to_GPT"><div class="titlepage"><div><div><h2 class="title">2.1. Cleaning Ceph Storage node disks</h2></div></div></div><p>
				The Ceph Storage OSDs and journal partitions require GPT disk labels. This means the additional disks on Ceph Storage require conversion to GPT before installing the Ceph OSD services. You must delete all metadata from the disks to allow the director to set GPT labels on them.
			</p><p>
				You can configure the director to delete all disk metadata by default by adding the following setting to your <code class="literal">/home/stack/undercloud.conf</code> file:
			</p><pre class="screen">clean_nodes=true</pre><p>
				With this option, the Bare Metal Provisioning service runs an additional step to boot the nodes and clean the disks each time the node is set to <code class="literal">available</code>. This process adds an additional power cycle after the first introspection and before each deployment. The Bare Metal Provisioning service uses the <code class="literal">wipefs --force --all</code> command to perform the clean.
			</p><p>
				After setting this option, run the <code class="literal">openstack undercloud install</code> command to execute this configuration change.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					The <code class="literal">wipefs --force --all</code> command deletes all data and metadata on the disk, but does not perform a secure erase. A secure erase takes much longer.
				</p></div></div></section><section class="section" id="register-nodes"><div class="titlepage"><div><div><h2 class="title">2.2. Registering nodes</h2></div></div></div><p>
				Import a node inventory file (<code class="literal">instackenv.json</code>) in JSON format to the director so that the director can communicate with the nodes. This inventory file contains hardware and power management details that the director can use to register nodes:
			</p><pre class="screen">{
    "nodes":[
        {
            "mac":[
                "b1:b1:b1:b1:b1:b1"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.205"
        },
        {
            "mac":[
                "b2:b2:b2:b2:b2:b2"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.206"
        },
        {
            "mac":[
                "b3:b3:b3:b3:b3:b3"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.207"
        },
        {
            "mac":[
                "c1:c1:c1:c1:c1:c1"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.208"
        },
        {
            "mac":[
                "c2:c2:c2:c2:c2:c2"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.209"
        },
        {
            "mac":[
                "c3:c3:c3:c3:c3:c3"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.210"
        },
        {
            "mac":[
                "d1:d1:d1:d1:d1:d1"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.211"
        },
        {
            "mac":[
                "d2:d2:d2:d2:d2:d2"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.212"
        },
        {
            "mac":[
                "d3:d3:d3:d3:d3:d3"
            ],
            "cpu":"4",
            "memory":"6144",
            "disk":"40",
            "arch":"x86_64",
            "pm_type":"ipmi",
            "pm_user":"admin",
            "pm_password":"p@55w0rd!",
            "pm_addr":"192.0.2.213"
        }
    ]
}</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						After you create the inventory file, save the file to the home directory of the stack user (<code class="literal">/home/stack/instackenv.json</code>).
					</li><li class="listitem"><p class="simpara">
						Initialize the stack user, then import the <code class="literal">instackenv.json</code> inventory file into director:
					</p><pre class="screen">$ source ~/stackrc
$ openstack overcloud node import ~/instackenv.json</pre><p class="simpara">
						The <code class="literal">openstack overcloud node import</code> command imports the inventory file and registers each node with the director.
					</p></li><li class="listitem"><p class="simpara">
						Assign the kernel and ramdisk images to each node:
					</p><pre class="screen">$ openstack overcloud node configure &lt;node&gt;</pre><div class="variablelist"><dl class="variablelist"><dt><span class="term">Result</span></dt><dd>
									The nodes are registered and configured in director.
								</dd></dl></div></li></ol></div></section><section class="section" id="assembly_pre-deployment-validations-for-ceph"><div class="titlepage"><div><div><h2 class="title">2.3. Pre-deployment validations for Ceph Storage</h2></div></div></div><p>
				To help avoid overcloud deployment failures, verify that the required packages exist on your servers.
			</p><section class="section" id="proc_verifying-ceph-ansible-version_assembly-pre-deployment-validations-for-ceph"><div class="titlepage"><div><div><h3 class="title">2.3.1. Verifying the ceph-ansible package version</h3></div></div></div><p>
					The undercloud contains Ansible-based validations that you can run to identify potential problems before you deploy the overcloud. These validations can help you avoid overcloud deployment failures by identifying common problems before they happen.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Verify that the correction version of the <code class="literal">ceph-ansible</code> package is installed:
					</p></div><pre class="screen">$ ansible-playbook -i /usr/bin/tripleo-ansible-inventory /usr/share/openstack-tripleo-validations/validations/ceph-ansible-installed.yaml</pre></section><section class="section" id="proc_verifying-packages-for-pre-provisioned-nodes_assembly-pre-deployment-validations-for-ceph"><div class="titlepage"><div><div><h3 class="title">2.3.2. Verifying packages for pre-provisioned nodes</h3></div></div></div><p>
					Ceph can only service overcloud nodes that have a certain set of packages. When you use pre-provisioned nodes, you can verify the presence of these packages.
				</p><p>
					For more information about pre-provisioned nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#chap-Configuring_Basic_Overcloud_Requirements_on_Pre_Provisioned_Nodes">Configuring a Basic Overcloud using Pre-Provisioned Nodes</a>.
				</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						Verify that the servers contained the required packages:
					</p></div><pre class="screen">ansible-playbook -i /usr/bin/tripleo-ansible-inventory /usr/share/openstack-tripleo-validations/validations/ceph-dependencies-installed.yaml</pre></section></section><section class="section" id="manual-node-tag"><div class="titlepage"><div><div><h2 class="title">2.4. Manually tagging nodes into profiles</h2></div></div></div><p>
				After you register each node, you must inspect the hardware and tag the node into a specific profile. Use profile tags to match your nodes to flavors, and then assign flavors to deployment roles.
			</p><p>
				To inspect and tag new nodes, complete the following steps:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Trigger hardware introspection to retrieve the hardware attributes of each node:
					</p><pre class="screen">$ openstack overcloud node introspect --all-manageable --provide</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The <code class="literal">--all-manageable</code> option introspects only the nodes that are in a managed state. In this example, all nodes are in a managed state.
							</li><li class="listitem"><p class="simpara">
								The <code class="literal">--provide</code> option resets all nodes to an <code class="literal">active</code> state after introspection.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Ensure that this process completes successfully. This process usually takes 15 minutes for bare metal nodes.
								</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Retrieve a list of your nodes to identify their UUIDs:
					</p><pre class="screen">$ openstack baremetal node list</pre></li><li class="listitem"><p class="simpara">
						Add a profile option to the <code class="literal">properties/capabilities</code> parameter for each node to manually tag a node to a specific profile. The addition of the <code class="literal">profile</code> option tags the nodes into each respective profile.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							As an alternative to manual tagging, use the Automated Health Check (AHC) Tools to automatically tag larger numbers of nodes based on benchmarking data.
						</p></div></div><p class="simpara">
						For example, a typical deployment contains three profiles: <code class="literal">control</code>, <code class="literal">compute</code>, and <code class="literal">ceph-storage</code>. Run the following commands to tag three nodes for each profile:
					</p><pre class="literallayout">$ ironic node-update 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0 add properties/capabilities='<span class="strong strong"><strong>profile:control</strong></span>,boot_option:local'
$ ironic node-update 6faba1a9-e2d8-4b7c-95a2-c7fbdc12129a add properties/capabilities='<span class="strong strong"><strong>profile:control</strong></span>,boot_option:local'
$ ironic node-update 5e3b2f50-fcd9-4404-b0a2-59d79924b38e add properties/capabilities='<span class="strong strong"><strong>profile:control</strong></span>,boot_option:local'
$ ironic node-update 484587b2-b3b3-40d5-925b-a26a2fa3036f add properties/capabilities='<span class="strong strong"><strong>profile:compute</strong></span>,boot_option:local'
$ ironic node-update d010460b-38f2-4800-9cc4-d69f0d067efe add properties/capabilities='<span class="strong strong"><strong>profile:compute</strong></span>,boot_option:local'
$ ironic node-update d930e613-3e14-44b9-8240-4f3559801ea6 add properties/capabilities='<span class="strong strong"><strong>profile:compute</strong></span>,boot_option:local'
$ ironic node-update da0cc61b-4882-45e0-9f43-fab65cf4e52b add properties/capabilities='<span class="strong strong"><strong>profile:ceph-storage</strong></span>,boot_option:local'
$ ironic node-update b9f70722-e124-4650-a9b1-aade8121b5ed add properties/capabilities='<span class="strong strong"><strong>profile:ceph-storage</strong></span>,boot_option:local'
$ ironic node-update 68bf8f29-7731-4148-ba16-efb31ab8d34f add properties/capabilities='<span class="strong strong"><strong>profile:ceph-storage</strong></span>,boot_option:local'</pre><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
						You can also configure a new custom profile that you can use to tag a node for the Ceph MON and Ceph MDS services. See <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a> for details.
					</p></div></div></li></ol></div></section><section class="section" id="defining-the-root-disk"><div class="titlepage"><div><div><h2 class="title">2.5. Defining the root disk for multi-disk clusters</h2></div></div></div><p>
				Director must identify the root disk during provisioning in the case of nodes with multiple disks. For example, most Ceph Storage nodes use multiple disks. By default, director writes the overcloud image to the root disk during the provisioning process
			</p><p>
				There are several properties that you can define to help director identify the root disk:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">model</code> (String): Device identifier.
					</li><li class="listitem">
						<code class="literal">vendor</code> (String): Device vendor.
					</li><li class="listitem">
						<code class="literal">serial</code> (String): Disk serial number.
					</li><li class="listitem">
						<code class="literal">hctl</code> (String): Host:Channel:Target:Lun for SCSI.
					</li><li class="listitem">
						<code class="literal">size</code> (Integer): Size of the device in GB.
					</li><li class="listitem">
						<code class="literal">wwn</code> (String): Unique storage identifier.
					</li><li class="listitem">
						<code class="literal">wwn_with_extension</code> (String): Unique storage identifier with the vendor extension appended.
					</li><li class="listitem">
						<code class="literal">wwn_vendor_extension</code> (String): Unique vendor storage identifier.
					</li><li class="listitem">
						<code class="literal">rotational</code> (Boolean): True for a rotational device (HDD), otherwise false (SSD).
					</li><li class="listitem">
						<code class="literal">name</code> (String): The name of the device, for example: /dev/sdb1.
					</li></ul></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Use the <code class="literal">name</code> property only for devices with persistent names. Do not use <code class="literal">name</code> to set the root disk for any other devices because this value can change when the node boots.
				</p></div></div><p>
				Complete the following steps to specify the root device using its serial number.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Check the disk information from the hardware introspection of each node. Run the following command to display the disk information of a node:
					</p><pre class="screen">(undercloud) $ openstack baremetal introspection data save 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0 | jq ".inventory.disks"</pre><p class="simpara">
						For example, the data for one node might show three disks:
					</p><pre class="screen">[
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sda",
    "wwn_vendor_extension": "0x1ea4dcc412a9632b",
    "wwn_with_extension": "0x61866da04f3807001ea4dcc412a9632b",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f380700",
    "serial": "61866da04f3807001ea4dcc412a9632b"
  }
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sdb",
    "wwn_vendor_extension": "0x1ea4e13c12e36ad6",
    "wwn_with_extension": "0x61866da04f380d001ea4e13c12e36ad6",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f380d00",
    "serial": "61866da04f380d001ea4e13c12e36ad6"
  }
  {
    "size": 299439751168,
    "rotational": true,
    "vendor": "DELL",
    "name": "/dev/sdc",
    "wwn_vendor_extension": "0x1ea4e31e121cfb45",
    "wwn_with_extension": "0x61866da04f37fc001ea4e31e121cfb45",
    "model": "PERC H330 Mini",
    "wwn": "0x61866da04f37fc00",
    "serial": "61866da04f37fc001ea4e31e121cfb45"
  }
]</pre></li><li class="listitem"><p class="simpara">
						Run the <code class="literal">openstack baremetal node set --property root_device=</code> command to set the root disk for a node. Include the most appropriate hardware attribute value to define the root disk.
					</p><pre class="screen">(undercloud) $ openstack baremetal node set --property root_device=’{“serial”:”&lt;serial_number&gt;”}' &lt;node-uuid&gt;</pre><p class="simpara">
						For example, to set the root device to disk 2, which has the serial number <code class="literal">61866da04f380d001ea4e13c12e36ad6</code> run the following command:
					</p></li></ol></div><pre class="screen">(undercloud) $ openstack baremetal node set --property root_device='{"serial": "61866da04f380d001ea4e13c12e36ad6"}' 1a4e30da-b6dc-499d-ba87-0bd8a3819bc0</pre><p>
				+
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Ensure that you configure the BIOS of each node to include booting from the root disk that you choose. Configure the boot order to boot from the network first, then to boot from the root disk.
				</p></div></div><p>
				Director identifies the specific disk to use as the root disk. When you run the <code class="literal">openstack overcloud deploy</code> command, director provisions and writes the overcloud image to the root disk.
			</p></section><section class="section" id="using-the-overcloud-minimal-image-to-avoid-using-a-Red-Hat-subscription-entitlement"><div class="titlepage"><div><div><h2 class="title">2.6. Using the overcloud-minimal image to avoid using a Red Hat subscription entitlement</h2></div></div></div><p>
				By default, director writes the QCOW2 <code class="literal">overcloud-full</code> image to the root disk during the provisioning process. The <code class="literal">overcloud-full</code> image uses a valid Red Hat subscription. However, you can also use the <code class="literal">overcloud-minimal</code> image, for example, to provision a bare OS where you do not want to run any other OpenStack services and consume your subscription entitlements.
			</p><p>
				A common use case for this occurs when you want to provision nodes with only Ceph daemons. For this and similar use cases, you can use the <code class="literal">overcloud-minimal</code> image option to avoid reaching the limit of your paid Red Hat subscriptions. For information about how to obtain the <code class="literal">overcloud-minimal</code> image, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#sect-Obtaining_Images_for_Overcloud_Nodes">Obtaining images for overcloud nodes</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					A Red Hat OpenStack Platform subscription contains Open vSwitch (OVS), but core services, such as OVS, are not available when you use the <code class="literal">overcloud-minimal</code> image. OVS is not required to deploy Ceph Storage nodes. Instead of using 'ovs_bond' to define bonds, use 'linux_bond'. For more information about <code class="literal">linux_bond</code>, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#linux-bonding-options">Linux bonding options</a>.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To configure director to use the <code class="literal">overcloud-minimal</code> image, create an environment file that contains the following image definition:
					</p><pre class="screen">parameter_defaults:
  &lt;roleName&gt;Image: overcloud-minimal</pre></li><li class="listitem"><p class="simpara">
						Replace <code class="literal">&lt;roleName&gt;</code> with the name of the role and append <code class="literal">Image</code> to the name of the role. The following example shows an <code class="literal">overcloud-minimal</code> image for Ceph storage nodes:
					</p><pre class="screen">parameter_defaults:
  CephStorageImage: overcloud-minimal</pre></li><li class="listitem">
						Pass the environment file to the <code class="literal">openstack overcloud deploy</code> command.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					The <code class="literal">overcloud-minimal</code> image supports only standard Linux bridges and not OVS because OVS is an OpenStack service that requires a Red Hat OpenStack Platform subscription entitlement.
				</p></div></div></section></section><section class="chapter" id="dedicated-nodes"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Deploying Ceph services on dedicated nodes</h1></div></div></div><p>
			By default, the director deploys the Ceph MON and Ceph MDS services on the Controller nodes. This is suitable for small deployments. However, with larger deployments Red Hat recommends that you deploy the Ceph MON and Ceph MDS services on dedicated nodes to improve the performance of your Ceph cluster. Create a custom role for services that you want to isolate on dedicated nodes.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				For more information about custom roles, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#sect-Creating_a_New_Role">Creating a New Role</a> in the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/">Advanced Overcloud Customization</a> guide.
			</p></div></div><p>
			The director uses the following file as a default reference for all overcloud roles:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">/usr/share/openstack-tripleo-heat-templates/roles_data.yaml</code>
				</li></ul></div><section class="section" id="custom-node-file"><div class="titlepage"><div><div><h2 class="title">3.1. Creating a custom roles file</h2></div></div></div><p>
				To create a custom role file, complete the following steps:
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Make a copy of the <code class="literal">roles_data.yaml</code> file in <code class="literal">/home/stack/templates/</code> so that you can add custom roles:
					</p><pre class="screen">$ cp /usr/share/openstack-tripleo-heat-templates/roles_data.yaml /home/stack/templates/roles_data_custom.yaml</pre></li><li class="listitem">
						Include the new custom role file in the <code class="literal">openstack overcloud deploy</code> command.
					</li></ol></div></section><section class="section" id="dedicated-nodes-mon"><div class="titlepage"><div><div><h2 class="title">3.2. Creating a custom role and flavor for the Ceph MON service</h2></div></div></div><p>
				Complete the following steps to create a custom role <code class="literal">CephMon</code> and flavor <code class="literal">ceph-mon</code> for the Ceph MON role. You must already have a copy of the default roles data file as described in <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open the <code class="literal">/home/stack/templates/roles_data_custom.yaml</code> file.
					</li><li class="listitem">
						Remove the service entry for the Ceph MON service (namely, <span class="strong strong"><strong>OS::TripleO::Services::CephMon</strong></span>) from the Controller role.
					</li><li class="listitem"><p class="simpara">
						Add the <span class="strong strong"><strong>OS::TripleO::Services::CephClient</strong></span> service to the Controller role:
					</p><pre class="literallayout">[...]
- name: Controller # the 'primary' role goes first
  CountDefault: 1
  ServicesDefault:
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CephMds
    <span class="strong strong"><strong>- OS::TripleO::Services::CephClient</strong></span>
    - OS::TripleO::Services::CephExternal
    - OS::TripleO::Services::CephRbdMirror
    - OS::TripleO::Services::CephRgw
    - OS::TripleO::Services::CinderApi
[...]</pre></li><li class="listitem"><p class="simpara">
						At the end of the <code class="literal">roles_data_custom.yaml</code> file, add a custom <code class="literal">CephMon</code> role that contains the Ceph MON service and all the other required node services:
					</p><pre class="literallayout">- name: <span class="strong strong"><strong>CephMon</strong></span>
  ServicesDefault:
    # Common Services
    - OS::TripleO::Services::AuditD
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CertmongerUser
    - OS::TripleO::Services::Collectd
    - OS::TripleO::Services::Docker
    - OS::TripleO::Services::FluentdClient
    - OS::TripleO::Services::Kernel
    - OS::TripleO::Services::Ntp
    - OS::TripleO::Services::ContainersLogrotateCrond
    - OS::TripleO::Services::SensuClient
    - OS::TripleO::Services::Snmp
    - OS::TripleO::Services::Timezone
    - OS::TripleO::Services::TripleoFirewall
    - OS::TripleO::Services::TripleoPackages
    - OS::TripleO::Services::Tuned
    # Role-Specific Services
    <span class="strong strong"><strong>- OS::TripleO::Services::CephMon</strong></span></pre></li><li class="listitem"><p class="simpara">
						Run the <code class="literal">openstack flavor create</code> command to define a new flavor named <code class="literal">ceph-mon</code> for the <code class="literal">CephMon</code> role:
					</p><pre class="literallayout">$ openstack flavor create --id auto --ram 6144 --disk 40 --vcpus 4 ceph-mon</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For details about this command, run <code class="literal">openstack flavor create --help</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Map this flavor to a new profile, also named <code class="literal">ceph-mon</code>:
					</p><pre class="literallayout">$ openstack flavor set --property "cpu_arch"="x86_64" --property "capabilities:boot_option"="local" --property "capabilities:profile"="ceph-mon" ceph-mon</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For details about this command, run <code class="literal">openstack flavor set --help</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Tag nodes into the new <code class="literal">ceph-mon</code> profile:
					</p><pre class="literallayout">$ ironic node-update <span class="emphasis"><em>UUID</em></span> add properties/capabilities='<span class="strong strong"><strong>profile:ceph-mon</strong></span>,boot_option:local'</pre></li><li class="listitem"><p class="simpara">
						Add the following configuration to the <code class="literal">node-info.yaml</code> file to associate the <code class="literal">ceph-mon</code> flavor with the CephMon role:
					</p><pre class="screen">parameter_defaults:
  OvercloudCephMonFlavor: CephMon
  CephMonCount: 3</pre></li></ol></div><p>
				For more information about tagging nodes, see <a class="xref" href="index.html#manual-node-tag" title="2.4. Manually tagging nodes into profiles">Section 2.4, “Manually tagging nodes into profiles”</a>. For more information about custom role profiles, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/#sect-Tagging_Nodes_into_Profiles">Tagging Nodes Into Profiles</a>.
			</p></section><section class="section" id="dedicated-nodes-mds"><div class="titlepage"><div><div><h2 class="title">3.3. Creating a custom role and flavor for the Ceph MDS service</h2></div></div></div><p>
				Complete the following steps to create a custom role <code class="literal">CephMDS</code> and flavor <code class="literal">ceph-mds</code> for the Ceph MDS role. You must already have a copy of the default roles data file as described in <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Open the <code class="literal">/home/stack/templates/roles_data_custom.yaml</code> file.
					</li><li class="listitem"><p class="simpara">
						Remove the service entry for the Ceph MDS service (namely, <span class="strong strong"><strong>OS::TripleO::Services::CephMds</strong></span>) from the Controller role:
					</p><pre class="literallayout">[...]
- name: Controller # the 'primary' role goes first
  CountDefault: 1
  ServicesDefault:
    - OS::TripleO::Services::CACerts
    <span class="strong strong"><strong># - OS::TripleO::Services::CephMds</strong></span> <span id="CO1-1"><!--Empty--></span><span class="callout">1</span>
    - OS::TripleO::Services::CephMon
    - OS::TripleO::Services::CephExternal
    - OS::TripleO::Services::CephRbdMirror
    - OS::TripleO::Services::CephRgw
    - OS::TripleO::Services::CinderApi
[...]</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Comment out this line. In the next step, you add this service to the new custom role.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						At the end of the <code class="literal">roles_data_custom.yaml</code> file, add a custom <code class="literal">CephMDS</code> role containing the Ceph MDS service and all the other required node services:
					</p><pre class="literallayout">- name: <span class="strong strong"><strong>CephMDS</strong></span>
  ServicesDefault:
    # Common Services
    - OS::TripleO::Services::AuditD
    - OS::TripleO::Services::CACerts
    - OS::TripleO::Services::CertmongerUser
    - OS::TripleO::Services::Collectd
    - OS::TripleO::Services::Docker
    - OS::TripleO::Services::FluentdClient
    - OS::TripleO::Services::Kernel
    - OS::TripleO::Services::Ntp
    - OS::TripleO::Services::ContainersLogrotateCrond
    - OS::TripleO::Services::SensuClient
    - OS::TripleO::Services::Snmp
    - OS::TripleO::Services::Timezone
    - OS::TripleO::Services::TripleoFirewall
    - OS::TripleO::Services::TripleoPackages
    - OS::TripleO::Services::Tuned
    # Role-Specific Services
    <span class="strong strong"><strong>- OS::TripleO::Services::CephMds</strong></span>
    <span class="strong strong"><strong>- OS::TripleO::Services::CephClient</strong></span> <span id="CO2-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist" id="BZ1430096"><dl class="calloutlist"><dt><a href="index.html#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The Ceph MDS service requires the admin keyring, which you can set with either the Ceph MON or Ceph Client service. If you deploy Ceph MDS on a dedicated node without the Ceph MON service, you must also include the Ceph Client service in the new <code class="literal">CephMDS</code> role.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						Run the <code class="literal">openstack flavor create</code> command to define a new flavor named <code class="literal">ceph-mds</code> for this role:
					</p><pre class="literallayout">$ openstack flavor create --id auto --ram 6144 --disk 40 --vcpus 4 ceph-mds</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For details about this command, run <code class="literal">openstack flavor create --help</code>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Map the new <code class="literal">ceph-mds</code> flavor to a new profile, also named <code class="literal">ceph-mds</code>:
					</p><pre class="literallayout">$ openstack flavor set --property "cpu_arch"="x86_64" --property "capabilities:boot_option"="local" --property "capabilities:profile"="ceph-mds" ceph-mds</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For details about this command, run <code class="literal">openstack flavor set --help</code>.
						</p></div></div></li><li class="listitem">
						Tag nodes into the new <code class="literal">ceph-mds</code> profile:
					</li></ol></div><pre class="literallayout">$ ironic node-update <span class="emphasis"><em>UUID</em></span> add properties/capabilities='<span class="strong strong"><strong>profile:ceph-mds</strong></span>,boot_option:local'</pre><p>
				For more information about tagging nodes, see <a class="xref" href="index.html#manual-node-tag" title="2.4. Manually tagging nodes into profiles">Section 2.4, “Manually tagging nodes into profiles”</a>. For more information about custom role profiles, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/#sect-Tagging_Nodes_into_Profiles">Tagging Nodes Into Profiles</a>.
			</p></section></section><section class="chapter" id="enable-ceph-overcloud"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Customizing the Storage service</h1></div></div></div><p>
			The heat template collection provided by the director already contains the necessary templates and environment files to enable a basic Ceph Storage configuration.
		</p><p>
			The director uses the <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml</code> environment file to create a Ceph cluster and integrate it with your overcloud during deployment. This cluster features containerized Ceph Storage nodes. For more information about containerized services in OpenStack, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#creating-a-basic-overcloud-with-cli-tools">Configuring a basic overcloud with the CLI tools</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
		</p><p>
			The Red Hat OpenStack director also applies basic, default settings to the deployed Ceph cluster. You must also define any additional configuration in a custom environment file:
		</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					Create the file <code class="literal">storage-config.yaml</code> in <code class="literal">/home/stack/templates/</code>. In this example, the <code class="literal">~/templates/storage-config.yaml</code> file contains most of the overcloud-related custom settings for your environment. Parameters that you include in the custom environment file override the corresponding default settings from the <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml</code> file.
				</li><li class="listitem"><p class="simpara">
					Add a <code class="literal">parameter_defaults</code> section to <code class="literal">~/templates/storage-config.yaml</code>. This section contains custom settings for your overcloud. For example, to set <code class="literal">vxlan</code> as the network type of the networking service (<code class="literal">neutron</code>), add the following snippet to your custom environment file:
				</p><pre class="literallayout">parameter_defaults:
  NeutronNetworkType: vxlan</pre></li><li class="listitem"><p class="simpara">
					If necessary, set the following options under <code class="literal">parameter_defaults</code> according to your requirements:
				</p><div class="informaltable"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 33%; " class="col_1"><!--Empty--></col><col style="width: 33%; " class="col_2"><!--Empty--></col><col style="width: 33%; " class="col_3"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140342042118128" scope="col">Option</th><th align="left" valign="top" id="idm140342042117040" scope="col">Description</th><th align="left" valign="top" id="idm140342042115952" scope="col">Default value</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									CinderEnableIscsiBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Enables the iSCSI backend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									false
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									CinderEnableRbdBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Enables the Ceph Storage back end
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									true
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									CinderBackupBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Sets ceph or swift as the back end for volume backups. For more information, see <a class="xref" href="index.html#cinder-backup-ceph" title="4.4. Configuring the Backup Service to use Ceph">Section 4.4, “Configuring the Backup Service to use Ceph”</a>.
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									ceph
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									NovaEnableRbdBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Enables Ceph Storage for Nova ephemeral storage
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									true
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									GlanceBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Defines which back end the Image service should use: <code class="literal">rbd</code> (Ceph), <code class="literal">swift</code>, or <code class="literal">file</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									rbd
								</p>
								 </td></tr><tr><td align="left" valign="top" headers="idm140342042118128"> <p>
									GnocchiBackend
								</p>
								 </td><td align="left" valign="top" headers="idm140342042117040"> <p>
									Defines which back end the Telemetry service should use: <code class="literal">rbd</code> (Ceph), <code class="literal">swift</code>, or <code class="literal">file</code>
								</p>
								 </td><td align="left" valign="top" headers="idm140342042115952"> <p>
									rbd
								</p>
								 </td></tr></tbody></table></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can omit an option from <code class="literal">~/templates/storage-config.yaml</code> if you intend to use the default setting.
					</p></div></div></li></ol></div><p>
			The contents of your custom environment file change depending on the settings that you apply in the following sections. See <a class="xref" href="index.html#envfile-createceph" title="Appendix A. Sample environment file: creating a Ceph Storage cluster">Appendix A, <em>Sample environment file: creating a Ceph Storage cluster</em></a> for a completed example.
		</p><p>
			The following subsections contain information about overriding the common default storage service settings that the director applies.
		</p><section class="section" id="ceph-mds"><div class="titlepage"><div><div><h2 class="title">4.1. Enabling the Ceph Metadata Server</h2></div></div></div><p>
				The Ceph Metadata Server (MDS) runs the <code class="literal">ceph-mds</code> daemon, which manages metadata related to files stored on CephFS. CephFS can be consumed through NFS. For more information about using CephFS through NFS, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/file_system_guide/index">File System Guide</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/cephfs_via_nfs_back_end_guide_for_the_shared_file_system_service/">CephFS via NFS Back End Guide for the Shared File Systems service</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Red Hat supports deploying Ceph MDS only with the CephFS through NFS back end for the Shared File Systems service.
				</p></div></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To enable the Ceph Metadata Server, invoke the following environment file when you create your overcloud:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-mds.yaml</code>
					</li></ul></div><p>
				For more information, see <a class="xref" href="index.html#sect-Creating_the_Overcloud" title="7.2. Initiating overcloud deployment">Section 7.2, “Initiating overcloud deployment”</a>. For more information about the Ceph Metadata Server, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/file_system_guide/configuring-metadata-server-daemons">Configuring Metadata Server Daemons</a>.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					By default, the Ceph Metadata Server will be deployed on the Controller node. You can deploy the Ceph Metadata Server on its own dedicated node. For more information, see <a class="xref" href="index.html#dedicated-nodes-mds" title="3.3. Creating a custom role and flavor for the Ceph MDS service">Section 3.3, “Creating a custom role and flavor for the Ceph MDS service”</a>.
				</p></div></div></section><section class="section" id="ceph-rgw"><div class="titlepage"><div><div><h2 class="title">4.2. Enabling the Ceph Object Gateway</h2></div></div></div><p>
				The Ceph Object Gateway (RGW) provides applications with an interface to object storage capabilities within a Ceph Storage cluster. When you deploy RGW, you can replace the default Object Storage service (<code class="literal">swift</code>) with Ceph. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/object_gateway_configuration_and_administration_guide/index#overview-rgw">Object Gateway Configuration and Administration Guide</a>.
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					To enable RGW in your deployment, invoke the following environment file when you create the overcloud:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml</code>
					</li></ul></div><p>
				For more information, see <a class="xref" href="index.html#sect-Creating_the_Overcloud" title="7.2. Initiating overcloud deployment">Section 7.2, “Initiating overcloud deployment”</a>.
			</p><p>
				By default, Ceph Storage allows 250 placement groups per OSD. When you enable RGW, Ceph Storage creates six additional pools that are required by RGW. The new pools are:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						.rgw.root
					</li><li class="listitem">
						default.rgw.control
					</li><li class="listitem">
						default.rgw.meta
					</li><li class="listitem">
						default.rgw.log
					</li><li class="listitem">
						default.rgw.buckets.index
					</li><li class="listitem">
						default.rgw.buckets.data
					</li></ul></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					In your deployment, <code class="literal">default</code> is replaced with the name of the zone to which the pools belongs.
				</p></div></div><p>
				Therefore, when you enable RGW, be sure to set the default <code class="literal">pg_num</code> using the <code class="literal">CephPoolDefaultPgNum</code> parameter to account for the new pools. For more information about how to calculate the number of placement groups for Ceph pools, see <a class="xref" href="index.html#custom-ceph-pools" title="5.4. Assigning custom attributes to different Ceph pools">Section 5.4, “Assigning custom attributes to different Ceph pools”</a>.
			</p><p>
				The Ceph Object Gateway is a direct replacement for the default Object Storage service. As such, all other services that normally use <code class="literal">swift</code> can seamlessly start using the Ceph Object Gateway instead without further configuration. For more information, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/block_storage_backup_guide/">Block Storage Backup Guide</a>.
			</p></section><section class="section" id="configuring_ceph_object_store_to_use_external_ceph_object_gateway"><div class="titlepage"><div><div><h2 class="title">4.3. Configuring Ceph Object Store to use external Ceph Object Gateway</h2></div></div></div><p>
				Red Hat OpenStack Platform (RHOSP) director supports configuring an external Ceph Object Gateway (RGW) as an Object Store service. To authenticate with the external RGW service, you must configure RGW to verify users and their roles in the Identity service (keystone).
			</p><p>
				For more information about how to configure an external Ceph Object Gateway, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/using_keystone_with_the_ceph_object_gateway_guide/index#configuring-the-ceph-object-getaway-to-use-keystone-authentication_rgw-keystone">Configuring the Ceph Object Gateway to use Keystone authentication</a> in the <span class="emphasis"><em>Using Keystone with the Ceph Object Gateway Guide</em></span>.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Add the following <code class="literal">parameter_defaults</code> to a custom environment file, for example, <code class="literal">swift-external-params.yaml</code>, and adjust the values to suit your deployment:
					</p><pre class="screen">parameter_defaults:
   ExternalSwiftPublicUrl: 'http://&lt;Public RGW endpoint or loadbalancer&gt;:8080/swift/v1/AUTH_%(project_id)s'
   ExternalSwiftInternalUrl: 'http://&lt;Internal RGW endpoint&gt;:8080/swift/v1/AUTH_%(project_id)s'
   ExternalSwiftAdminUrl: 'http://&lt;Admin RGW endpoint&gt;:8080/swift/v1/AUTH_%(project_id)s'
   ExternalSwiftUserTenant: 'service'
   SwiftPassword: 'choose_a_random_password'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							The example code snippet contains parameter values that might differ from values that you use in your environment:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The default port where the remote RGW instance listens is <code class="literal">8080</code>. The port might be different depending on how the external RGW is configured.
								</li><li class="listitem">
									The <code class="literal">swift</code> user created in the overcloud uses the password defined by the <code class="literal">SwiftPassword</code> parameter. You must configure the external RGW instance to use the same password to authenticate with the Identity service by using the <code class="literal">rgw_keystone_admin_password</code>.
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Add the following code to the Ceph config file to configure RGW to use the Identity service. Adjust the variable values to suit your environment.
					</p><pre class="screen">    rgw_keystone_api_version: 3
    rgw_keystone_url: http://&lt;public Keystone endpoint&gt;:5000/
    rgw_keystone_accepted_roles: 'member, Member, admin'
    rgw_keystone_accepted_admin_roles: ResellerAdmin, swiftoperator
    rgw_keystone_admin_domain: default
    rgw_keystone_admin_project: service
    rgw_keystone_admin_user: swift
    rgw_keystone_admin_password: &lt;Password as defined in the environment parameters&gt;
    rgw_keystone_implicit_tenants: 'true'
    rgw_keystone_revocation_interval: '0'
    rgw_s3_auth_use_keystone: 'true'
    rgw_swift_versioning_enabled: 'true'
    rgw_swift_account_in_url: 'true'</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							Director creates the following roles and users in the Identity service by default:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									rgw_keystone_accepted_admin_roles: ResellerAdmin, swiftoperator
								</li><li class="listitem">
									rgw_keystone_admin_domain: default
								</li><li class="listitem">
									rgw_keystone_admin_project: service
								</li><li class="listitem">
									rgw_keystone_admin_user: swift
								</li></ul></div></div></div></li><li class="listitem"><p class="simpara">
						Deploy the overcloud with the additional environment files:
					</p><pre class="screen">openstack overcloud deploy --templates \
-e &lt;your environment files&gt;
-e /usr/share/openstack-tripleo-heat-templates/environments/swift-external.yaml
-e swift-external-params.yaml</pre></li></ol></div></section><section class="section" id="cinder-backup-ceph"><div class="titlepage"><div><div><h2 class="title">4.4. Configuring the Backup Service to use Ceph</h2></div></div></div><p>
				The Block Storage Backup service (<code class="literal">cinder-backup</code>) is disabled by default. To enable the Block Storage Backup service, complete the following steps:
			</p><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
					Invoke the following environment file when you create your overcloud:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml</code>
					</li></ul></div></section><section class="section" id="multibonded-nics"><div class="titlepage"><div><div><h2 class="title">4.5. Configuring multiple bonded interfaces for Ceph nodes</h2></div></div></div><p>
				Use a bonded interface to combine multiple NICs and add redundancy to a network connection. If you have enough NICs on your Ceph nodes, you can create multiple bonded interfaces on each node to expand redundancy capability.
			</p><p>
				You can then use a bonded interface for each network connection that the node requires. This provides both redundancy and a dedicated connection for each network.
			</p><p>
				The simplest implementation of bonded interfaces involves the use of two bonds, one for each storage network used by the Ceph nodes. These networks are the following:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Front-end storage network (<code class="literal">StorageNet</code>)</span></dt><dd>
							The Ceph client uses this network to interact with the corresponding Ceph cluster.
						</dd><dt><span class="term">Back-end storage network (<code class="literal">StorageMgmtNet</code>)</span></dt><dd>
							The Ceph cluster uses this network to balance data in accordance with the placement group policy of the cluster. For more information, see <a class="link" href="https://access.redhat.com/documentation/en/red-hat-ceph-storage/2/single/architecture-guide#placement_groups_pgs">Placement Groups (PG)</a> in the in the <span class="emphasis"><em>Red Hat Ceph Architecture Guide</em></span>.
						</dd></dl></div><p>
				To configure multiple bonded interfaces, you must create a new network interface template, as the director does not provide any sample templates that you can use to deploy multiple bonded NICs. However, the director does provide a template that deploys a single bonded interface. This template is <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code>. You can define an additional bonded interface for your additional NICs in this template.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					For more information about creating custom interface templates, <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#custom-network-interface-templates">Creating Custom Interface Templates</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
				</p></div></div><p>
				The following snippet contains the default definition for the single bonded interface defined in the <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code> file:
			</p><pre class="literallayout">  type: ovs_bridge // <span id="CO3-1"><!--Empty--></span><span class="callout">1</span>
  name: br-bond
  members:
    -
      type: ovs_bond // <span id="CO3-2"><!--Empty--></span><span class="callout">2</span>
      name: bond1 // <span id="CO3-3"><!--Empty--></span><span class="callout">3</span>
      ovs_options: {get_param: BondInterfaceOvsOptions} <span id="CO3-4"><!--Empty--></span><span class="callout">4</span>
      members: // <span id="CO3-5"><!--Empty--></span><span class="callout">5</span>
        -
          type: interface
          name: nic2
          primary: true
        -
          type: interface
          name: nic3
    -
      type: vlan // <span id="CO3-6"><!--Empty--></span><span class="callout">6</span>
      device: bond1 // <span id="CO3-7"><!--Empty--></span><span class="callout">7</span>
      vlan_id: {get_param: StorageNetworkVlanID}
      addresses:
        -
          ip_netmask: {get_param: StorageIpSubnet}
    -
      type: vlan
      device: bond1
      vlan_id: {get_param: StorageMgmtNetworkVlanID}
      addresses:
        -
          ip_netmask: {get_param: StorageMgmtIpSubnet}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						A single bridge named <code class="literal">br-bond</code> holds the bond defined in this template. This line defines the bridge type, namely OVS.
					</div></dd><dt><a href="index.html#CO3-2"><span class="callout">2</span></a> </dt><dd><div class="para">
						The first member of the <code class="literal">br-bond</code> bridge is the bonded interface itself, named <code class="literal">bond1</code>. This line defines the bond type of <code class="literal">bond1</code>, which is also OVS.
					</div></dd><dt><a href="index.html#CO3-3"><span class="callout">3</span></a> </dt><dd><div class="para">
						The default bond is named <code class="literal">bond1</code>.
					</div></dd><dt><a href="index.html#CO3-4"><span class="callout">4</span></a> </dt><dd><div class="para">
						The <code class="literal">ovs_options</code> entry instructs director to use a specific set of bonding module directives. Those directives are passed through the <code class="literal">BondInterfaceOvsOptions</code>, which you can also configure in this file. For more information about configuring bonding module directives, see <a class="xref" href="index.html#multibonded-nics-ovs-opts" title="4.5.1. Configuring bonding module directives">Section 4.5.1, “Configuring bonding module directives”</a>.
					</div></dd><dt><a href="index.html#CO3-5"><span class="callout">5</span></a> </dt><dd><div class="para">
						The <code class="literal">members</code> section of the bond defines which network interfaces are bonded by <code class="literal">bond1</code>. In this example, the bonded interface uses <code class="literal">nic2</code> (set as the primary interface) and <code class="literal">nic3</code>.
					</div></dd><dt><a href="index.html#CO3-6"><span class="callout">6</span></a> </dt><dd><div class="para">
						The <code class="literal">br-bond</code> bridge has two other members: a VLAN for both front-end (<code class="literal">StorageNetwork</code>) and back-end (<code class="literal">StorageMgmtNetwork</code>) storage networks.
					</div></dd><dt><a href="index.html#CO3-7"><span class="callout">7</span></a> </dt><dd><div class="para">
						The <code class="literal">device</code> parameter defines which device a VLAN should use. In this example, both VLANs use the bonded interface, <code class="literal">bond1</code>.
					</div></dd></dl></div><p>
				With at least two more NICs, you can define an additional bridge and bonded interface. Then, you can move one of the VLANs to the new bonded interface, which increases throughput and reliability for both storage network connections.
			</p><p>
				When you customize the <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code> file for this purpose, Red Hat recommends that you use Linux bonds (<code class="literal">type: linux_bond</code> ) instead of the default OVS (<code class="literal">type: ovs_bond</code>). This bond type is more suitable for enterprise production deployments.
			</p><p>
				The following edited snippet defines an additional OVS bridge (<code class="literal">br-bond2</code>) which houses a new Linux bond named <code class="literal">bond2</code>. The <code class="literal">bond2</code> interface uses two additional NICs, <code class="literal">nic4</code> and <code class="literal">nic5</code>, and is used solely for back-end storage network traffic:
			</p><pre class="literallayout">  type: ovs_bridge
  name: br-bond
  members:
    -
      type: linux_bond
      name: bond1
      <span class="strong strong"><strong>bonding_options</strong></span>: {get_param: BondInterfaceOvsOptions} // <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
      members:
        -
          type: interface
          name: nic2
          primary: true
        -
          type: interface
          name: nic3
    -
      type: vlan
      device: bond1
      vlan_id: {get_param: StorageNetworkVlanID}
      addresses:
        -
          ip_netmask: {get_param: StorageIpSubnet}
-
  type: ovs_bridge
  name: br-bond2
  members:
    -
      type: linux_bond
      name: bond2
      <span class="strong strong"><strong>bonding_options</strong></span>: {get_param: BondInterfaceOvsOptions}
      members:
        -
          type: interface
          name: nic4
          primary: true
        -
          type: interface
          name: nic5
    -
      type: vlan
      device: bond1
      vlan_id: {get_param: StorageMgmtNetworkVlanID}
      addresses:
        -
          ip_netmask: {get_param: StorageMgmtIpSubnet}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						As <code class="literal">bond1</code> and <code class="literal">bond2</code> are both Linux bonds (instead of OVS), they use <code class="literal">bonding_options</code> instead of <code class="literal">ovs_options</code> to set bonding directives. For more information, see <a class="xref" href="index.html#multibonded-nics-ovs-opts" title="4.5.1. Configuring bonding module directives">Section 4.5.1, “Configuring bonding module directives”</a>.
					</div></dd></dl></div><p>
				For the full contents of this customized template, see <a class="xref" href="index.html#template-multibonded-nics" title="Appendix B. Sample custom interface template: multiple bonded interfaces">Appendix B, <em>Sample custom interface template: multiple bonded interfaces</em></a>.
			</p><section class="section" id="multibonded-nics-ovs-opts"><div class="titlepage"><div><div><h3 class="title">4.5.1. Configuring bonding module directives</h3></div></div></div><p>
					After you add and configure the bonded interfaces, use the <code class="literal">BondInterfaceOvsOptions</code> parameter to set the directives that you want each bonded interface to use. You can find this information in the <code class="literal">parameters:</code> section of the <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code> file. The following snippet shows the default definition of this parameter (namely, empty):
				</p><pre class="screen">BondInterfaceOvsOptions:
    default: ''
    description: The ovs_options string for the bond interface. Set
                 things like lacp=active and/or bond_mode=balance-slb
                 using this option.
    type: string</pre><p>
					Define the options you need in the <code class="literal">default:</code> line. For example, to use 802.3ad (mode 4) and a LACP rate of 1 (fast), use <code class="literal">'mode=4 lacp_rate=1'</code>:
				</p><pre class="screen">BondInterfaceOvsOptions:
    default: 'mode=4 lacp_rate=1'
    description: The bonding_options string for the bond interface. Set
                 things like lacp=active and/or bond_mode=balance-slb
                 using this option.
    type: string</pre><p>
					For more information about other supported bonding options, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#open-vswitch-bonding-options">Open vSwitch Bonding Options</a> in the <span class="emphasis"><em>Advanced Overcloud Optimization</em></span> guide. For the full contents of the customized <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code> template, see <a class="xref" href="index.html#template-multibonded-nics" title="Appendix B. Sample custom interface template: multiple bonded interfaces">Appendix B, <em>Sample custom interface template: multiple bonded interfaces</em></a>.
				</p></section></section></section><section class="chapter" id="Configuring_Ceph_Storage_Cluster_Settings"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Customizing the Ceph Storage cluster</h1></div></div></div><p>
			Director deploys containerized Red Hat Ceph Storage using a default configuration. You can customize Ceph Storage by overriding the default settings.
		</p><div class="formalpara"><p class="title"><strong>Prerequistes</strong></p><p>
				To deploy containerized Ceph Storage you must include the <code class="literal">/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml</code> file during overcloud deployment. This environment file defines the following resources:
			</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<code class="literal">CephAnsibleDisksConfig</code> - This resource maps the Ceph Storage node disk layout. For more information, see <a class="xref" href="index.html#Mapping_the_Ceph_Storage_Node_Disk_Layout" title="5.3. Mapping the Ceph Storage node disk layout">Section 5.3, “Mapping the Ceph Storage node disk layout”</a>.
				</li><li class="listitem">
					<code class="literal">CephConfigOverrides</code> - This resource applies all other custom settings to your Ceph Storage cluster.
				</li></ul></div><p>
			Use these resources to override any defaults that the director sets for containerized Ceph Storage.
		</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
					Enable the Red Hat Ceph Storage 4 Tools repository:
				</p><pre class="screen">$ sudo subscription-manager repos --enable=rhceph-4-tools-for-rhel-8-x86_64-rpms</pre></li><li class="listitem"><p class="simpara">
					Install the <code class="literal">ceph-ansible</code> package on the undercloud:
				</p><pre class="screen">$ sudo dnf install ceph-ansible</pre></li><li class="listitem"><p class="simpara">
					To customize your Ceph Storage cluster, define custom parameters in a new environment file, for example, <code class="literal">/home/stack/templates/ceph-config.yaml</code>. You can apply Ceph Storage cluster settings with the following syntax in the <code class="literal">parameter_defaults</code> section of your environment file:
				</p><pre class="screen">parameter_defaults:
  CephConfigOverrides:
    section:
      KEY:VALUE</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						You can apply the <code class="literal">CephConfigOverrides</code> parameter to the <code class="literal">[global]</code> section of the <code class="literal">ceph.conf</code> file, as well as any other section, such as <code class="literal">[osd]</code>, <code class="literal">[mon]</code>, and <code class="literal">[client]</code>. If you specify a section, the <code class="literal">key:value</code> data goes into the specified section. If you do not specify a section, the data goes into the <code class="literal">[global]</code> section by default. For information about Ceph Storage configuration, customization, and supported parameters, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/configuration_guide/index">Red Hat Ceph Storage Configuration Guide</a>.
					</p></div></div></li><li class="listitem"><p class="simpara">
					Replace <code class="literal">KEY</code> and <code class="literal">VALUE</code> with the Ceph cluster settings that you want to apply. For example, in the <code class="literal">global</code> section, <code class="literal">max_open_files</code> is the <code class="literal">KEY</code> and <code class="literal">131072</code> is the corresponding <code class="literal">VALUE</code>:
				</p><pre class="screen">parameter_defaults:
  CephConfigOverrides:
    global:
      max_open_files: 131072
    osd:
      osd_scrub_during_recovery: false</pre><p class="simpara">
					This configuration results in the following settings defined in the configuration file of your Ceph cluster:
				</p><pre class="screen">[global]
max_open_files = 131072
[osd]
osd_scrub_during_recovery = false</pre></li></ol></div><section class="section" id="setting_ceph_ansible_group_variables"><div class="titlepage"><div><div><h2 class="title">5.1. Setting ceph-ansible group variables</h2></div></div></div><p>
				The <code class="literal">ceph-ansible</code> tool is a playbook used to install and manage Ceph Storage clusters.
			</p><p>
				The <code class="literal">ceph-ansible</code> tool has a <code class="literal">group_vars</code> directory that defines configuration options and the default settings for those options. Use the <code class="literal">group_vars</code> directory to set Ceph Storage parameters.
			</p><p>
				For information about the <code class="literal">group_vars</code> directory, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/installation_guide/index#installing-a-red-hat-ceph-storage-cluster_install">Installing a Red Hat Ceph Storage cluster</a> in the <span class="emphasis"><em>Installation Guide</em></span>.
			</p><p>
				To change the variable defaults in director, use the <code class="literal">CephAnsibleExtraConfig</code> parameter to pass the new values in heat environment files. For example, to set the <code class="literal">ceph-ansible</code> group variable <code class="literal">journal_size</code> to 40960, create an environment file with the following <code class="literal">journal_size</code> definition:
			</p><pre class="screen">parameter_defaults:
  CephAnsibleExtraConfig:
    journal_size: 40960</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Change <code class="literal">ceph-ansible</code> group variables with the override parameters; do not edit group variables directly in the <code class="literal">/usr/share/ceph-ansible</code> directory on the undercloud.
				</p></div></div></section><section class="section" id="ceph-containers-for-osp-with-ceph-storage"><div class="titlepage"><div><div><h2 class="title">5.2. Ceph containers for Red Hat OpenStack Platform with Ceph Storage</h2></div></div></div><p>
				A Ceph container is required to configure OpenStack Platform to use Ceph, even with an external Ceph cluster. To be compatible with Red Hat Enterprise Linux 8, OpenStack Platform 15 requires Red Hat Ceph Storage 4. The Ceph Storage 4 container is hosted at registry.redhat.io, a registry which requires authentication.
			</p><p>
				You can use the heat environment parameter <code class="literal">ContainerImageRegistryCredentials</code> to authenticate at <code class="literal">registry.redhat.io</code>, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/transitioning_to_containerized_services/#container-image-preparation-parameters">Container image preparation parameters</a>.
			</p></section><section class="section" id="Mapping_the_Ceph_Storage_Node_Disk_Layout"><div class="titlepage"><div><div><h2 class="title">5.3. Mapping the Ceph Storage node disk layout</h2></div></div></div><p>
				When you deploy containerized Ceph Storage, you must map the disk layout and specify dedicated block devices for the Ceph OSD service. You can perform this mapping in the environment file that you created earlier to define your custom Ceph parameters: <code class="literal">/home/stack/templates/ceph-config.yaml</code>.
			</p><p>
				Use the <code class="literal">CephAnsibleDisksConfig</code> resource in <code class="literal">parameter_defaults</code> to map your disk layout. This resource uses the following variables:
			</p><div class="informaltable"><table class="gt-4-cols lt-7-rows"><colgroup><col style="width: 25%; " class="col_1"><!--Empty--></col><col style="width: 25%; " class="col_2"><!--Empty--></col><col style="width: 25%; " class="col_3"><!--Empty--></col><col style="width: 25%; " class="col_4"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140342045764016" scope="col">Variable</th><th align="left" valign="top" id="idm140342045762928" scope="col">Required?</th><th align="left" valign="top" id="idm140342045761840" scope="col">Default value (if unset)</th><th align="left" valign="top" id="idm140342045760784" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140342045764016"> <p>
								osd_scenario
							</p>
							 </td><td align="left" valign="top" headers="idm140342045762928"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm140342045761840"> <p>
								lvm
							</p>
							 <p>
								NOTE: The default value is <code class="literal">lvm</code>.
							</p>
							 </td><td align="left" valign="top" headers="idm140342045760784"> <p>
								The <code class="literal">lvm</code> value allows ceph-ansible to use <code class="literal">ceph-volume</code> to configure OSDs and BlueStore WAL devices.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342045764016"> <p>
								devices
							</p>
							 </td><td align="left" valign="top" headers="idm140342045762928"> <p>
								Yes
							</p>
							 </td><td align="left" valign="top" headers="idm140342045761840"> <p>
								NONE. Variable must be set.
							</p>
							 </td><td align="left" valign="top" headers="idm140342045760784"> <p>
								A list of block devices that you want to use for OSDs on the node.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342045764016"> <p>
								dedicated_devices
							</p>
							 </td><td align="left" valign="top" headers="idm140342045762928"> <p>
								Yes (only if <code class="literal">osd_scenario</code> is <code class="literal">non-collocated</code>)
							</p>
							 </td><td align="left" valign="top" headers="idm140342045761840"> <p>
								devices
							</p>
							 </td><td align="left" valign="top" headers="idm140342045760784"> <p>
								A list of block devices that maps each entry in the <code class="literal">devices</code> parameter to a dedicated journaling block device. You can use this variable only when <code class="literal">osd_scenario=non-collocated</code>.
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342045764016"> <p>
								dmcrypt
							</p>
							 </td><td align="left" valign="top" headers="idm140342045762928"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm140342045761840"> <p>
								false
							</p>
							 </td><td align="left" valign="top" headers="idm140342045760784"> <p>
								Sets whether data stored on OSDs is encrypted (<code class="literal">true</code>) or unencrypted (<code class="literal">false</code>).
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342045764016"> <p>
								osd_objectstore
							</p>
							 </td><td align="left" valign="top" headers="idm140342045762928"> <p>
								No
							</p>
							 </td><td align="left" valign="top" headers="idm140342045761840"> <p>
								bluestore
							</p>
							 <p>
								NOTE: The default value is <code class="literal">bluestore</code>.
							</p>
							 </td><td align="left" valign="top" headers="idm140342045760784"> <p>
								Sets the storage back end used by Ceph.
							</p>
							 </td></tr></tbody></table></div><section class="section" id="using_bluestore"><div class="titlepage"><div><div><h3 class="title">5.3.1. Using BlueStore</h3></div></div></div><p>
					To specify the block devices that you want to use as Ceph OSDs, use a variation of the following snippet:
				</p><pre class="literallayout">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/nvme0n1
    osd_scenario: lvm
    osd_objectstore: bluestore</pre><p>
					Because <code class="literal">/dev/nvme0n1</code> is in a higher performing device class, the example parameter defaults produce three OSDs that run on <code class="literal">/dev/sdb</code>, <code class="literal">/dev/sdc</code>, and <code class="literal">/dev/sdd</code>. The three OSDs use <code class="literal">/dev/nvme0n1</code> as a BlueStore WAL device. The ceph-volume tool does this by using the <a class="link" href="http://docs.ceph.com/docs/master/ceph-volume/lvm/batch"><code class="literal">batch</code></a> subcommand. The same setup is duplicated for each Ceph storage node and assumes uniform hardware. If the BlueStore WAL data resides on the same disks as the OSDs, then change the parameter defaults:
				</p><pre class="literallayout">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></section><section class="section" id="referring_to_devices_with_persistent_names"><div class="titlepage"><div><div><h3 class="title">5.3.2. Referring to devices with persistent names</h3></div></div></div><p>
					In some nodes, disk paths, such as <code class="literal">/dev/sdb</code> and <code class="literal">/dev/sdc</code>, may not point to the same block device during reboots. If this is the case with your <code class="literal">CephStorage</code> nodes, specify each disk with the <code class="literal">/dev/disk/by-path/</code> symlink to ensure that the block device mapping is consistent throughout deployments:
				</p><pre class="literallayout">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:

      - /dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:10:0
      - /dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:11:0


    dedicated_devices
      - /dev/nvme0n1
      - /dev/nvme0n1</pre><p>
					Because you must set the list of OSD devices prior to overcloud deployment, it may not be possible to identify and set the PCI path of disk devices. In this case, gather the <code class="literal">/dev/disk/by-path/symlink</code> data for block devices during introspection.
				</p><p>
					In the following example, run the first command to download the introspection data from the undercloud Object Storage service (swift) for the server <code class="literal">b08-h03-r620-hci</code> and saves the data in a file called <code class="literal">b08-h03-r620-hci.json</code>. Run the second command to grep for “by-path”. The output of this command contains the unique <code class="literal">/dev/disk/by-path</code> values that you can use to identify disks.
				</p><pre class="literallayout">(undercloud) [stack@b08-h02-r620 ironic]$ openstack baremetal introspection data save b08-h03-r620-hci | jq . &gt; b08-h03-r620-hci.json
(undercloud) [stack@b08-h02-r620 ironic]$ grep by-path b08-h03-r620-hci.json
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:0:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:1:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:3:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:4:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:5:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:6:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:7:0",
        "by_path": "/dev/disk/by-path/pci-0000:02:00.0-scsi-0:2:0:0",</pre><p>
					For more information about naming conventions for storage devices, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_storage_devices/index#assembly_overview-of-persistent-naming-attributes_managing-storage-devices">Overview of persistent naming attributes</a> in the <span class="emphasis"><em>Managing storage devices</em></span> guide.
				</p><p>
					For details about each journaling scenario and disk mapping for containerized Ceph Storage, see the <a class="link" href="http://docs.ceph.com/ceph-ansible/master/osds/scenarios.html">OSD Scenarios</a> section of the <a class="link" href="http://docs.ceph.com/ceph-ansible/master/">project documentation for ceph-ansible</a>.
				</p></section></section><section class="section" id="custom-ceph-pools"><div class="titlepage"><div><div><h2 class="title">5.4. Assigning custom attributes to different Ceph pools</h2></div></div></div><p>
				By default, Ceph pools created with director have the same number of placement groups (<code class="literal">pg_num</code> and <code class="literal">pgp_num</code>) and sizes. You can use either method in <a class="xref" href="index.html#Configuring_Ceph_Storage_Cluster_Settings" title="Chapter 5. Customizing the Ceph Storage cluster">Chapter 5, <em>Customizing the Ceph Storage cluster</em></a> to override these settings globally; that is, doing so applies the same values for all pools.
			</p><p>
				You can also apply different attributes to each Ceph pool. To do so, use the <code class="literal">CephPools</code> parameter:
			</p><pre class="screen">parameter_defaults:
  CephPools:
    - name: POOL
      pg_num: 128
      application: rbd</pre><p>
				Replace <code class="literal">POOL</code> with the name of the pool that you want to configure and the <code class="literal">pg_num</code> setting to indicate the number of placement groups. This overrides the default <code class="literal">pg_num</code> for the specified pool.
			</p><p>
				If you use the <code class="literal">CephPools</code> parameter, you must also specify the application type. The application type for Compute, Block Storage, and Image Storage should be <code class="literal">rbd</code>, as shown in the examples, but depending on what the pool is used for, you might need to specify a different application type. For example, the application type for the gnocchi metrics pool is <code class="literal">openstack_gnocchi</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/storage_strategies_guide/index#enable_application">Enable Application</a> in the <span class="emphasis"><em>Storage Strategies Guide</em></span> .
			</p><p>
				If you do not use the <code class="literal">CephPools</code> parameter, director sets the appropriate application type automatically, but only for the default pool list.
			</p><p>
				You can also create new custom pools through the <code class="literal">CephPools</code> parameter. For example, to add a pool called <code class="literal">custompool</code>:
			</p><pre class="screen">parameter_defaults:
  CephPools:
    - name: custompool
      pg_num: 128
      application: rbd</pre><p>
				This creates a new custom pool in addition to the default pools.
			</p><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
				For typical pool configurations of common Ceph use cases, see the <a class="link" href="https://access.redhat.com/labs/cephpgc/">Ceph Placement Groups (PGs) per Pool Calculator</a>. This calculator is normally used to generate the commands for manually configuring your Ceph pools. In this deployment, the director will configure the pools based on your specifications.
			</p></div></div><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					Red Hat Ceph Storage 3 (Luminous) introduced a hard limit on the maximum number of PGs an OSD can have, which is 200 by default. Do not override this parameter beyond 200. If there is a problem because the Ceph PG number exceeds the maximum, adjust the <code class="literal">pg_num</code> per pool to address the problem, not the <code class="literal">mon_max_pg_per_osd</code>.
				</p></div></div></section><section class="section" id="map_disk_layout_non-homogen_ceph"><div class="titlepage"><div><div><h2 class="title">5.5. Mapping the disk layout to non-homogeneous Ceph Storage nodes</h2></div></div></div><p>
				By default, all nodes of a role that host Ceph OSDs (indicated by the <span class="strong strong"><strong>OS::TripleO::Services::CephOSD</strong></span> service in <code class="literal filename">roles_data.yaml</code>), for example <code class="literal">CephStorage</code> or <code class="literal">ComputeHCI</code> nodes, use the global <code class="literal">devices</code> and <code class="literal">dedicated_devices</code> lists set in <a class="xref" href="index.html#Mapping_the_Ceph_Storage_Node_Disk_Layout" title="5.3. Mapping the Ceph Storage node disk layout">Section 5.3, “Mapping the Ceph Storage node disk layout”</a>. This assumes that all of these servers have homogeneous hardware. If a subset of these servers do not have homogeneous hardware, then director needs to be aware that each of these servers has different <code class="literal">devices</code> and <code class="literal">dedicated_devices</code> lists. This is known as a <span class="emphasis"><em>node-specific disk configuration</em></span>.
			</p><p>
				To pass a node-specific disk configuration to director, you must pass a heat environment file, such as <code class="literal filename">node-spec-overrides.yaml</code>, to the <code class="literal command">openstack overcloud deploy</code> command and the file content must identify each server by a machine unique UUID and a list of local variables to override the global variables.
			</p><p>
				You can extract the machine unique UUID for each individual server or from the Ironic database.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						To locate the UUID for an individual server, log in to the server and enter the following command:
					</p><pre class="screen">dmidecode -s system-uuid</pre></li><li class="listitem"><p class="simpara">
						To extract the UUID from the Ironic database, enter the following command on the undercloud.
					</p><pre class="screen">openstack baremetal introspection data save NODE-ID | jq .extra.system.product.uuid</pre><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
							If the <span class="strong strong"><strong>undercloud.conf</strong></span> does not have <code class="literal">inspection_extras = true</code> before undercloud installation or upgrade and introspection, then the machine unique UUID is not in the Ironic database.
						</p></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							The machine unique UUID is not the Ironic UUID.
						</p></div></div><p class="simpara">
						A valid <code class="literal">node-spec-overrides.yaml</code> file might look like the following:
					</p><pre class="screen">parameter_defaults:
  NodeDataLookup: {"32E87B4C-C4A7-418E-865B-191684A6883B": {"devices": ["/dev/sdc"]}}</pre></li><li class="listitem"><p class="simpara">
						All lines after the first two lines must be valid JSON. You can verify that the JSON is valid by using the <code class="literal">jq</code> command:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Remove the first two lines (<code class="literal">parameter_defaults:</code> and <code class="literal">NodeDataLookup:</code>) from the file temporarily.
							</li><li class="listitem">
								Run <code class="literal">cat node-spec-overrides.yaml | jq .</code>
							</li></ol></div></li><li class="listitem"><p class="simpara">
						As the <code class="literal">node-spec-overrides.yaml</code> file grows, you can also use <code class="literal">jq</code> to ensure that the embedded JSON is valid. For example, because the <code class="literal">devices</code> and <code class="literal">dedicated_devices</code> list must be the same length, use the following to verify that they are the same length before you start the deployment.
					</p><pre class="screen">(undercloud) [stack@b08-h02-r620 tht]$ cat node-spec-c05-h17-h21-h25-6048r.yaml | jq '.[] | .devices | length'
33
30
33
(undercloud) [stack@b08-h02-r620 tht]$ cat node-spec-c05-h17-h21-h25-6048r.yaml | jq '.[] | .dedicated_devices | length'
33
30
33
(undercloud) [stack@b08-h02-r620 tht]$</pre><p class="simpara">
						In the above example, the <code class="literal">node-spec-c05-h17-h21-h25-6048r.yaml</code> has three servers in rack c05 in which slots h17, h21, and h25 are missing disks. A more complicated example is included at the end of this section.
					</p></li><li class="listitem"><p class="simpara">
						After the JSON has been validated add back the two lines which makes it a valid environment YAML file (<code class="literal">parameter_defaults:</code> and <code class="literal">NodeDataLookup:</code>) and include it with <code class="literal">-e</code> in the deployment. In the example below, the updated heat environment file uses <code class="literal">NodeDataLookup</code> for Ceph deployment. All of the servers had a devices list with 35 disks except one of them had a disk missing. This environment file overrides the default devices list for only that single node and gives it the list of 34 disks it must use instead of the global list.
					</p><pre class="screen">parameter_defaults:
  # c05-h01-6048r is missing scsi-0:2:35:0 (00000000-0000-0000-0000-0CC47A6EFD0C)
  NodeDataLookup: {
    "00000000-0000-0000-0000-0CC47A6EFD0C": {
      "devices": [
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:1:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:32:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:2:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:3:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:4:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:5:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:6:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:33:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:7:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:8:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:34:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:9:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:10:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:11:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:12:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:13:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:14:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:15:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:16:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:17:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:18:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:19:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:20:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:21:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:22:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:23:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:24:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:25:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:26:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:27:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:28:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:29:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:30:0",
    "/dev/disk/by-path/pci-0000:03:00.0-scsi-0:2:31:0"
        ],
      "dedicated_devices": [
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:81:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1",
    "/dev/disk/by-path/pci-0000:84:00.0-nvme-1"
        ]
      }
    }</pre></li></ol></div></section><section class="section" id="increasing-restart-delay-for-large-ceph-clusters"><div class="titlepage"><div><div><h2 class="title">5.6. Increasing the restart delay for large Ceph clusters</h2></div></div></div><p>
				During deployment, Ceph services such as OSDs and Monitors, are restarted and the deployment does not continue until the service is running again. Ansible waits 15 seconds (the delay) and checks 5 times for the service to start (the retries). If the service does not restart, the deployment stops so the operator can intervene.
			</p><p>
				Depending on the size of the Ceph cluster, you may need to increase the retry or delay values. The exact names of these parameters and their defaults are as follows:
			</p><pre class="screen"> health_mon_check_retries: 5
 health_mon_check_delay: 15
 health_osd_check_retries: 5
 health_osd_check_delay: 15</pre><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Update the <code class="literal">CephAnsibleExtraConfig</code> parameter to change the default delay and retry values:
					</p><pre class="screen">parameter_defaults:
  CephAnsibleExtraConfig:
    health_osd_check_delay: 40
    health_osd_check_retries: 30
    health_mon_check_delay: 20
    health_mon_check_retries: 10</pre><p class="simpara">
						This example makes the cluster check 30 times and wait 40 seconds between each check for the Ceph OSDs, and check 20 times and wait 10 seconds between each check for the Ceph MONs.
					</p></li><li class="listitem">
						To incorporate the changes, pass the updated <code class="literal">yaml</code> file with <code class="literal">-e</code> using <code class="literal">openstack overcloud deploy</code>.
					</li></ol></div></section></section><section class="chapter" id="proc_ceph-defining-performance-tiers-for-varying-workloads-with-ceph-ansible"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Defining performance tiers for varying workloads in a Ceph Storage cluster with director</h1></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Important</span></dt><dd>
						This procedure is currently for new director deployments only.
					</dd></dl></div><p>
			You can use Red Hat OpenStack Platform (RHOSP) director to deploy different Red Hat Ceph Storage performance tiers. You can combine Ceph CRUSH rules and the <code class="literal">CephPools</code> director parameter to use the device classes feature and build different tiers to accommodate workloads that have different performance requirements. For example, you can define a HDD class for normal workloads and an SSD class that distributes data only over SSDs for high performance loads. In this scenario, when you create a new Block Storage volume, you can choose the performance tier, either HDDs or SSDs.
		</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Ceph autodetects the disk type and assigns it to the corresponding device class, either HDD, SSD, or NVMe based on the hardware properties exposed by the Linux kernel. However, you can also customize the category according to your needs.
			</p></div></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					Red Hat Ceph Storage (RHCS) version 4.1 or later.
				</li></ul></div><p>
			To deploy different Red Hat Ceph Storage performance tiers, create a new environment file that contains the CRUSH map details and then include it in the deployment command.
		</p><p>
			In the following procedures, each Ceph Storage node contains three OSDs, <code class="literal">sdb</code> and <code class="literal">sdc</code> are spinning disks and <code class="literal">sdc</code> is a SSD. Ceph automatically detects the correct disk type. You then configure two CRUSH rules, HDD and SSD, to map to the two respective device classes. The HDD rule is the default and applies to all pools unless you configure pools with a different rule.
		</p><p>
			Finally, you create an extra pool called <code class="literal">fastpool</code> and map it to the SSD rule. This pool is ultimately exposed through a Block Storage (cinder) back end. Any workload that consumes this Block Storage back end is backed by SSD only for fast performances. You can leverage this for either data or boot from volume.
		</p><section class="section" id="configuring_the_performance_tiers"><div class="titlepage"><div><div><h2 class="title">6.1. Configuring the performance tiers</h2></div></div></div><p>
				Director does not expose specific parameters to cover this feature, however, you can generate the <code class="literal">ceph-ansible</code> expected variables by completing the following steps.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the undercloud node as the <code class="literal">stack</code> user.
					</li><li class="listitem">
						Create an environment file to contain the Ceph config parameters and the device classes variables <code class="literal">/home/stack/templates/ceph-config.yaml</code>. You can also add the following configurations to an existing environment file.
					</li><li class="listitem"><p class="simpara">
						In the environment file, use the <code class="literal">CephAnsibleDisksConfig</code> parameter to list the block devices that you want to use as Ceph OSDs:
					</p><pre class="screen">CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem"><p class="simpara">
						Optional: Ceph autodects the type of disk and assigns it to the corresponding device class. However, you can also use the <code class="literal">crush_device_class</code> property to force a specific device to belong to a specific class or create your own custom classes. The following example contains the same list of OSDs with specified classes:
					</p><pre class="screen">CephAnsibleDisksConfig:
    lvm_volumes:
      - data: '/dev/sdb'
        crush_device_class: 'hdd'
      - data: '/dev/sdc'
        crush_device_class: 'hdd'
      - data: '/dev/sdd'
        crush_device_class: 'ssd'
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">CephAnsibleExtraVars</code> parameters. The <code class="literal">crush_rules</code> parameter must contain a rule for each class that you define or that Ceph detects automatically. When you create a new pool, if no rule is specified, the rule that you want Ceph to use as the default is selected.
					</p><pre class="screen">CephAnsibleExtraConfig:
    crush_rule_config: true
    create_crush_tree: true
    crush_rules:
      - name: HDD
        root: default
        type: host
        class: hdd
        default: true
      - name: SSD
        root: default
        type: host
        class: ssd
        default: false</pre></li><li class="listitem"><p class="simpara">
						Add the <code class="literal">CephPools</code> parameter and use the <code class="literal">rule_name</code> parameter to specify the tier for each pool that does not use the default rule. In the following example, the <code class="literal">fastpool</code> pool uses the SSD device class that is configured as a fast tier, to manage Block Storage volumes. Use the <code class="literal">CinderRbdExtraPools</code> parameter to configure <code class="literal">fastpool</code> as a Block Storage back end. Replace &lt;appropriate_PG_num&gt; with the appropriate number of placement groups. For more information about how to calculate the number of placement groups for Ceph pools, see <a class="xref" href="index.html#custom-ceph-pools" title="5.4. Assigning custom attributes to different Ceph pools">Section 5.4, “Assigning custom attributes to different Ceph pools”</a>.
					</p><pre class="screen">CephPools:
  - name: fastpool
    pg_num: &lt;appropraiate_PG_num&gt;
    rule_name: SSD
    application: rbd
CinderRbdExtraPools: fastpool</pre></li><li class="listitem"><p class="simpara">
						Use the following example to ensure that your environment file contains the correct values:
					</p><pre class="screen">parameter_defaults:
    CephAnsibleDisksConfig:
        devices:
            - '/dev/sdb'
            - '/dev/sdc'
            - '/dev/sdd'
        osd_scenario: lvm
        osd_objectstore: bluestore
    CephAnsibleExtraConfig:
        crush_rule_config: true
        create_crush_tree: true
        crush_rules:
            - name: HDD
              root: default
              type: host
              class: hdd
              default: true
            - name: SSD
              root: default
              type: host
              class: ssd
              default: false
    CinderRbdExtraPools: fastpool
    CephPools:
        - name: fastpool
          pg_num: &lt;appropriate_PG_num&gt;
          rule_name: SSD
          application: rbd</pre></li><li class="listitem"><p class="simpara">
						Include the new environment file in the <code class="literal">openstack overcloud deploy</code> command. Replace <code class="literal">&lt;existing_overcloud_environment_files&gt;</code> with the list of environment files that are part of your existing deployment.
					</p><pre class="screen">$ openstack overcloud deploy \
--templates \
…
-e &lt;existing_overcloud_environment_files&gt; \
-e /home/stack/templates/ceph-config.yaml  \
…</pre></li></ol></div></section><section class="section" id="mapping_a_block_storage_cinder_type_to_your_new_ceph_pool"><div class="titlepage"><div><div><h2 class="title">6.2. Mapping a Block Storage (cinder) type to your new Ceph pool</h2></div></div></div><p>
				After you complete the configuration steps, make the performance tiers feature available to RHOSP tenants by using Block Storage (cinder) to create a type that is mapped to the <code class="literal">fastpool</code> tier that you created.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the undercloud node as the <code class="literal">stack</code> user.
					</li><li class="listitem"><p class="simpara">
						Source the <code class="literal">overcloudrc</code> file:
					</p><pre class="screen">$ source overcloudrc</pre></li><li class="listitem"><p class="simpara">
						Check the Block Storage volume existing types:
					</p><pre class="screen">$ cinder type-list</pre></li><li class="listitem"><p class="simpara">
						Create the new Block Storage volume fast_tier:
					</p><pre class="screen">$ cinder type-create fast_tier</pre></li><li class="listitem"><p class="simpara">
						Check that the Block Storage type is created:
					</p><pre class="screen">$ cinder type-list</pre></li><li class="listitem"><p class="simpara">
						When the <code class="literal">fast_tier</code> Block Storage type is available, set the <code class="literal">fastpool</code> as the Block Storage volume back end for the new tier that you created:
					</p><pre class="screen">$ cinder type-key fast_tier set volume_backend_name=tripleo_ceph_fastpool</pre></li><li class="listitem"><p class="simpara">
						Use the new tier to create new volumes:
					</p><pre class="screen">$ cinder create 1 --volume-type fast_tier --name fastdisk</pre></li></ol></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					If you apply the environment file to an existing Ceph cluster, the pre-existing Ceph pools are not updated with the new rules. For this reason, you must enter the following command after the deployment completes to set the rules to the specified pools.
				</p><pre class="screen">$ ceph osd pool set &lt;pool&gt; crush_rule &lt;rule&gt;</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Replace &lt;pool&gt; with the name of the pool that you want to apply the new rule to.
						</li><li class="listitem">
							Replace &lt;rule&gt; with one of the rule names that you specified with the <code class="literal">crush_rules</code> parameter.
						</li><li class="listitem">
							Replace &lt;appropriate_PG_num&gt; with the appropriate number of placement groups.
						</li></ul></div><p>
					For every rule that you change with this command, update the existing entry or add a new entry in the <code class="literal">CephPools</code> parameter in your existing templates:
				</p><pre class="screen">CephPools:
    - name: &lt;pool&gt;
      pg_num: &lt;appropriate_PG_num&gt;
      rule_name: &lt;rule&gt;
      application: rbd</pre></div></div></section><section class="section" id="verifying_that_the_crush_rules_are_created_and_that_your_pools_are_set_to_the_correct_crush_rule"><div class="titlepage"><div><div><h2 class="title">6.3. Verifying that the CRUSH rules are created and that your pools are set to the correct CRUSH rule</h2></div></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the overcloud Controller node as the <code class="literal">heat-admin</code> user.
					</li><li class="listitem"><p class="simpara">
						To verify that your OSD tiers are successfully set, enter the following command. Replace &lt;controller_hostname&gt; with the name of your host Controller node.
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-&lt;controller_hostname&gt; ceph osd tree</pre></li><li class="listitem">
						In the resulting tree view, verify that the <span class="strong strong"><strong>CLASS</strong></span> column displays the correct device class for each OSD that you set.
					</li><li class="listitem"><p class="simpara">
						Also verify that the OSDs are properly assigned to the device classes with following command. Replace &lt;controller_hostname&gt; with the name of your host Controller node.
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-&lt;controller hostname&gt; ceph osd crush tree --show-shadow</pre></li><li class="listitem"><p class="simpara">
						Compare the resulting hierarchy with the results of the following command to ensure that the same values apply for each rule.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace &lt;controller_hostname&gt; with the name of your host Controller node.
							</li><li class="listitem"><p class="simpara">
								Replace &lt;rule_name&gt; with the name of the rule you want to check.
							</p><pre class="screen">$ sudo podman exec &lt;controller hostname&gt; ceph osd crush rule dump &lt;rule_name&gt;</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						Verify that the rules name and ID that you created are correct according to the <code class="literal">crush_rules</code> parameter that you used during deployment. Replace &lt;controller_hostname&gt; with the name of your host Controller node.
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-&lt;controller_hostname&gt;  ceph osd crush rule dump | grep -E "rule_(id|name)"</pre></li><li class="listitem"><p class="simpara">
						Verify that the Ceph pools are tied to the correct CRUSH rule ID that you retrieved in Step 3. Replace &lt;controller_hostname&gt; with the name of your host Controller node.
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-&lt;controller_hostname&gt; ceph osd dump | grep pool</pre></li><li class="listitem">
						For each pool, ensure that the rule ID matches the rule name that you expect.
					</li></ol></div></section></section><section class="chapter" id="creating_the_overcloud"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Creating the overcloud</h1></div></div></div><p>
			When your custom environment files are ready, you can specify the flavors and nodes that each role uses and then execute the deployment. The following subsections explain both steps in greater detail.
		</p><section class="section" id="node-assignments"><div class="titlepage"><div><div><h2 class="title">7.1. Assigning nodes and flavors to roles</h2></div></div></div><p>
				Planning an overcloud deployment involves specifying how many nodes and which flavors to assign to each role. Like all Heat template parameters, these role specifications are declared in the <code class="literal">parameter_defaults</code> section of your environment file (in this case, <code class="literal">~/templates/storage-config.yaml</code>).
			</p><p>
				For this purpose, use the following parameters:
			</p><div class="table" id="idm140342048303392"><p class="title"><strong>Table 7.1. Roles and Flavors for Overcloud Nodes</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 50%; " class="col_1"><!--Empty--></col><col style="width: 50%; " class="col_2"><!--Empty--></col></colgroup><thead><tr><th align="left" valign="top" id="idm140342048298592" scope="col">Heat Template Parameter</th><th align="left" valign="top" id="idm140342048297504" scope="col">Description</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								ControllerCount
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The number of Controller nodes to scale out
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								OvercloudControlFlavor
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The flavor to use for Controller nodes (<code class="literal">control</code>)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								ComputeCount
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The number of Compute nodes to scale out
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								OvercloudComputeFlavor
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The flavor to use for Compute nodes (<code class="literal">compute</code>)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								CephStorageCount
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The number of Ceph storage (OSD) nodes to scale out
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								OvercloudCephStorageFlavor
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The flavor to use for Ceph Storage (OSD) nodes (<code class="literal">ceph-storage</code>)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								CephMonCount
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The number of dedicated Ceph MON nodes to scale out
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								OvercloudCephMonFlavor
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The flavor to use for dedicated Ceph MON nodes (<code class="literal">ceph-mon</code>)
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								CephMdsCount
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The number of dedicated Ceph MDS nodes to scale out
							</p>
							 </td></tr><tr><td align="left" valign="top" headers="idm140342048298592"> <p>
								OvercloudCephMdsFlavor
							</p>
							 </td><td align="left" valign="top" headers="idm140342048297504"> <p>
								The flavor to use for dedicated Ceph MDS nodes (<code class="literal">ceph-mds</code>)
							</p>
							 </td></tr></tbody></table></div></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The <code class="literal">CephMonCount</code>, <code class="literal">CephMdsCount</code>, <code class="literal">OvercloudCephMonFlavor</code>, and <code class="literal">OvercloudCephMdsFlavor</code> parameters (along with the <code class="literal">ceph-mon</code> and <code class="literal">ceph-mds</code> flavors) will only be valid if you created a custom <code class="literal">CephMON</code> and <code class="literal">CephMds</code> role, as described in <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a>.
				</p></div></div><p>
				For example, to configure the overcloud to deploy three nodes for each role (Controller, Compute, Ceph-Storage, and CephMon), add the following to your <code class="literal">parameter_defaults</code>:
			</p><pre class="literallayout">parameter_defaults:
  ControllerCount: 3
  OvercloudControlFlavor: control
  ComputeCount: 3
  OvercloudComputeFlavor: compute
  CephStorageCount: 3
  OvercloudCephStorageFlavor: ceph-storage
  CephMonCount: 3
  OvercloudCephMonFlavor: ceph-mon
  CephMdsCount: 3
  OvercloudCephMdsFlavor: ceph-mds</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/#sect-Creating_the_Overcloud_CLI">Creating the Overcloud with the CLI Tools</a> from the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/">Director Installation and Usage</a> guide for a more complete list of Heat template parameters.
				</p></div></div></section><section class="section" id="sect-Creating_the_Overcloud"><div class="titlepage"><div><div><h2 class="title">7.2. Initiating overcloud deployment</h2></div></div></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					During undercloud installation, set <code class="literal">generate_service_certificate=false</code> in the <code class="literal">undercloud.conf</code> file. Otherwise, you must inject a trust anchor when you deploy the overcloud, as described in <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/sect-enabling_ssltls_on_the_overcloud#sect-Enabling_SSLTLS_on_the_Overcloud">Enabling SSL/TLS on Overcloud Public Endpoints</a> in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide.
				</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Note</span></dt><dd>
							If you want to add Ceph Dashboard during your overcloud deployment, see <a class="xref" href="index.html#adding-ceph-dashboard" title="Chapter 8. Adding the Red Hat Ceph Storage Dashboard to an overcloud deployment">Chapter 8, <em>Adding the Red Hat Ceph Storage Dashboard to an overcloud deployment</em></a>.
						</dd></dl></div><p>
				The creation of the overcloud requires additional arguments for the <code class="literal">openstack overcloud deploy</code> command. For example:
			</p><pre class="screen">$ openstack overcloud deploy --templates -r /home/stack/templates/roles_data_custom.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-mds.yaml
  -e /usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml \
  -e /home/stack/templates/storage-config.yaml \
  -e /home/stack/templates/ceph-config.yaml \
  --ntp-server pool.ntp.org</pre><p>
				The above command uses the following options:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						<code class="literal">--templates</code> - Creates the Overcloud from the default Heat template collection (namely, <code class="literal">/usr/share/openstack-tripleo-heat-templates/</code>).
					</li><li class="listitem">
						<code class="literal">-r /home/stack/templates/roles_data_custom.yaml</code> - Specifies the customized roles definition file from <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a>, which adds custom roles for either Ceph MON or Ceph MDS services. These roles allow either service to be installed on dedicated nodes.
					</li><li class="listitem">
						<code class="literal">-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml</code> - Sets the director to create a Ceph cluster. In particular, this environment file will deploy a Ceph cluster with <span class="emphasis"><em>containerized</em></span> Ceph Storage nodes.
					</li><li class="listitem">
						<code class="literal">-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml</code> - Enables the Ceph Object Gateway, as described in <a class="xref" href="index.html#ceph-rgw" title="4.2. Enabling the Ceph Object Gateway">Section 4.2, “Enabling the Ceph Object Gateway”</a>.
					</li><li class="listitem">
						<code class="literal">-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-mds.yaml</code> - Enables the Ceph Metadata Server, as described in <a class="xref" href="index.html#ceph-mds" title="4.1. Enabling the Ceph Metadata Server">Section 4.1, “Enabling the Ceph Metadata Server”</a>.
					</li><li class="listitem">
						<code class="literal">-e /usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml</code> - Enables the Block Storage Backup service (<code class="literal">cinder-backup</code>), as described in <a class="xref" href="index.html#cinder-backup-ceph" title="4.4. Configuring the Backup Service to use Ceph">Section 4.4, “Configuring the Backup Service to use Ceph”</a>.
					</li><li class="listitem">
						<code class="literal">-e /home/stack/templates/storage-config.yaml</code> - Adds the environment file containing your custom Ceph Storage configuration.
					</li><li class="listitem">
						<code class="literal">-e /home/stack/templates/ceph-config.yaml</code> - Adds the environment file containing your custom Ceph cluster settings, as described in <a class="xref" href="index.html#Configuring_Ceph_Storage_Cluster_Settings" title="Chapter 5. Customizing the Ceph Storage cluster">Chapter 5, <em>Customizing the Ceph Storage cluster</em></a>.
					</li><li class="listitem">
						<code class="literal">--ntp-server pool.ntp.org</code> - Sets our NTP server.
					</li></ul></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
				You can also use an <span class="emphasis"><em>answers file</em></span> to invoke all your templates and environment files. For example, you can use the following command to deploy an identical overcloud:
			</p><pre class="screen">$ openstack overcloud deploy -r /home/stack/templates/roles_data_custom.yaml \
  --answers-file /home/stack/templates/answers.yaml --ntp-server pool.ntp.org</pre><p>
				In this case, the answers file <code class="literal">/home/stack/templates/answers.yaml</code> contains:
			</p><pre class="screen">templates: /usr/share/openstack-tripleo-heat-templates/
environments:
  - /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml
  - /usr/share/openstack-tripleo-heat-templates/environments/ceph-rgw.yaml
  - /usr/share/openstack-tripleo-heat-templates/environments/ceph-mds.yaml
  - /usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml
  - /home/stack/templates/storage-config.yaml
  - /home/stack/templates/ceph-config.yaml</pre><p>
				See <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/#sect-Including_Environment_Files_in_Overcloud_Creation">Including environment files in an overcloud deployment</a> for more details.
			</p></div></div><p>
				For a full list of options, run:
			</p><pre class="screen">$ openstack help overcloud deploy</pre><p>
				For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#creating-a-basic-overcloud-with-cli-tools">Configuring a basic overcloud with the CLI tools</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
			</p><p>
				The overcloud creation process begins and the director provisions your nodes. This process takes some time to complete. To view the status of the Overcloud creation, open a separate terminal as the <code class="literal">stack</code> user and run:
			</p><pre class="screen">$ source ~/stackrc
$ openstack stack list --nested</pre></section></section><section class="chapter" id="adding-ceph-dashboard"><div class="titlepage"><div><div><h1 class="title">Chapter 8. Adding the Red Hat Ceph Storage Dashboard to an overcloud deployment</h1></div></div></div><p>
			Red Hat Ceph Storage Dashboard is disabled by default but you can now enable it in your overcloud with the Red Hat OpenStack Platform director. The Ceph Dashboard is a built-in, web-based Ceph management and monitoring application to administer various aspects and objects in your cluster. Red Hat Ceph Storage Dashboard comprises the Ceph Dashboard manager module, which provides the user interface and embeds Grafana, the front end of the platform, Prometheus as a monitoring plugin, Alertmanager and Node Exporters that are deployed throughout the cluster and send alerts and export cluster data to the Dashboard.
		</p><div class="variablelist" id="adding-ceph-dashboard-to-an-overcloud"><dl class="variablelist"><dt><span class="term">Note</span></dt><dd>
						This feature is supported with Ceph Storage 4.1 or later. For more information about how to determine the version of Ceph Storage installed on your system, see <a class="link" href="https://access.redhat.com/solutions/2045583">Red Hat Ceph Storage releases and corresponding Ceph package versions</a>.
					</dd><dt><span class="term">Note</span></dt><dd>
						The Red Hat Ceph Storage Dashboard is always colocated on the same nodes as the other Ceph manager components.
					</dd><dt><span class="term">Note</span></dt><dd>
						If you want to add Ceph Dashboard during your initial overcloud deployment, complete the procedures in this chapter before you deploy your initial overcloud in <a class="xref" href="index.html#sect-Creating_the_Overcloud" title="7.2. Initiating overcloud deployment">Section 7.2, “Initiating overcloud deployment”</a>.
					</dd></dl></div><p>
			The following diagram shows the architecture of Ceph Dashboard on Red Hat OpenStack Platform:
		</p><div class="informalfigure"><div class="mediaobject"><img src="89_Ceph_Dashboard_on_OpenStack_0520.png" alt="Ceph Dashboard on Red Hat OpenStack Platform"/></div></div><p>
			For more information about the Dashboard and its features and limitations, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/dashboard_guide/index#dashboard-features_dash">Dashboard features</a> in the <span class="emphasis"><em>Red Hat Ceph Storage Dashboard Guide</em></span>.
		</p><div class="formalpara"><p class="title"><strong>TLS everywhere with Ceph Dashboard</strong></p><p>
				The Dashboard front end is fully integrated with the TLS everywhere framework. You can enable TLS everywhere provided that you have the required environment files and they are included in the overcloud deploy command. This triggers the certificate request for both Grafana and the Ceph Dashboard and the generated certificate and key files are passed to <code class="literal">ceph-ansible</code> during the overcloud deployment. For instructions and more information about how to enable TLS for the Dashboard as well as for other openstack services, see the following locations in the <span class="emphasis"><em>Advanced Overcloud Customization</em></span> guide:
			</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization/index#sect-Enabling_SSLTLS_on_the_Overcloud">Enabling SSL/TLS on Overcloud Public Endpoints</a>.
				</li><li class="listitem"><p class="simpara">
					<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/advanced_overcloud_customization/sect-enabling_internal_ssltls_on_the_overcloud">Enabling SSL/TLS on Internal and Public Endpoints with Identity Management</a>.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Note</span></dt><dd>
								The port to reach the Ceph Dashboard remains the same even in the TLS-everywhere context.
							</dd></dl></div></li></ul></div><section class="section" id="proc_including-ceph-dashboard-containers"><div class="titlepage"><div><div><h2 class="title">8.1. Including the necessary containers for the Ceph Dashboard</h2></div></div></div><p>
				Before you can add the Ceph Dashboard templates to your overcloud, you must include the necessary containers by using the <code class="literal">containers-prepare-parameter.yaml</code> file. To generate the <code class="literal">containers-prepare-parameter.yaml</code> file to prepare your container images, complete the following steps:
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to your undercloud host as the <code class="literal">stack</code> user.
					</li><li class="listitem"><p class="simpara">
						Generate the default container image preparation file:
					</p><pre class="screen">$ openstack tripleo container image prepare default \
  --local-push-destination \
  --output-env-file containers-prepare-parameter.yaml</pre></li><li class="listitem"><p class="simpara">
						Edit the <code class="literal">containers-prepare-parameter.yaml</code> file and make the modifications to suit your requirements. The following example <code class="literal">containers-prepare-parameter.yaml</code> file contains the image locations and tags related to the Dashboard services including Grafana, Prometheus, Alertmanager, and Node Exporter. Edit the values depending on your specific scenario:
					</p><pre class="screen">parameter_defaults:
    ContainerImagePrepare:
    - push_destination: true
        set:
            ceph_alertmanager_image: ose-prometheus-alertmanager
            ceph_alertmanager_namespace: registry.redhat.io/openshift4
            ceph_alertmanager_tag: v4.1
            ceph_grafana_image: rhceph-4-dashboard-rhel8
            ceph_grafana_namespace: registry.redhat.io/rhceph
            ceph_grafana_tag: 4
            ceph_image: rhceph-4-rhel8
            ceph_namespace: registry.redhat.io/rhceph
            ceph_node_exporter_image: ose-prometheus-node-exporter
            ceph_node_exporter_namespace: registry.redhat.io/openshift4
            ceph_node_exporter_tag: v4.1
            ceph_prometheus_image: ose-prometheus
            ceph_prometheus_namespace: registry.redhat.io/openshift4
            ceph_prometheus_tag: v4.1
            ceph_tag: latest</pre></li></ol></div><p>
				For more information about registry and image configuration with the <code class="literal">containers-prepare-parameter.yaml</code> file, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/transitioning_to_containerized_services/obtaining-container-images#container-image-preparation-parameters">Container image preparation parameters</a> in the <span class="emphasis"><em>Transitioning to Containerized Services</em></span> guide.
			</p></section><section class="section" id="proc_deploying-ceph-dashboard"><div class="titlepage"><div><div><h2 class="title">8.2. Deploying Ceph Dashboard</h2></div></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term">Note</span></dt><dd>
							The Ceph Dashboard admin user role is set to read-only mode by default. To change the Ceph Dashboard admin default mode, see <a class="xref" href="index.html#proc_changing-the-default-permissions" title="8.3. Changing the default permissions">Section 8.3, “Changing the default permissions”</a>.
						</dd></dl></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the undercloud node as the <code class="literal">stack</code> user.
					</li><li class="listitem"><p class="simpara">
						Include the following environment files, with all environment files that are part of your existing deployment, in the <code class="literal">openstack overcloud deploy</code> command:
					</p><pre class="screen">$ openstack overcloud deploy \
  --templates \
  -e &lt;existing_overcloud_environment_files&gt; \
  -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \
  -e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-dashboard.yaml</pre><p class="simpara">
						Replace <code class="literal">&lt;existing_overcloud_environment_files&gt;</code> with the list of environment files that are part of your existing deployment.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Result</span></dt><dd>
									The resulting deployment comprises an external stack with the grafana, prometheus, alertmanager, and node-exporter containers. The Ceph Dashboard manager module is the back end for this stack and embeds the grafana layouts to provide ceph cluster specific metrics to the end users.
								</dd></dl></div></li></ol></div></section><section class="section" id="proc_changing-the-default-permissions"><div class="titlepage"><div><div><h2 class="title">8.3. Changing the default permissions</h2></div></div></div><p>
				The Ceph Dashboard admin user role is set to read-only mode by default for safe monitoring of the Ceph cluster. To permit an admin user to have elevated privileges so that they can alter elements of the Ceph cluster with the Dashboard, you can use the <code class="literal">CephDashboardAdminRO</code> parameter to change the default admin permissions.
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Warning</span></dt><dd>
							A user with full permissions might alter elements of your cluster that director configures. This can cause a conflict with director-configured options when you run a stack update. To avoid this problem, do not alter director-configured options with Ceph Dashboard, for example, Ceph OSP pools attributes.
						</dd></dl></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the undercloud as the <code class="literal">stack</code> user.
					</li><li class="listitem"><p class="simpara">
						Create the following <code class="literal">ceph_dashboard_admin.yaml</code> environment file:
					</p><pre class="screen">  parameter_defaults:
     CephDashboardAdminRO: false</pre></li><li class="listitem"><p class="simpara">
						Run the overcloud deploy command to update the existing stack and include the environment file you created with all other environment files that are part of your existing deployment:
					</p><pre class="screen">$ openstack overcloud deploy \
--templates \
-e &lt;existing_overcloud_environment_files&gt; \
-e ceph_dashboard_admin.yml</pre><p class="simpara">
						Replace <code class="literal">&lt;existing_overcloud_environment_files&gt;</code> with the list of environment files that are part of your existing deployment.
					</p></li></ol></div></section><section class="section" id="proc_accessing-ceph-dashboard"><div class="titlepage"><div><div><h2 class="title">8.4. Accessing Ceph Dashboard</h2></div></div></div><p>
				To test that Ceph Dashboard is running correctly, complete the following verification steps to access it and check that the data it displays from the Ceph cluster is correct.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to the undercloud node as the <code class="literal">stack</code> user.
					</li><li class="listitem"><p class="simpara">
						Retrieve the dashboard admin login credentials:
					</p><pre class="screen">[stack@undercloud ~]$ grep dashboard_admin_password /var/lib/mistral/overcloud/ceph-ansible/group_vars/all.yml</pre></li><li class="listitem"><p class="simpara">
						Retrieve the VIP address to access the Ceph Dashboard:
					</p><pre class="screen">[stack@undercloud-0 ~]$ grep dashboard_frontend /var/lib/mistral/overcloud/ceph-ansible/group_vars/mgrs.yml</pre></li><li class="listitem"><p class="simpara">
						Use a web browser to point to the front end VIP and access the Dashboard. Director configures and exposes the Dashboard on the provisioning network, so you can use the VIP that you retrieved in step 2 to access the dashboard directly on TCP port 8444. Ensure that the following conditions are met:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The Web client host is layer 2 connected to the provisioning network.
							</li><li class="listitem"><p class="simpara">
								The provisioning network is properly routed or proxied, and it can be reached from the web client host. If these conditions are not met, you can still open a SSH tunnel to reach the Dashboard VIP on the overcloud:
							</p><pre class="screen">client_host$ ssh -L 8444:&lt;dashboard vip&gt;:8444 stack@&lt;your undercloud&gt;</pre><p class="simpara">
								Replace &lt;dashboard vip&gt; with the IP address of the control plane VIP that you retrieved in step 3.
							</p></li></ul></div></li><li class="listitem"><p class="simpara">
						Access the Dashboard by pointing your web browser to <a class="link" href="http://localhost:8444">http://localhost:8444</a>. The default user that <code class="literal">ceph-ansible</code> creates is admin. You can retrieve the password in <code class="literal">/var/lib/mistral/overcloud/ceph-ansible/group_vars/all.yml</code>.
					</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Results</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											You can access the Ceph Dashboard.
										</li><li class="listitem">
											The numbers and graphs that the Dashboard displays reflect the same cluster status that the CLI command, <code class="literal">ceph -s</code>, returns.
										</li></ul></div></dd></dl></div></li></ol></div><p>
				For more information about the Red Hat Ceph Storage Dashboard, see the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/dashboard_guide/index"><span class="emphasis"><em>Red Hat Ceph Storage Administration Guide</em></span></a>
			</p></section></section><section class="chapter" id="post-deploy"><div class="titlepage"><div><div><h1 class="title">Chapter 9. Post-deployment</h1></div></div></div><p>
			The following subsections describe several post-deployment operations for managing the Ceph cluster.
		</p><section class="section" id="accessing_the_overcloud"><div class="titlepage"><div><div><h2 class="title">9.1. Accessing the overcloud</h2></div></div></div><p>
				The director generates a script to configure and help authenticate interactions with your overcloud from the undercloud. The director saves this file (<code class="literal">overcloudrc</code>) in your <code class="literal">stack</code> user’s home directory. Run the following command to use this file:
			</p><pre class="screen">$ source ~/overcloudrc</pre><p>
				This loads the necessary environment variables to interact with your overcloud from the undercloud CLI. To return to interacting with the undercloud, run the following command:
			</p><pre class="screen">$ source ~/stackrc</pre></section><section class="section" id="monitoring_ceph_storage_nodes"><div class="titlepage"><div><div><h2 class="title">9.2. Monitoring Ceph Storage nodes</h2></div></div></div><p>
				After you create the overcloud, check the status of the Ceph Storage Cluster to ensure that it works correctly.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Log in to a Controller node as the <code class="literal">heat-admin</code> user:
					</p><pre class="screen">$ nova list
$ ssh heat-admin@192.168.0.25</pre></li><li class="listitem"><p class="simpara">
						Check the health of the cluster:
					</p><pre class="screen">$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph health</pre><p class="simpara">
						If the cluster has no issues, the command reports back <code class="literal">HEALTH_OK</code>. This means the cluster is safe to use.
					</p></li><li class="listitem"><p class="simpara">
						Log in to an overcloud node that runs the Ceph monitor service and check the status of all OSDs in the cluster:
					</p><pre class="screen">$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd tree</pre></li><li class="listitem"><p class="simpara">
						Check the status of the Ceph Monitor quorum:
					</p><pre class="screen">$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph quorum_status</pre><p class="simpara">
						This shows the monitors participating in the quorum and which one is the leader.
					</p></li><li class="listitem"><p class="simpara">
						Verify that all Ceph OSDs are running:
					</p><pre class="screen">$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd stat</pre></li></ol></div><p>
				For more information on monitoring Ceph Storage clusters, see <a class="link" href="https://access.redhat.com/documentation/en/red-hat-ceph-storage/1.3/administration-guide/chapter-3-monitoring">Monitoring</a> in the <span class="emphasis"><em>Red Hat Ceph Storage Administration Guide</em></span>.
			</p></section></section><section class="chapter" id="rebooting_the_environment"><div class="titlepage"><div><div><h1 class="title">Chapter 10. Rebooting the environment</h1></div></div></div><p>
			A situation might occur where you need to reboot the environment. For example, when you might need to modify the physical servers, or you might need to recover from a power outage. In this situation, it is important to make sure your Ceph Storage nodes boot correctly.
		</p><p>
			Make sure to boot the nodes in the following order:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					<span class="strong strong"><strong>Boot all Ceph Monitor nodes first</strong></span> - This ensures the Ceph Monitor service is active in your high availability cluster. By default, the Ceph Monitor service is installed on the Controller node. If the Ceph Monitor is separate from the Controller in a custom role, make sure this custom Ceph Monitor role is active.
				</li><li class="listitem">
					<span class="strong strong"><strong>Boot all Ceph Storage nodes</strong></span> - This ensures the Ceph OSD cluster can connect to the active Ceph Monitor cluster on the Controller nodes.
				</li></ul></div><section class="section" id="rebooting_a_ceph_storage_cluster"><div class="titlepage"><div><div><h2 class="title">10.1. Rebooting a Ceph Storage (OSD) cluster</h2></div></div></div><p>
				Complete the following steps to reboot a cluster of Ceph Storage (OSD) nodes.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Log into a Ceph MON or Controller node and disable Ceph Storage cluster rebalancing temporarily:
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph osd set noout
$ sudo podman exec -it ceph-mon-controller-0 ceph osd set norebalance</pre></li><li class="listitem">
						Select the first Ceph Storage node that you want to reboot and log in to the node.
					</li><li class="listitem"><p class="simpara">
						Reboot the node:
					</p><pre class="screen">$ sudo reboot</pre></li><li class="listitem">
						Wait until the node boots.
					</li><li class="listitem"><p class="simpara">
						Log into the node and check the cluster status:
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph status</pre><p class="simpara">
						Check that the <code class="literal">pgmap</code> reports all <code class="literal">pgs</code> as normal (<code class="literal">active+clean</code>).
					</p></li><li class="listitem">
						Log out of the node, reboot the next node, and check its status. Repeat this process until you have rebooted all Ceph storage nodes.
					</li><li class="listitem"><p class="simpara">
						When complete, log into a Ceph MON or Controller node and re-enable cluster rebalancing:
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph osd unset noout
$ sudo podman exec -it ceph-mon-controller-0 ceph osd unset norebalance</pre></li><li class="listitem"><p class="simpara">
						Perform a final status check to verify that the cluster reports <code class="literal">HEALTH_OK</code>:
					</p><pre class="screen">$ sudo podman exec -it ceph-mon-controller-0 ceph status</pre></li></ol></div><p>
				If a situation occurs where all overcloud nodes boot at the same time, the Ceph OSD services might not start correctly on the Ceph Storage nodes. In this situation, reboot the Ceph Storage OSDs so they can connect to the Ceph Monitor service.
			</p><p>
				Verify a <code class="literal">HEALTH_OK</code> status of the Ceph Storage node cluster with the following command:
			</p><pre class="screen">$ sudo ceph status</pre></section></section><section class="chapter" id="scaling_the_ceph_storage_cluster"><div class="titlepage"><div><div><h1 class="title">Chapter 11. Scaling the Ceph Storage cluster</h1></div></div></div><section class="section" id="scaling_up"><div class="titlepage"><div><div><h2 class="title">11.1. Scaling up the Ceph Storage cluster</h2></div></div></div><p>
				You can scale up the number of Ceph Storage nodes in your overcloud by re-running the deployment with the number of Ceph Storage nodes you need.
			</p><p>
				Before doing so, ensure that you have enough nodes for the updated deployment. These nodes must be registered with the director and tagged accordingly.
			</p><div class="formalpara"><p class="title"><strong>Registering New Ceph Storage Nodes</strong></p><p>
					To register new Ceph storage nodes with the director, follow these steps:
				</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Log in to the undercloud as the <code class="literal">stack</code> user and initialize your director configuration:
					</p><pre class="screen">$ source ~/stackrc</pre></li><li class="listitem">
						Define the hardware and power management details for the new nodes in a new node definition template; for example, <code class="literal">instackenv-scale.json</code>.
					</li><li class="listitem"><p class="simpara">
						Import this file to the OpenStack director:
					</p><pre class="screen">$ openstack overcloud node import ~/instackenv-scale.json</pre><p class="simpara">
						Importing the node definition template registers each node defined there to the director.
					</p></li><li class="listitem"><p class="simpara">
						Assign the kernel and ramdisk images to all nodes:
					</p><pre class="screen">$ openstack overcloud node configure</pre></li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					For more information about registering new nodes, see <a class="xref" href="index.html#register-nodes" title="2.2. Registering nodes">Section 2.2, “Registering nodes”</a>.
				</p></div></div><div class="formalpara"><p class="title"><strong>Manually Tagging New Nodes</strong></p><p>
					After you register each node, you must inspect the hardware and tag the node into a specific profile. Use profile tags to match your nodes to flavors, and then assign flavors to deployment roles.
				</p></div><p>
				To inspect and tag new nodes, complete the following steps:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Trigger hardware introspection to retrieve the hardware attributes of each node:
					</p><pre class="screen">$ openstack overcloud node introspect --all-manageable --provide</pre><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The <code class="literal">--all-manageable</code> option introspects only the nodes that are in a managed state. In this example, all nodes are in a managed state.
							</li><li class="listitem"><p class="simpara">
								The <code class="literal">--provide</code> option resets all nodes to an <code class="literal">active</code> state after introspection.
							</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
									Ensure that this process completes successfully. This process usually takes 15 minutes for bare metal nodes.
								</p></div></div></li></ul></div></li><li class="listitem"><p class="simpara">
						Retrieve a list of your nodes to identify their UUIDs:
					</p><pre class="screen">$ openstack baremetal node list</pre></li><li class="listitem"><p class="simpara">
						Add a profile option to the <code class="literal">properties/capabilities</code> parameter for each node to manually tag a node to a specific profile. The addition of the <code class="literal">profile</code> option tags the nodes into each respective profile.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							As an alternative to manual tagging, use the Automated Health Check (AHC) Tools to automatically tag larger numbers of nodes based on benchmarking data.
						</p></div></div><p class="simpara">
						For example, the following commands tag three additional nodes with the <code class="literal">ceph-storage</code> profile:
					</p><pre class="screen">$ ironic node-update 551d81f5-4df2-4e0f-93da-6c5de0b868f7 add properties/capabilities='profile:ceph-storage,boot_option:local'
$ ironic node-update 5e735154-bd6b-42dd-9cc2-b6195c4196d7 add properties/capabilities='profile:ceph-storage,boot_option:local'
$ ironic node-update 1a2b090c-299d-4c20-a25d-57dd21a7085b add properties/capabilities='profile:ceph-storage,boot_option:local'</pre></li></ol></div><div class="admonition tip"><div class="admonition_header">Tip</div><div><p>
				If the nodes you just tagged and registered use multiple disks, you can set the director to use a specific root disk on each node. See <a class="xref" href="index.html#defining-the-root-disk" title="2.5. Defining the root disk for multi-disk clusters">Section 2.5, “Defining the root disk for multi-disk clusters”</a> for instructions on how to do so.
			</p></div></div><div class="formalpara"><p class="title"><strong>Re-deploying the Overcloud with Additional Ceph Storage Nodes</strong></p><p>
					After registering and tagging the new nodes, you can now scale up the number of Ceph Storage nodes by re-deploying the overcloud. When you do, set the <code class="literal">CephStorageCount</code> parameter in the <code class="literal">parameter_defaults</code> of your environment file (in this case, <code class="literal">~/templates/storage-config.yaml</code>). In <a class="xref" href="index.html#node-assignments" title="7.1. Assigning nodes and flavors to roles">Section 7.1, “Assigning nodes and flavors to roles”</a>, the overcloud is configured to deploy with 3 Ceph Storage nodes. To scale it up to 6 nodes instead, use:
				</p></div><pre class="literallayout">parameter_defaults:
  ControllerCount: 3
  OvercloudControlFlavor: control
  ComputeCount: 3
  OvercloudComputeFlavor: compute
  <span class="strong strong"><strong>CephStorageCount: 6</strong></span>
  OvercloudCephStorageFlavor: ceph-storage
  CephMonCount: 3
  OvercloudCephMonFlavor: ceph-mon</pre><p>
				Upon re-deployment with this setting, the overcloud should now have 6 Ceph Storage nodes instead of 3.
			</p></section><section class="section" id="Replacing_Ceph_Storage_Nodes"><div class="titlepage"><div><div><h2 class="title">11.2. Scaling down and replacing Ceph Storage nodes</h2></div></div></div><p>
				In some cases, you might need to scale down your Ceph cluster, or even replace a Ceph Storage node, for example, if a Ceph Storage node is faulty. In either situation, you must disable and rebalance any Ceph Storage node that you want to remove from the overcloud to avoid data loss.
			</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This procedure uses steps from the <span class="emphasis"><em>Red Hat Ceph Storage Administration Guide</em></span> to manually remove Ceph Storage nodes. For more in-depth information about manual removal of Ceph Storage nodes, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/administration_guide/index#starting-stopping-and-restarting-ceph-daemons-that-run-in-containers_admin">Starting, stopping, and restarting Ceph daemons that run in containers</a> and <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html-single/operations_guide/index#removing-a-ceph-monitor-using-the-command-line-interface-ops">Removing a Ceph OSD using the command-line interface</a>.
				</p></div></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to a Controller node as the <code class="literal">heat-admin</code> user. The director’s <code class="literal">stack</code> user has an SSH key to access the <code class="literal">heat-admin</code> user.
					</li><li class="listitem"><p class="simpara">
						List the OSD tree and find the OSDs for your node. For example, the node you want to remove might contain the following OSDs:
					</p><pre class="screen">-2 0.09998     host overcloud-cephstorage-0
0 0.04999         osd.0                         up  1.00000          1.00000
1 0.04999         osd.1                         up  1.00000          1.00000</pre></li><li class="listitem"><p class="simpara">
						Disable the OSDs on the Ceph Storage node. In this case, the OSD IDs are 0 and 1.
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd out 0
[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd out 1</pre></li><li class="listitem"><p class="simpara">
						The Ceph Storage cluster begins rebalancing. Wait for this process to complete. Follow the status by using the following command:
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph -w</pre></li><li class="listitem"><p class="simpara">
						After the Ceph cluster completes rebalancing, log in to the Ceph Storage node you are removing, in this case <code class="literal">overcloud-cephstorage-0</code>, as the <code class="literal">heat-admin</code> user and stop the node.
					</p><pre class="screen">[heat-admin@overcloud-cephstorage-0 ~]$ sudo systemctl disable ceph-osd@0
[heat-admin@overcloud-cephstorage-0 ~]$ sudo systemctl disable ceph-osd@1</pre></li><li class="listitem"><p class="simpara">
						Stop the OSDs.
					</p><pre class="screen">[heat-admin@overcloud-cephstorage-0 ~]$ sudo systemctl stop ceph-osd@0
[heat-admin@overcloud-cephstorage-0 ~]$ sudo systemctl stop ceph-osd@1</pre></li><li class="listitem"><p class="simpara">
						While logged in to the Controller node, remove the OSDs from the CRUSH map so that they no longer receive data.
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd crush remove osd.0
[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd crush remove osd.1</pre></li><li class="listitem"><p class="simpara">
						Remove the OSD authentication key.
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph auth del osd.0
[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph auth del osd.1</pre></li><li class="listitem"><p class="simpara">
						Remove the OSD from the cluster.
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd rm 0
[heat-admin@overcloud-controller-0 ~]$ sudo podman exec ceph-mon-&lt;HOSTNAME&gt; ceph osd rm 1</pre></li><li class="listitem"><p class="simpara">
						Leave the node and return to the undercloud as the <code class="literal">stack</code> user.
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ exit
[stack@director ~]$</pre></li><li class="listitem"><p class="simpara">
						Disable the Ceph Storage node so that director does not reprovision it.
					</p><pre class="screen">[stack@director ~]$ openstack baremetal node list
[stack@director ~]$ openstack baremetal node maintenance set UUID</pre></li><li class="listitem"><p class="simpara">
						Removing a Ceph Storage node requires an update to the <code class="literal">overcloud</code> stack in director with the local template files. First identify the UUID of the overcloud stack:
					</p><pre class="screen">$ openstack stack list</pre></li><li class="listitem"><p class="simpara">
						Identify the UUIDs of the Ceph Storage node you want to delete:
					</p><pre class="screen">$ openstack server list</pre></li><li class="listitem"><p class="simpara">
						Delete the node from the stack and update the plan accordingly:
					</p><pre class="screen">$ openstack overcloud node delete --stack overcloud &lt;NODE_UUID&gt;</pre><div class="admonition important"><div class="admonition_header">Important</div><div><p>
							If you passed any extra environment files when you created the overcloud, pass them again here using the <code class="literal">-e</code> option to avoid making undesired changes to the overcloud. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/director_installation_and_usage/index#modifying-the-overcloud-environment">Modifying the overcloud environment</a> in the <span class="emphasis"><em>Director Installation and Usage</em></span> guide.
						</p></div></div></li><li class="listitem">
						Wait until the stack completes its update. Use the <code class="literal">heat stack-list --show-nested</code> command to monitor the stack update.
					</li><li class="listitem"><p class="simpara">
						Add new nodes to the director node pool and deploy them as Ceph Storage nodes. Use the <code class="literal">CephStorageCount</code> parameter in the <code class="literal">parameter_defaults</code> of your environment file, in this case, <code class="literal">~/templates/storage-config.yaml</code>, to define the total number of Ceph Storage nodes in the overcloud.
					</p><pre class="literallayout">parameter_defaults:
  ControllerCount: 3
  OvercloudControlFlavor: control
  ComputeCount: 3
  OvercloudComputeFlavor: compute
  <span class="strong strong"><strong>CephStorageCount: 3</strong></span>
  OvercloudCephStorageFlavor: ceph-storage
  CephMonCount: 3
  OvercloudCephMonFlavor: ceph-mon</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							For more information about how to define the number of nodes per role, see <a class="xref" href="index.html#node-assignments" title="7.1. Assigning nodes and flavors to roles">Section 7.1, “Assigning nodes and flavors to roles”</a>.
						</p></div></div></li><li class="listitem"><p class="simpara">
						After you update your environment file, redeploy the overcloud:
					</p><pre class="screen">$ openstack overcloud deploy --templates -e &lt;ENVIRONMENT_FILE&gt;</pre><p class="simpara">
						Director provisions the new node and updates the entire stack with the details of the new node.
					</p></li><li class="listitem"><p class="simpara">
						Log in to a Controller node as the <code class="literal">heat-admin</code> user and check the status of the Ceph Storage node:
					</p><pre class="screen">[heat-admin@overcloud-controller-0 ~]$ sudo ceph status</pre></li><li class="listitem">
						Confirm that the value in the <code class="literal">osdmap</code> section matches the number of nodes in your cluster that you want. The Ceph Storage node that you removed is replaced with a new node.
					</li></ol></div></section><section class="section" id="adding-osd-to-ceph-storage-node"><div class="titlepage"><div><div><h2 class="title">11.3. Adding an OSD to a Ceph Storage node</h2></div></div></div><p>
				This procedure demonstrates how to add an OSD to a node. For more information about Ceph OSDs, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/operations_guide/managing-the-storage-cluster-size#ceph-osds-ops">Ceph OSDs</a> in the <span class="emphasis"><em>Red Hat Ceph Storage Operations Guide</em></span>.
			</p><p>
				<span class="strong strong"><strong>Procedure</strong></span>
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Notice the following heat template deploys Ceph Storage with three OSD devices:
					</p><pre class="screen">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem"><p class="simpara">
						To add an OSD, update the node disk layout as described in <a class="xref" href="index.html#Mapping_the_Ceph_Storage_Node_Disk_Layout" title="5.3. Mapping the Ceph Storage node disk layout">Section 5.3, “Mapping the Ceph Storage node disk layout”</a>. In this example, add <code class="literal">/dev/sde</code> to the template:
					</p><pre class="screen">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem">
						Run <code class="literal">openstack overcloud deploy</code> to update the overcloud.
					</li></ol></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					This example assumes that all hosts with OSDs have a new device called <code class="literal">/dev/sde</code>. If you do not want all nodes to have the new device, update the heat template as shown and see <a class="xref" href="index.html#map_disk_layout_non-homogen_ceph" title="5.5. Mapping the disk layout to non-homogeneous Ceph Storage nodes">Section 5.5, “Mapping the disk layout to non-homogeneous Ceph Storage nodes”</a> for information about how to define hosts with a differing <code class="literal">devices</code> list.
				</p></div></div></section><section class="section" id="removing-osd-from-ceph-storage-node"><div class="titlepage"><div><div><h2 class="title">11.4. Removing an OSD from a Ceph Storage node</h2></div></div></div><p>
				This procedure demonstrates how to remove an OSD from a node. It assumes the following about the environment:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						A server (<code class="literal">ceph-storage0</code>) has an OSD (<code class="literal">ceph-osd@4</code>) running on <code class="literal">/dev/sde</code>.
					</li><li class="listitem">
						The Ceph monitor service (<code class="literal">ceph-mon</code>) is running on <code class="literal">controller0</code>.
					</li><li class="listitem">
						There are enough available OSDs to ensure the storage cluster is not at its near-full ratio.
					</li></ul></div><p>
				For more information about Ceph OSDs, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/4/html/operations_guide/managing-the-storage-cluster-size#ceph-osds-ops">Ceph OSDs</a> in the <span class="emphasis"><em>Red Hat Ceph Storage Operations Guide</em></span>.
			</p><p>
				<span class="strong strong"><strong>Procedure</strong></span>
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						SSH into <code class="literal">ceph-storage0</code> and log in as <code class="literal">root</code>.
					</li><li class="listitem"><p class="simpara">
						Disable and stop the OSD service:
					</p><pre class="screen">[root@ceph-storage0 ~]# systemctl disable ceph-osd@4
[root@ceph-stoarge0 ~]# systemctl stop ceph-osd@4</pre></li><li class="listitem">
						Disconnect from <code class="literal">ceph-storage0</code>.
					</li><li class="listitem">
						SSH into <code class="literal">controller0</code> and log in as <code class="literal">root</code>.
					</li><li class="listitem"><p class="simpara">
						Identify the name of the Ceph monitor container:
					</p><pre class="screen">[root@controller0 ~]# podman ps | grep ceph-mon
ceph-mon-controller0
[root@controller0 ~]#</pre></li><li class="listitem"><p class="simpara">
						Enable the Ceph monitor container to mark the undesired OSD as <code class="literal">out</code>:
					</p><pre class="screen">[root@controller0 ~]# podman exec ceph-mon-controller0 ceph osd out 4</pre><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This command causes Ceph to rebalance the storage cluster and copy data to other OSDs in the cluster. The cluster temporarily leaves the <code class="literal">active+clean</code> state until rebalancing is complete.
						</p></div></div></li><li class="listitem"><p class="simpara">
						Run the following command and wait for the storage cluster state to become <code class="literal">active+clean</code>:
					</p><pre class="screen">[root@controller0 ~]# podman exec ceph-mon-controller0 ceph -w</pre></li><li class="listitem"><p class="simpara">
						Remove the OSD from the CRUSH map so that it no longer receives data:
					</p><pre class="screen">[root@controller0 ~]# podman exec ceph-mon-controller0 ceph osd crush remove osd.4</pre></li><li class="listitem"><p class="simpara">
						Remove the OSD authentication key:
					</p><pre class="screen">[root@controller0 ~]# podman exec ceph-mon-controller0 ceph auth del osd.4</pre></li><li class="listitem"><p class="simpara">
						Remove the OSD:
					</p><pre class="screen">[root@controller0 ~]# podman exec ceph-mon-controller0 ceph osd rm 4</pre></li><li class="listitem">
						Disconnect from <code class="literal">controller0</code>.
					</li><li class="listitem">
						SSH into the undercloud as the <code class="literal">stack</code> user and locate the heat environment file in which you defined the <code class="literal">CephAnsibleDisksConfig</code> parameter.
					</li><li class="listitem"><p class="simpara">
						Notice the heat template contains four OSDs:
					</p><pre class="screen">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
      - /dev/sde
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem"><p class="simpara">
						Modify the template to remove <code class="literal">/dev/sde</code>.
					</p><pre class="screen">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
    osd_scenario: lvm
    osd_objectstore: bluestore</pre></li><li class="listitem"><p class="simpara">
						Run <code class="literal">openstack overcloud deploy</code> to update the overcloud.
					</p><div class="admonition note"><div class="admonition_header">Note</div><div><p>
							This example assumes that you removed the <code class="literal">/dev/sde</code> device from all hosts with OSDs. If you do not remove the same device from all nodes, update the heat template as shown and see <a class="xref" href="index.html#map_disk_layout_non-homogen_ceph" title="5.5. Mapping the disk layout to non-homogeneous Ceph Storage nodes">Section 5.5, “Mapping the disk layout to non-homogeneous Ceph Storage nodes”</a> for information about how to define hosts with a differing <code class="literal">devices</code> list.
						</p></div></div></li></ol></div></section></section><section class="chapter" id="replacing_a_failed_disk"><div class="titlepage"><div><div><h1 class="title">Chapter 12. Replacing a failed disk</h1></div></div></div><p id="handling-disk-failure">
			If one of the disks fails in your Ceph cluster, complete the following procedures to replace it:
		</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
					Determining if there is a device name change, see <a class="xref" href="index.html#determining-if-device-name-changed" title="12.1. Determining if there is a device name change">Section 12.1, “Determining if there is a device name change”</a>.
				</li><li class="listitem">
					Ensuring that the OSD is down and destroyed, see <a class="xref" href="index.html#ensuring-osd-down-and-destroyed" title="12.2. Ensuring that the OSD is down and destroyed">Section 12.2, “Ensuring that the OSD is down and destroyed”</a>.
				</li><li class="listitem">
					Removing the old disk from the system and installing the replacement disk, see <a class="xref" href="index.html#removing-old-disk-installing-replacement" title="12.3. Removing the old disk from the system and installing the replacement disk">Section 12.3, “Removing the old disk from the system and installing the replacement disk”</a>.
				</li><li class="listitem">
					Verifying that the disk replacement is successful, see <a class="xref" href="index.html#verifying-the-disk-replacement-is-successful" title="12.4. Verifying that the disk replacement is successful">Section 12.4, “Verifying that the disk replacement is successful”</a>.
				</li></ol></div><section class="section" id="determining-if-device-name-changed"><div class="titlepage"><div><div><h2 class="title">12.1. Determining if there is a device name change</h2></div></div></div><p>
				Before you replace the disk, determine if the replacement disk for the replacement OSD has a different name in the operating system than the device that you want to replace. If the replacement disk has a different name, you must update Ansible parameters for the devices list so that subsequent runs of <code class="literal">ceph-ansible</code>, including when director runs <code class="literal">ceph-ansible</code>, do not fail as a result of the change. For an example of the devices list that you must change when you use director, see <a class="xref" href="index.html#Mapping_the_Ceph_Storage_Node_Disk_Layout" title="5.3. Mapping the Ceph Storage node disk layout">Section 5.3, “Mapping the Ceph Storage node disk layout”</a>.
			</p><div class="admonition warning"><div class="admonition_header">Warning</div><div><p>
					If the device name changes and you use the following procedures to update your system outside of <code class="literal">ceph-ansible</code> or director, there is a risk that the configuration management tools are out of sync with the system that they manage until you update the system definition files and the configuration is reasserted without error.
				</p></div></div><div class="formalpara"><p class="title"><strong>Persistent naming of storage devices</strong></p><p>
					Storage devices that the <code class="literal">sd</code> driver manages might not always have the same name across reboots. For example, a disk that is normally identified by <code class="literal">/dev/sdc</code> might be named <code class="literal">/dev/sdb</code>. It is also possible for the replacement disk, <code class="literal">/dev/sdc</code>, to appear in the operating system as <code class="literal">/dev/sdd</code> even if you want to use it as a replacement for <code class="literal">/dev/sdc</code>. To address this issue, use names that are persistent and match the following pattern: <code class="literal">/dev/disk/by-*</code>. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/persistent_naming">Persistent Naming</a> in the Red Hat Enterprise Linux (RHEL) 7 <span class="emphasis"><em>Storage Administration Guide</em></span>.
				</p></div><p>
				Depending on the naming method that you use to deploy Ceph, you might need to update the <code class="literal">devices</code> list after you replace the OSD. Use the following list of naming methods to determine if you must change the devices list:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">The major and minor number range method</span></dt><dd><p class="simpara">
							If you used <code class="literal">sd</code> and want to continue to use it, after you install the new disk, check if the name has changed. If the name did not change, for example, if the same name appears correctly as <code class="literal">/dev/sdd</code>, it is not necessary to change the name after you complete the disk replacement procedures.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								This naming method is not recommended because there is still a risk that the name becomes inconsistent over time. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/persistent_naming">Persistent Naming</a> in the RHEL 7 <span class="emphasis"><em>Storage Administration Guide</em></span>.
							</p></div></div></dd><dt><span class="term">The <code class="literal">by-path</code> method</span></dt><dd><p class="simpara">
							If you use this method, and you add a replacement disk in the same slot, then the path is consistent and no change is necessary.
						</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
								Although this naming method is preferable to the major and minor number range method, use caution to ensure that the target numbers do not change. For example, use persistent binding and update the names if a host adapter is moved to a different PCI slot. In addition, there is the possibility that the SCSI host numbers can change if a HBA fails to probe, if drivers are loaded in a different order, or if a new HBA is installed on the system. The <code class="literal">by-path</code> naming method also differs between RHEL7 and RHEL8. For more information, see:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Article [What is the difference between "by-path" links created in RHEL8 and RHEL7?] <a class="link" href="https://access.redhat.com/solutions/5171991">https://access.redhat.com/solutions/5171991</a>
									</li><li class="listitem">
										<a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_file_systems/index#assembly_overview-of-persistent-naming-attributes_managing-file-systems">Overview of persistent naming attributes</a> in the RHEL 8 <span class="emphasis"><em>Managing file systems</em></span> guide.
									</li></ul></div></div></div></dd><dt><span class="term">The <code class="literal">by-uuid</code> method</span></dt><dd>
							If you use this method, you can use the <code class="literal">blkid</code> utility to set the new disk to have the same UUID as the old disk. For more information, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/persistent_naming">Persistent Naming</a> in the RHEL 7 <span class="emphasis"><em>Storage Administration Guide</em></span>.
						</dd><dt><span class="term">The <code class="literal">by-id</code> method</span></dt><dd>
							If you use this method, you must change the devices list because this identifier is a property of the device and the device has been replaced.
						</dd></dl></div><p>
				When you add the new disk to the system, if it is possible to modify the persistent naming attributes according to the <span class="emphasis"><em>RHEL7 Storage Administrator Guide</em></span>, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/persistent_naming">Persistent Naming</a>, so that the device name is unchanged, then it is not necessary to update the devices list and re-run <code class="literal">ceph-ansible</code>, or trigger director to re-run <code class="literal">ceph-ansible</code> and you can proceed with the disk replacement procedures. However, you can re-run <code class="literal">ceph-ansible</code> to ensure that the change did not result in any inconsistencies.
			</p></section><section class="section" id="ensuring-osd-down-and-destroyed"><div class="titlepage"><div><div><h2 class="title">12.2. Ensuring that the OSD is down and destroyed</h2></div></div></div><p>
				On the server that hosts the Ceph Monitor, use the <code class="literal">ceph</code> command in the running monitor container to ensure that the OSD that you want to replace is down, and then destroy it.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Identify the name of the running Ceph monitor container and store it in an environment variable called <code class="literal">MON</code>:
					</p><pre class="screen">MON=$(podman ps | grep ceph-mon | awk {'print $1'})</pre></li><li class="listitem"><p class="simpara">
						Alias the <code class="literal">ceph</code> command so that it executes within the running Ceph monitor container:
					</p><pre class="screen">alias ceph="podman exec $MON ceph"</pre></li><li class="listitem"><p class="simpara">
						Use the new alias to verify that the OSD that you want to replace is down:
					</p><pre class="screen">[root@overcloud-controller-0 ~]# ceph osd tree | grep 27
27   hdd 0.04790         osd.27                    down  1.00000 1.00000</pre></li><li class="listitem"><p class="simpara">
						Destroy the OSD. The following example command destroys <code class="literal">OSD 27</code>:
					</p><pre class="screen">[root@overcloud-controller-0 ~]# ceph osd destroy 27 --yes-i-really-mean-it
destroyed osd.27</pre></li></ol></div></section><section class="section" id="removing-old-disk-installing-replacement"><div class="titlepage"><div><div><h2 class="title">12.3. Removing the old disk from the system and installing the replacement disk</h2></div></div></div><p>
				On the container host with the OSD that you want to replace, remove the old disk from the system and install the replacement disk.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites:</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						Verify that the device ID has changed. For more information, see <a class="xref" href="index.html#determining-if-device-name-changed" title="12.1. Determining if there is a device name change">Section 12.1, “Determining if there is a device name change”</a>.
					</li></ul></div><p>
				The <code class="literal">ceph-volume</code> command is present in the Ceph container but is not installed on the overcloud node. Create an alias so that the <code class="literal">ceph-volume</code> command runs the <code class="literal">ceph-volume</code> binary inside the Ceph container. Then use the <code class="literal">ceph-volume</code> command to clean the new disk and add it as an OSD.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Ensure that the failed OSD is not running:
					</p><pre class="screen">systemctl stop ceph-osd@27</pre></li><li class="listitem"><p class="simpara">
						Identify the image ID of the ceph container image and store it in an environment variable called <code class="literal">IMG</code>:
					</p><pre class="screen">IMG=$(podman images | grep ceph | awk {'print $3'})</pre></li><li class="listitem"><p class="simpara">
						Alias the <code class="literal">ceph-volume</code> command so that it runs inside the <code class="literal">$IMG</code> Ceph container, with the <code class="literal">ceph-volume</code> entry point and relevant directories:
					</p><pre class="screen">alias ceph-volume="podman run --rm --privileged --net=host --ipc=host -v /run/lock/lvm:/run/lock/lvm:z -v /var/run/udev/:/var/run/udev/:z -v /dev:/dev -v /etc/ceph:/etc/ceph:z -v /var/lib/ceph/:/var/lib/ceph/:z -v /var/log/ceph/:/var/log/ceph/:z --entrypoint=ceph-volume $IMG --cluster ceph"</pre></li><li class="listitem"><p class="simpara">
						Verify that the aliased command runs successfully:
					</p><pre class="screen">ceph-volume lvm list</pre></li><li class="listitem"><p class="simpara">
						Check that your new OSD device is not already part of LVM. Use the <code class="literal">pvdisplay</code> command to inspect the device, and ensure that the <code class="literal">VG Name</code> field is empty. Replace <code class="literal">&lt;NEW_DEVICE&gt;</code> with the <code class="literal">/dev/*</code> path of your new OSD device:
					</p><pre class="screen">[root@overcloud-computehci-2 ~]# pvdisplay &lt;NEW_DEVICE&gt;
  --- Physical volume ---
  PV Name               /dev/sdj
  VG Name               ceph-0fb0de13-fc8e-44c8-99ea-911e343191d2
  PV Size               50.00 GiB / not usable 1.00 GiB
  Allocatable           yes (but full)
  PE Size               1.00 GiB
  Total PE              49
  Free PE               0
  Allocated PE          49
  PV UUID               kOO0If-ge2F-UH44-6S1z-9tAv-7ypT-7by4cp
[root@overcloud-computehci-2 ~]#</pre><p class="simpara">
						If the <code class="literal">VG Name</code> field is not empty, then the device belongs to a volume group that you must remove.
					</p></li><li class="listitem"><p class="simpara">
						If the device belongs to a volume group, use the <code class="literal">lvdisplay</code> command to check if there is a logical volume in the volume group. Replace <code class="literal">&lt;VOLUME_GROUP&gt;</code> with the value of the <code class="literal">VG Name</code> field that you retrieved from the <code class="literal">pvdisplay</code> command:
					</p><pre class="screen">[root@overcloud-computehci-2 ~]# lvdisplay | grep &lt;VOLUME_GROUP&gt;
  LV Path                /dev/ceph-0fb0de13-fc8e-44c8-99ea-911e343191d2/osd-data-a0810722-7673-43c7-8511-2fd9db1dbbc6
  VG Name                ceph-0fb0de13-fc8e-44c8-99ea-911e343191d2
[root@overcloud-computehci-2 ~]#</pre><p class="simpara">
						If the <code class="literal">LV Path</code> field is not empty, then the device contains a logical volume that you must remove.
					</p></li><li class="listitem"><p class="simpara">
						If the new device is part of a logical volume or volume group, remove the logical volume, volume group, and the device association as a physical volume within the LVM system.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Replace <code class="literal">&lt;LV_PATH&gt;</code> with the value of the <code class="literal">LV Path</code> field.
							</li><li class="listitem">
								Replace <code class="literal">&lt;VOLUME_GROUP&gt;</code> with the value of the <code class="literal">VG Name</code> field.
							</li><li class="listitem"><p class="simpara">
								Replace <code class="literal">&lt;NEW_DEVICE&gt;</code> with the <code class="literal">/dev/*</code> path of your new OSD device.
							</p><pre class="screen">[root@overcloud-computehci-2 ~]# lvremove --force &lt;LV_PATH&gt;
  Logical volume "osd-data-a0810722-7673-43c7-8511-2fd9db1dbbc6" successfully removed</pre><pre class="screen">[root@overcloud-computehci-2 ~]# vgremove --force &lt;VOLUME_GROUP&gt;
  Volume group "ceph-0fb0de13-fc8e-44c8-99ea-911e343191d2" successfully removed</pre><pre class="screen">[root@overcloud-computehci-2 ~]# pvremove &lt;NEW_DEVICE&gt;
  Labels on physical volume "/dev/sdj" successfully wiped.</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						Ensure that the new OSD device is clean. In the following example, the device is <code class="literal">/dev/sdj</code>:
					</p><pre class="screen">[root@overcloud-computehci-2 ~]# ceph-volume lvm zap /dev/sdj
--&gt; Zapping: /dev/sdj
--&gt; --destroy was not specified, but zapping a whole device will remove the partition table
Running command: /usr/sbin/wipefs --all /dev/sdj
Running command: /bin/dd if=/dev/zero of=/dev/sdj bs=1M count=10
 stderr: 10+0 records in
10+0 records out
10485760 bytes (10 MB, 10 MiB) copied, 0.010618 s, 988 MB/s
--&gt; Zapping successful for: &lt;Raw Device: /dev/sdj&gt;
[root@overcloud-computehci-2 ~]#</pre></li><li class="listitem"><p class="simpara">
						Create the new OSD with the existing OSD ID by using the new device but pass <code class="literal">--no-systemd</code> so that <code class="literal">ceph-volume</code> does not attempt to start the OSD. This is not possible from within the container:
					</p><pre class="screen">ceph-volume lvm create --osd-id 27 --data /dev/sdj --no-systemd</pre></li><li class="listitem"><p class="simpara">
						Start the OSD outside of the container:
					</p><pre class="screen">systemctl start ceph-osd@27</pre></li></ol></div></section><section class="section" id="verifying-the-disk-replacement-is-successful"><div class="titlepage"><div><div><h2 class="title">12.4. Verifying that the disk replacement is successful</h2></div></div></div><p>
				To check that your disk replacement is successful, on the undercloud, complete the following steps.
			</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Check if the device name changed, update the devices list according to the naming method you used to deploy Ceph. For more information, see <a class="xref" href="index.html#determining-if-device-name-changed" title="12.1. Determining if there is a device name change">Section 12.1, “Determining if there is a device name change”</a>.
					</li><li class="listitem">
						To ensure that the change did not introduce any inconsistencies, re-run the overcloud deploy command to perform a stack update.
					</li><li class="listitem"><p class="simpara">
						In cases where you have hosts that have different device lists, you might have to define an exception. For example, you might use the following example heat environment file to deploy a node with three OSD devices.
					</p><pre class="screen">parameter_defaults:
  CephAnsibleDisksConfig:
    devices:
      - /dev/sdb
      - /dev/sdc
      - /dev/sdd
    osd_scenario: lvm
    osd_objectstore: bluestore</pre><p class="simpara">
						The <code class="literal">CephAnsibleDisksConfig</code> parameter applies to all nodes that host OSDs, so you cannot update the <code class="literal">devices</code> parameter with the new device list. Instead, you must define an exception for the new host that has a different device list. For more information about defining an exception, see <a class="xref" href="index.html#map_disk_layout_non-homogen_ceph" title="5.5. Mapping the disk layout to non-homogeneous Ceph Storage nodes">Section 5.5, “Mapping the disk layout to non-homogeneous Ceph Storage nodes”</a>.
					</p></li></ol></div></section></section><section class="appendix" id="envfile-createceph"><div class="titlepage"><div><div><h1 class="title">Appendix A. Sample environment file: creating a Ceph Storage cluster</h1></div></div></div><p>
			The following custom environment file uses many of the options described throughout <a class="xref" href="index.html#creation" title="Chapter 2. Preparing Ceph Storage nodes for overcloud deployment">Chapter 2, <em>Preparing Ceph Storage nodes for overcloud deployment</em></a>. This sample does not include any commented-out options. For an overview on environment files, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization#sect-Environment_Files">Environment Files</a> (from the <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/advanced_overcloud_customization">Advanced Overcloud Customization</a> guide).
		</p><div class="formalpara"><p class="title"><strong>/home/stack/templates/storage-config.yaml</strong></p><p>
				
<pre class="literallayout">parameter_defaults: <span id="CO5-1"><!--Empty--></span><span class="callout">1</span>
  CinderBackupBackend: ceph <span id="CO5-2"><!--Empty--></span><span class="callout">2</span>
  CephAnsibleDisksConfig: <span id="CO5-3"><!--Empty--></span><span class="callout">3</span>
    osd_scenario: lvm
    osd_objectstore: bluestore
    dmcrypt: true
    devices:
      - /dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:10:0
      - /dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:11:0
      - /dev/nvme0n1
  ControllerCount: 3 <span id="CO5-4"><!--Empty--></span><span class="callout">4</span>
  OvercloudControlFlavor: control
  ComputeCount: 3
  OvercloudComputeFlavor: compute
  CephStorageCount: 3
  OvercloudCephStorageFlavor: ceph-storage
  CephMonCount: 3
  OvercloudCephMonFlavor: ceph-mon
  CephMdsCount: 3
  OvercloudCephMdsFlavor: ceph-mds
  NeutronNetworkType: vxlan <span id="CO5-5"><!--Empty--></span><span class="callout">5</span></pre>

			</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="index.html#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
					The <code class="literal">parameter_defaults</code> section modifies the default values for parameters in all templates. Most of the entries listed here are described in <a class="xref" href="index.html#enable-ceph-overcloud" title="Chapter 4. Customizing the Storage service">Chapter 4, <em>Customizing the Storage service</em></a>.
				</div></dd><dt><a href="index.html#CO5-2"><span class="callout">2</span></a> </dt><dd><div class="para">
					If you are deploying the Ceph Object Gateway, you can use Ceph Object Storage (<code class="literal">ceph-rgw</code>) as a backup target. To configure this, set <code class="literal">CinderBackupBackend</code> to <code class="literal">swift</code>. See <a class="xref" href="index.html#ceph-rgw" title="4.2. Enabling the Ceph Object Gateway">Section 4.2, “Enabling the Ceph Object Gateway”</a> for details.
				</div></dd><dt><a href="index.html#CO5-3"><span class="callout">3</span></a> </dt><dd><div class="para">
					The <code class="literal">CephAnsibleDisksConfig</code> section defines a custom disk layout for deployments using BlueStore.
				</div></dd><dt><a href="index.html#CO5-4"><span class="callout">4</span></a> </dt><dd><div class="para">
					For each role, the <code class="literal">*Count</code> parameters assign a number of nodes while the <code class="literal">Overcloud*Flavor</code> parameters assign a flavor. For example, <code class="literal">ControllerCount: 3</code> assigns 3 nodes to the Controller role, and <code class="literal">OvercloudControlFlavor: control</code> sets each of those roles to use the <code class="literal">control</code> flavor. See <a class="xref" href="index.html#node-assignments" title="7.1. Assigning nodes and flavors to roles">Section 7.1, “Assigning nodes and flavors to roles”</a> for details.
				</div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
						The <code class="literal">CephMonCount</code>, <code class="literal">CephMdsCount</code>, <code class="literal">OvercloudCephMonFlavor</code>, and <code class="literal">OvercloudCephMdsFlavor</code> parameters (along with the <code class="literal">ceph-mon</code> and <code class="literal">ceph-mds</code> flavors) will only be valid if you created a custom <code class="literal">CephMON</code> and <code class="literal">CephMds</code> role, as described in <a class="xref" href="index.html#dedicated-nodes" title="Chapter 3. Deploying Ceph services on dedicated nodes">Chapter 3, <em>Deploying Ceph services on dedicated nodes</em></a>.
					</p></div></div></dd><dt><a href="index.html#CO5-5"><span class="callout">5</span></a> </dt><dd><div class="para">
					<code class="literal">NeutronNetworkType:</code> sets the network type that the <code class="literal">neutron</code> service should use (in this case, <code class="literal">vxlan</code>).
				</div></dd></dl></div></section><section class="appendix" id="template-multibonded-nics"><div class="titlepage"><div><div><h1 class="title">Appendix B. Sample custom interface template: multiple bonded interfaces</h1></div></div></div><p>
			The following template is a customized version of <code class="literal">/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml</code>. It features multiple bonded interfaces to isolate back-end and front-end storage network traffic, along with redundancy for both connections, as described in <a class="xref" href="index.html#multibonded-nics" title="4.5. Configuring multiple bonded interfaces for Ceph nodes">Section 4.5, “Configuring multiple bonded interfaces for Ceph nodes”</a>.
		</p><p>
			It also uses custom bonding options, <code class="literal">'mode=4 lacp_rate=1'</code>, as described in <a class="xref" href="index.html#multibonded-nics-ovs-opts" title="4.5.1. Configuring bonding module directives">Section 4.5.1, “Configuring bonding module directives”</a>.
		</p><div class="formalpara"><p class="title"><strong>/usr/share/openstack-tripleo-heat-templates/network/config/bond-with-vlans/ceph-storage.yaml (custom)</strong></p><p>
				
<pre class="literallayout">heat_template_version: 2015-04-30

description: &gt;
  Software Config to drive os-net-config with 2 bonded nics on a bridge
  with VLANs attached for the ceph storage role.

parameters:
  ControlPlaneIp:
    default: ''
    description: IP address/subnet on the ctlplane network
    type: string
  ExternalIpSubnet:
    default: ''
    description: IP address/subnet on the external network
    type: string
  InternalApiIpSubnet:
    default: ''
    description: IP address/subnet on the internal API network
    type: string
  StorageIpSubnet:
    default: ''
    description: IP address/subnet on the storage network
    type: string
  StorageMgmtIpSubnet:
    default: ''
    description: IP address/subnet on the storage mgmt network
    type: string
  TenantIpSubnet:
    default: ''
    description: IP address/subnet on the tenant network
    type: string
  ManagementIpSubnet: # Only populated when including environments/network-management.yaml
    default: ''
    description: IP address/subnet on the management network
    type: string
  <span class="strong strong"><strong>BondInterfaceOvsOptions:</strong></span>
    <span class="strong strong"><strong>default: 'mode=4 lacp_rate=1'</strong></span>
    description: The bonding_options string for the bond interface. Set
                 things like lacp=active and/or bond_mode=balance-slb
                 using this option.
    type: string
    constraints:
      - allowed_pattern: "^((?!balance.tcp).)*$"
        description: |
          The balance-tcp bond mode is known to cause packet loss and
          should not be used in BondInterfaceOvsOptions.
  ExternalNetworkVlanID:
    default: 10
    description: Vlan ID for the external network traffic.
    type: number
  InternalApiNetworkVlanID:
    default: 20
    description: Vlan ID for the internal_api network traffic.
    type: number
  StorageNetworkVlanID:
    default: 30
    description: Vlan ID for the storage network traffic.
    type: number
  StorageMgmtNetworkVlanID:
    default: 40
    description: Vlan ID for the storage mgmt network traffic.
    type: number
  TenantNetworkVlanID:
    default: 50
    description: Vlan ID for the tenant network traffic.
    type: number
  ManagementNetworkVlanID:
    default: 60
    description: Vlan ID for the management network traffic.
    type: number
  ControlPlaneSubnetCidr: # Override this via parameter_defaults
    default: '24'
    description: The subnet CIDR of the control plane network.
    type: string
  ControlPlaneDefaultRoute: # Override this via parameter_defaults
    description: The default route of the control plane network.
    type: string
  ExternalInterfaceDefaultRoute: # Not used by default in this template
    default: '10.0.0.1'
    description: The default route of the external network.
    type: string
  ManagementInterfaceDefaultRoute: # Commented out by default in this template
    default: unset
    description: The default route of the management network.
    type: string
  DnsServers: # Override this via parameter_defaults
    default: []
    description: A list of DNS servers (2 max for some implementations) that will be added to resolv.conf.
    type: comma_delimited_list
  EC2MetadataIp: # Override this via parameter_defaults
    description: The IP address of the EC2 metadata server.
    type: string

resources:
  OsNetConfigImpl:
    type: OS::Heat::StructuredConfig
    properties:
      group: os-apply-config
      config:
        os_net_config:
          network_config:
            -
              type: interface
              name: nic1
              use_dhcp: false
              dns_servers: {get_param: DnsServers}
              addresses:
                -
                  ip_netmask:
                    list_join:
                      - '/'
                      - - {get_param: ControlPlaneIp}
                        - {get_param: ControlPlaneSubnetCidr}
              routes:
                -
                  ip_netmask: 169.254.169.254/32
                  next_hop: {get_param: EC2MetadataIp}
                -
                  default: true
                  next_hop: {get_param: ControlPlaneDefaultRoute}
            -
              <span class="strong strong"><strong>type: ovs_bridge</strong></span>
              <span class="strong strong"><strong>name: br-bond</strong></span>
              <span class="strong strong"><strong>members:</strong></span>
                -
                  <span class="strong strong"><strong>type: linux_bond</strong></span>
                  <span class="strong strong"><strong>name: bond1</strong></span>
                  <span class="strong strong"><strong>bonding_options: {get_param: BondInterfaceOvsOptions}</strong></span>
                  <span class="strong strong"><strong>members:</strong></span>
                    -
                      <span class="strong strong"><strong>type: interface</strong></span>
                      <span class="strong strong"><strong>name: nic2</strong></span>
                      <span class="strong strong"><strong>primary: true</strong></span>
                    -
                      <span class="strong strong"><strong>type: interface</strong></span>
                      <span class="strong strong"><strong>name: nic3</strong></span>
                -
                  <span class="strong strong"><strong>type: vlan</strong></span>
                  <span class="strong strong"><strong>device: bond1</strong></span>
                  <span class="strong strong"><strong>vlan_id: {get_param: StorageNetworkVlanID}</strong></span>
                  <span class="strong strong"><strong>addresses:</strong></span>
                    -
                      <span class="strong strong"><strong>ip_netmask: {get_param: StorageIpSubnet}</strong></span>
            -
              <span class="strong strong"><strong>type: ovs_bridge</strong></span>
              <span class="strong strong"><strong>name: br-bond2</strong></span>
              <span class="strong strong"><strong>members:</strong></span>
                -
                  <span class="strong strong"><strong>type: linux_bond</strong></span>
                  <span class="strong strong"><strong>name: bond2</strong></span>
                  <span class="strong strong"><strong>bonding_options: {get_param: BondInterfaceOvsOptions}</strong></span>
                  <span class="strong strong"><strong>members:</strong></span>
                    -
                      <span class="strong strong"><strong>type: interface</strong></span>
                      <span class="strong strong"><strong>name: nic4</strong></span>
                      <span class="strong strong"><strong>primary: true</strong></span>
                    -
                      <span class="strong strong"><strong>type: interface</strong></span>
                      <span class="strong strong"><strong>name: nic5</strong></span>
                -
                  <span class="strong strong"><strong>type: vlan</strong></span>
                  <span class="strong strong"><strong>device: bond1</strong></span>
                  <span class="strong strong"><strong>vlan_id: {get_param: StorageMgmtNetworkVlanID}</strong></span>
                  <span class="strong strong"><strong>addresses:</strong></span>
                    -
                      <span class="strong strong"><strong>ip_netmask: {get_param: StorageMgmtIpSubnet}</strong></span>
outputs:
  OS::stack_id:
    description: The OsNetConfigImpl resource.
    value: {get_resource: OsNetConfigImpl}</pre>

			</p></div></section><div><div xml:lang="en-US" class="legalnotice" id="idm140342040708176"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"><!--Empty--></span>© 2020 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div>
  
  </div>
  </div>
</div>
<div id="comments-footer" class="book-comments">
  </div>
<meta itemscope="" itemref="md1">

    <!-- Display: Next/Previous Nav -->
          
      </div>
</article>


  

            </div>
        </main>
    </div>
    <!--googleoff: all-->
    <div id="to-top"><a class="btn_slideto" href="index.html#masthead" aria-label="Back to Top"><span class="web-icon-upload"></span></a></div>
    <footer class="footer-main">
        <div class="footer-top">
            <div class="container">

              <div class="brand">
                <a href="https://redhat.com">
                  <svg class="rh-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 613 145">
                    <defs>
                      <style>
                        .rh-logo-hat {
                          fill: #e00;
                        }
                        .rh-logo-type {
                          fill: #fff;
                        }
                      </style>
                    </defs>
                    <title>Red Hat</title>
                    <path
                      class="rh-logo-hat"
                      d="M127.47,83.49c12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42l-7.45-32.36c-1.72-7.12-3.23-10.35-15.73-16.6C124.89,8.69,103.76.5,97.51.5,91.69.5,90,8,83.06,8c-6.68,0-11.64-5.6-17.89-5.6-6,0-9.91,4.09-12.93,12.5,0,0-8.41,23.72-9.49,27.16A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33C22.27,52,.5,55,.5,74.22c0,31.48,74.59,70.28,133.65,70.28,45.28,0,56.7-20.48,56.7-36.65,0-12.72-11-27.16-30.83-35.78"/>
                      <path class="rh-logo-band"
                      d="M160,72.07c1.73,8.19,1.73,9.05,1.73,10.13,0,14-15.74,21.77-36.43,21.77C78.54,104,37.58,76.6,37.58,58.49a18.45,18.45,0,0,1,1.51-7.33l3.66-9.06A6.43,6.43,0,0,0,42.53,44c0,9.22,36.3,39.45,84.94,39.45,12.51,0,30.61-2.58,30.61-17.46a14,14,0,0,0-.31-3.42Z"/>
                      <path
                      class="rh-logo-type"
                      d="M579.74,92.8c0,11.89,7.15,17.67,20.19,17.67a52.11,52.11,0,0,0,11.89-1.68V95a24.84,24.84,0,0,1-7.68,1.16c-5.37,0-7.36-1.68-7.36-6.73V68.3h15.56V54.1H596.78v-18l-17,3.68V54.1H568.49V68.3h11.25Zm-53,.32c0-3.68,3.69-5.47,9.26-5.47a43.12,43.12,0,0,1,10.1,1.26v7.15a21.51,21.51,0,0,1-10.63,2.63c-5.46,0-8.73-2.1-8.73-5.57m5.2,17.56c6,0,10.84-1.26,15.36-4.31v3.37h16.82V74.08c0-13.56-9.14-21-24.39-21-8.52,0-16.94,2-26,6.1l6.1,12.52c6.52-2.74,12-4.42,16.83-4.42,7,0,10.62,2.73,10.62,8.31v2.73a49.53,49.53,0,0,0-12.62-1.58c-14.31,0-22.93,6-22.93,16.73,0,9.78,7.78,17.24,20.19,17.24m-92.44-.94h18.09V80.92h30.29v28.82H506V36.12H487.93V64.41H457.64V36.12H439.55ZM370.62,81.87c0-8,6.31-14.1,14.62-14.1A17.22,17.22,0,0,1,397,72.09V91.54A16.36,16.36,0,0,1,385.24,96c-8.2,0-14.62-6.1-14.62-14.09m26.61,27.87h16.83V32.44l-17,3.68V57.05a28.3,28.3,0,0,0-14.2-3.68c-16.19,0-28.92,12.51-28.92,28.5a28.25,28.25,0,0,0,28.4,28.6,25.12,25.12,0,0,0,14.93-4.83ZM320,67c5.36,0,9.88,3.47,11.67,8.83H308.47C310.15,70.3,314.36,67,320,67M291.33,82c0,16.2,13.25,28.82,30.28,28.82,9.36,0,16.2-2.53,23.25-8.42l-11.26-10c-2.63,2.74-6.52,4.21-11.14,4.21a14.39,14.39,0,0,1-13.68-8.83h39.65V83.55c0-17.67-11.88-30.39-28.08-30.39a28.57,28.57,0,0,0-29,28.81M262,51.58c6,0,9.36,3.78,9.36,8.31S268,68.2,262,68.2H244.11V51.58Zm-36,58.16h18.09V82.92h13.77l13.89,26.82H292l-16.2-29.45a22.27,22.27,0,0,0,13.88-20.72c0-13.25-10.41-23.45-26-23.45H226Z"/>
                  </svg>
                </a>
              </div>

            <div role="navigation">
                <h3>Quick Links</h3>
                <ul>
                    <li><a class="download-software" href="https://access.redhat.com/downloads/">Downloads</a></li>
                    <li><a class="manage-subscriptions" href="https://access.redhat.com/management/subscriptions/#active">Subscriptions</a></li>
                    <li><a class="support-cases" href="https://access.redhat.com/support">Support Cases</a></li>
                    <li><a class="customer-service" href="https://access.redhat.com/support/customer-service">Customer Service</a></li>
                    <li><a class="quick-docs" href="https://access.redhat.com/documentation">Product Documentation</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Help</h3>
                <ul>
                    <li><a class="contact-us" href="https://access.redhat.com/support/contact/">Contact Us</a></li>
                    <li><a class="cp-faqs" href="https://access.redhat.com/articles/33844">Customer Portal FAQ</a></li>
                    <li><a class="login-problems" href="https://access.redhat.com/help/login_assistance">Log-in Assistance</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Site Info</h3>
                <ul>
                  <li><a class="trust-red-hat" href="https://www.redhat.com/en/trust">Trust Red Hat</a></li>
                  <li><a class="browser-support-policy" href="https://access.redhat.com/help/browsers/">Browser Support Policy</a></li>
                  <li><a class="accessibility" href="https://access.redhat.com/help/accessibility/">Accessibility</a></li>
                  <li><a class="recognition" href="https://access.redhat.com/recognition/">Awards and Recognition</a></li>
                  <li><a class="colophon" href="https://access.redhat.com/help/colophon/">Colophon</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>Related Sites</h3>
                <ul>
                    <li><a href="https://www.redhat.com/" class="red-hat-com">redhat.com</a></li>
                    <li><a href="https://www.openshift.com" class="openshift-com">openshift.com</a></li>
                    <li><a href="http://developers.redhat.com/" class="red-hat-developers">developers.redhat.com</a></li>
                    <li><a href="https://connect.redhat.com/" class="partner-connect">connect.redhat.com</a></li>
                </ul>
            </div>

            <div role="navigation">
                <h3>About</h3>
                <ul>
                    <li><a href="https://access.redhat.com/subscription-value" class="subscription-value">Red Hat Subscription Value</a></li>
                    <li><a href="https://www.redhat.com/about/" class="about-red-hat">About Red Hat</a></li>
                    <li><a href="http://jobs.redhat.com" class="about-jobs">Red Hat Jobs</a></li>
                </ul>
            </div>

            </div>
        </div>

        <div class="anchor">
            <div class="container">
                <div class="status-legal">
                    <a hidden href="https://status.redhat.com" class="status-page-widget">
                          <span class="status-description"></span>
                          <span class="status-dot shape-circle"></span>
                    </a>
                    <div class="legal-copyright">
                        <div class="copyright">Copyright © 2020 Red Hat, Inc.</div>

                        <div role="navigation" class="legal">
                            <ul>
                                <li><a href="http://www.redhat.com/en/about/privacy-policy" class="privacy-policy">Privacy Statement</a></li>
                                <li><a href="https://access.redhat.com/help/terms/" class="terms-of-use">Customer Portal Terms of Use</a></li>
                                <li><a href="http://www.redhat.com/en/about/all-policies-guidelines" class="all-policies">All Policies and Guidelines</a></li>
                                <li><a id="teconsent"></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
                <div class="social">
                    <a href="http://www.redhat.com/summit/" class="summit">
                      <img src="rh-summit-red-a.svg" alt="Red Hat Summit" />
                    </a>

                    <div class="social-media">
                        <a href="https://twitter.com/RedHatSupport" class="sm-icon twitter"><span class="nicon-twitter"></span><span class="offscreen">Twitter</span></a>
                        <a href="https://www.facebook.com/RedHatSupport" class="sm-icon facebook"><span class="nicon-facebook"></span><span class="offscreen">Facebook</span></a>
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- TrustArc -->
    <div id="consent_blackbar"></div> 
    <!--googleon: all-->
</div>
  
  <div id="formatHelp" class="modal fade" tabindex="-1" role="dialog" aria-labelledby="formatTitle" aria-hidden="true"><div class="modal-dialog"><div class="modal-content">
    <div class="modal-header">
      <button type="button" class="close" data-dismiss="modal" aria-hidden="true">×</button>
      <h3 id="formatTitle">Formatting Tips</h3>
    </div>
    <div class="modal-body">
      <p>Here are the common uses of Markdown.</p><dl class="formatting-help">
        <dt class="codeblock">Code blocks</dt><dd class="codeblock"><pre><code>~~~
Code surrounded in tildes is easier to read
~~~</code></pre></dd>
        <dt class="urls">Links/URLs</dt><dd class="urls"><code>[Red Hat Customer Portal](https://access.redhat.com)</code></dd>
       </dl>
    </div>
    <div class="modal-footer">
      <a target="_blank" href="https://access.redhat.com/help/markdown" class="btn btn-primary">Learn more</a>
      <button class="btn" data-dismiss="modal" aria-hidden="true">Close</button>
    </div>
  </div></div></div></body>
</html>
